{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee7dc9e-cc1a-456e-9578-3a4d8231ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from model_util import SequenceDataset, DistilBertData\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPOCHS = 20\n",
    "MAX_STRING_LENGTH = 29\n",
    "batch_size=8\n",
    "SAVE_SPACE = True # deletes data structures not needed from dataset\n",
    "\n",
    "PROBLEM = \"04.03.TLT.4.1.6\" #\"04.03.TLT.2.1.2\"\n",
    "RESULTS_PATH = \"../trained_models/\"\n",
    "\n",
    "ACCEPTOR_NAME = \"distilbert_problem_{}.pk\".format(PROBLEM) # how to save model\n",
    "DATASET_CONTAINER_PATH = \"dataset_problem_{}.pk\".format(PROBLEM) # where to save dataset metadata \n",
    "\n",
    "TRAIN_DATA_PATH = \"../data/abbadingo/Mid/{}_Train.txt.dat\".format(PROBLEM)\n",
    "TEST_DATA_PATH = \"../data/abbadingo/Mid/{}_TestSR.txt.dat\".format(PROBLEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda14090-a4fa-493b-a291-1a1267748a3b",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72b38fa-59d2-4d84-a209-b028fc478a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet size:  4\n",
      "Sequences loaded. Some examples: \n",
      "[['a', 'a', 'a', 'a', 'a', 'a', 'b', 'a', 'b', 'b', 'b', 'd', 'c', 'c', 'b', 'b', 'b', 'b', 'a', 'a'], ['a', 'a', 'a', 'a', 'a', 'b', 'a', 'b', 'b', 'd', 'd', 'c', 'd', 'c', 'c', 'b', 'd', 'b', 'd', 'c'], ['a', 'a', 'a', 'a', 'a', 'c', 'c', 'a', 'b', 'a', 'c', 'c', 'a', 'a', 'b', 'd', 'a', 'd', 'd', 'b']]\n",
      "The symbol dictionary: {'a': 0, 'b': 1, 'd': 2, 'c': 3}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SequenceDataset(TRAIN_DATA_PATH, maxlen=MAX_STRING_LENGTH)\n",
    "#dataset.initialize(DATASET_CONTAINER_PATH)\n",
    "train_dataset.encode_sequences()\n",
    "train_dataset.save_state(os.path.join(RESULTS_PATH, DATASET_CONTAINER_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1cb6d-3fc7-47d8-8830-751557fa1dae",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee51c66c-bb20-4b5d-8d8b-95d66085d875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom message: Loading distilbert\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(7, 12, padding_idx=6)\n",
      "      (position_embeddings): Embedding(33, 12)\n",
      "      (LayerNorm): LayerNorm((12,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=12, out_features=12, bias=True)\n",
      "            (k_lin): Linear(in_features=12, out_features=12, bias=True)\n",
      "            (v_lin): Linear(in_features=12, out_features=12, bias=True)\n",
      "            (out_lin): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((12,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=12, out_features=8, bias=True)\n",
      "            (lin2): Linear(in_features=8, out_features=12, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((12,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=12, out_features=12, bias=True)\n",
      "  (classifier): Linear(in_features=12, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def make_dict(**kwargs):\n",
    "    return kwargs\n",
    "\n",
    "init_dict = make_dict(\n",
    "    vocab_size=train_dataset.alphabet_size+3,\n",
    "    max_position_embeddings=train_dataset.maxlen+2,\n",
    "    sinusoidal_pos_embds=True,\n",
    "    n_layers=2,\n",
    "    n_heads=4,\n",
    "    dim=train_dataset.alphabet_size*3,\n",
    "    hidden_dim=train_dataset.alphabet_size*2,\n",
    "    activation=\"gelu\",\n",
    "    dropout=0.1,\n",
    "    attention_dropout=0.1,\n",
    "    seq_classif_dropout=0.2,\n",
    "    pad_token_id=train_dataset.PAD\n",
    ")\n",
    "\n",
    "model=transformers.DistilBertForSequenceClassification(transformers.DistilBertConfig(**init_dict))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7507e7e-e97f-4dd6-9c57-df821d0ee267",
   "metadata": {},
   "source": [
    "## Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cadacb7-9b4d-49a5-99ec-18d9388a53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_SPACE:\n",
    "    del train_dataset.one_hot_seq\n",
    "    del train_dataset.one_hot_seq_sr\n",
    "    del train_dataset.ordinal_seq_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b24598-c226-47e7-b084-842303ed6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = torch.cat(list(torch.unsqueeze(x, 0) for x in train_dataset.ordinal_seq))\n",
    "train_labels = train_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e04ea2d-7c47-4442-94fe-55bf8e522f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_attn_mask(lengths, maxlen):\n",
    "    \"\"\"\n",
    "    Lengths is a list. For each sequence in input_ids it gives the length\n",
    "    \"\"\"\n",
    "    res = torch.ones((len(lengths), maxlen))\n",
    "    for i, l in enumerate(lengths):\n",
    "        res[i, l:] = 0\n",
    "    return res\n",
    "\n",
    "train_attn_mask = construct_attn_mask(train_dataset.sequence_lengths, train_dataset.maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2788c949-e8ba-4e7a-8c0c-64fec761561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forward_dict(x, y, mask, output_attentions=False):\n",
    "    forward_dict = make_dict(\n",
    "        input_ids=x, # the training data?\n",
    "        labels=y, # the training labels\n",
    "        attention_mask=mask, # TODO: we can do this to improve the models I suppose\n",
    "        head_mask=None,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    return forward_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9cc786-4df6-4b5c-bba5-ae482228197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DistilBertData(train_input_ids, train_labels, train_attn_mask)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb20b2-f932-4926-b6cf-a627b0f0f13e",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50654f-3c1e-4c71-be4c-66ad2448e62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "  batch 0 loss: 0.0866336077451706\n",
      "  batch 100 loss: 0.08664443984627723\n",
      "  batch 200 loss: 0.08663883507251739\n",
      "  batch 300 loss: 0.08665703825652599\n",
      "  batch 400 loss: 0.08663988970220089\n",
      "  batch 500 loss: 0.08663864806294441\n",
      "  batch 600 loss: 0.0866508735716343\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)# for whole sequence: lr=0.00001\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "divisor = 0.\n",
    "\n",
    "for i in range(1, EPOCHS+1):\n",
    "    print(\"Epoch: \", i)\n",
    "    for j, (x_batch, y_batch, mask_batch) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        model_input = get_forward_dict(x_batch, y_batch, mask_batch)\n",
    "        outputs = model(**model_input)\n",
    "        logits_before_softmax = outputs.logits\n",
    "        #loss = loss_fn(F.softmax(logits_before_softmax, dim=1), torch.argmax(y_batch, dim=1))\n",
    "        loss = loss_fn(logits_before_softmax, torch.argmax(y_batch, dim=1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        divisor += float( list(x_batch.size())[0] )\n",
    "        if j % 100 == 0:\n",
    "            last_loss = running_loss / divisor # loss per batch\n",
    "            print('  batch {} loss: {}'.format(j, last_loss))\n",
    "            running_loss = 0.\n",
    "            divisor = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd17b6-a779-4e32-9deb-14f923c130e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, os.path.join(\"..\", \"trained_models\", \"model_{}.pk\".format(PROBLEM)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baacf7d0-d263-47c3-8bda-00fcb22294cb",
   "metadata": {},
   "source": [
    "### Checking the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715eae4c-835e-451f-a6ce-a91035b0737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {v: k for k, v in train_dataset.symbol_dict.items()}\n",
    "mapping[train_dataset.SOS] = \"<SOS>\"\n",
    "mapping[train_dataset.EOS] = \"<EOS>\"\n",
    "mapping[train_dataset.PAD] = \"-\"\n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff61886-2dd3-4b89-9322-d8467a62ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids_to_chars(mapping, tensor_1d, eos_symbol: int = None):\n",
    "    \"\"\"\n",
    "    Converts the input_ids tensor to a list representing the original input.\n",
    "    Mapping is dict mapping int to str/char.\n",
    "\n",
    "    tensor_1d: The sequence as provided to the model.\n",
    "    \"\"\"\n",
    "\n",
    "    res = list()\n",
    "    eos_idx = None\n",
    "    try:\n",
    "        tensor_1d = tensor_1d.detach().numpy()\n",
    "    except:\n",
    "        tensor_1d = tensor_1d.numpy()\n",
    "    for i, x in enumerate(tensor_1d):\n",
    "        if eos_symbol is not None and x==eos_symbol:\n",
    "            eos_idx = i\n",
    "        res.append(mapping[x])\n",
    "    return res, eos_idx\n",
    "\n",
    "def map_sequences(mapping, x, eos_symbol: int=None):\n",
    "    \"\"\"\n",
    "    x is two dimensional tensor (bsize, len_of_sequence)\n",
    "    \"\"\"\n",
    "    for xx in list(x.detach()):\n",
    "        s, eos_idx = convert_ids_to_chars(mapping, xx, eos_symbol)\n",
    "        print(\"Mapped sequence: {}\".format(\" \".join(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9054f0-7e77-4c38-b53c-cb2a4d0ab6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for j, (x_batch, y_batch, mask_batch) in enumerate(train_dataloader):\n",
    "        model_input = get_forward_dict(x_batch, y_batch, mask_batch)\n",
    "        outputs = model(**model_input)\n",
    "\n",
    "        if j==100:\n",
    "            print(y_batch, \"\\n\", F.softmax(outputs.logits, dim=1), \"\\n\", x_batch.detach().numpy())\n",
    "            map_sequences(mapping=mapping, x=x_batch, eos_symbol=train_dataset.EOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00338d-b715-4efa-86a7-ee7b7696b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99626399-8260-4b01-8e4c-65fb09067106",
   "metadata": {},
   "source": [
    "## Model testing\n",
    "\n",
    "### 1. Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430c557-f4bb-4811-9a01-b560c9285dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attn_mask.dtype, train_input_ids.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1c2c2-2b53-4779-8939-90e972f49e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_res = model(**get_forward_dict(train_input_ids, None, train_attn_mask)).logits\n",
    "train_res = np.array(torch.argmax(train_res, dim=1))\n",
    "train_res.shape, np.unique(train_res, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2d573-cad8-4daf-9d27-bcb88f537851",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(train_labels, train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac70486-cfe7-4088-81e6-4b28f938829b",
   "metadata": {},
   "source": [
    "### 2. Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0daea13-9125-4627-a807-880cbef92dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SequenceDataset(TEST_DATA_PATH, maxlen=MAX_STRING_LENGTH)\n",
    "test_dataset.initialize(os.path.join(RESULTS_PATH, DATASET_CONTAINER_PATH))\n",
    "test_dataset.encode_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a178b1-3e3d-4668-9382-7b3d8abe5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_SPACE:\n",
    "    del test_dataset.one_hot_seq\n",
    "    del test_dataset.one_hot_seq_sr\n",
    "    del test_dataset.ordinal_seq_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe60d93-15cf-47f8-bbfa-62bdb1ccafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = torch.cat(list(torch.unsqueeze(x, 0) for x in test_dataset.ordinal_seq))\n",
    "test_labels = test_dataset.labels\n",
    "\n",
    "test_attn_mask = construct_attn_mask(test_dataset.sequence_lengths, test_dataset.maxlen)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_res = model(**get_forward_dict(test_input_ids, None, test_attn_mask)).logits\n",
    "test_res = np.array(torch.argmax(test_res, dim=1))\n",
    "test_res.shape, np.unique(test_res, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4189697-a4fb-48c5-81af-a32c249a8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_labels, test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc059f-38fa-479f-8521-72dfb3e29158",
   "metadata": {},
   "source": [
    "## Try a couple of sequences on your own"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146ab87-4916-4c2c-bd79-5adcea51d217",
   "metadata": {},
   "source": [
    "### Do sequences and just look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4016849-6c14-4462-96e3-bbf3da19912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_dict = {k: v for k, v in train_dataset.symbol_dict.items()}\n",
    "symbol_dict[\"<SOS>\"] = train_dataset.SOS\n",
    "symbol_dict[\"<EOS>\"] = train_dataset.EOS\n",
    "symbol_dict[\"<PAD>\"] = train_dataset.PAD\n",
    "\n",
    "def encode_sequences(sequences: list, symbol_dict: dict, maxlen: int):\n",
    "    \"\"\"\n",
    "    Encodes the sequences and returns a tensor. sequences is list of list.\n",
    "    Shape of result: (len(sequences, maxlen+2)), with +2 for SOS and EOS\n",
    "\n",
    "    => maxlen must be maximum length without SOS and EOS!!!\n",
    "    \"\"\"\n",
    "    res = torch.ones((len(sequences), maxlen+2), dtype=torch.int64) * symbol_dict[\"<PAD>\"]\n",
    "    lengths = list()\n",
    "    for i, seq in enumerate(sequences):\n",
    "        lengths.append(len(seq)+2) # plus 2 for SOS, EOS\n",
    "        res[i, 0] = symbol_dict[\"<SOS>\"]\n",
    "        for j, symbol in enumerate(seq):\n",
    "            res[i, j+1] = symbol_dict[symbol]\n",
    "        res[i, j+2] = symbol_dict[\"<EOS>\"]\n",
    "    return res, lengths\n",
    "symbol_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0711f-5056-49b4-9a9c-eeeb4b830eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char_map = {v: k for k, v in symbol_dict.items()}\n",
    "\n",
    "def map_to_chars(sequences, int_to_char_map):\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, s in enumerate(seq):\n",
    "            sequences[i][j] = int_to_char_map[s]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c17613-5254-40bc-a0da-c0277e67d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    [0],\n",
    "    [0, 0],\n",
    "    [0, 0, 1],\n",
    "    [1], \n",
    "    [1, 0],\n",
    "    [1, 0, 0]\n",
    "]\n",
    "\n",
    "encoded_s = map_to_chars(sequences, int_to_char_map)\n",
    "x_input, lengths = encode_sequences(encoded_s, symbol_dict, MAX_STRING_LENGTH)\n",
    "x_mask = construct_attn_mask(lengths, train_dataset.maxlen)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**get_forward_dict(x_input, None, x_mask, output_attentions=True))\n",
    "encoded_s, F.softmax(outputs.logits, dim=1), train_dataset.label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5339d16-f161-41ef-bb61-042b8efc0b04",
   "metadata": {},
   "source": [
    "### Now let's look at the attention-matrix. Do we see something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d4857-9ca3-4144-8d28-c238a5f820c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = outputs.attentions[0].detach().numpy()\n",
    "attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bddaf94-114d-4543-b8f9-30148e967d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def label_attention_matrix(attentions, encoded_sequences):\n",
    "    attn_res = None\n",
    "    label_res = list()\n",
    "    for i, seq in enumerate(encoded_sequences):\n",
    "        if attn_res is None:\n",
    "            attn_res = np.transpose(attentions[i, :, :len(seq)], (1, 0, -1))\n",
    "        else:\n",
    "            attn_res = np.vstack((attn_res, np.transpose(attentions[i, :, :len(seq)], (1, 0, -1)) ))\n",
    "        label_res.extend(seq)\n",
    "    return attn_res, label_res\n",
    "\n",
    "attn_matrix, attn_labels = label_attention_matrix(attentions, encoded_s)\n",
    "attn_matrix.shape, len(attn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3408d-d3a4-498a-9218-c96a458535d8",
   "metadata": {},
   "source": [
    "import copy\n",
    "\n",
    "class Stack():\n",
    "    def __init__(self):\n",
    "        self.list = list()\n",
    "    \n",
    "    def push(self, x):\n",
    "        self.list.append(x)\n",
    "\n",
    "    def pop(self):\n",
    "        x = self.list[-1]\n",
    "        del self.list[-1]\n",
    "        return x\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" \".join(self.list)\n",
    "\n",
    "test = Stack()\n",
    "test.push(\"a\")\n",
    "test.push(\"b\")\n",
    "print(test)\n",
    "print(test.pop())\n",
    "print(test)\n",
    "\n",
    "def get_sequences_dfs(alphabet: list, maxlen: int):\n",
    "    \"\"\"\n",
    "    Returns all lists until maxlen.\n",
    "    alphabet is list, maxlen is int\n",
    "    \"\"\"\n",
    "    return \n",
    "    # TODO: if I want an extensive coverage\n",
    "    \n",
    "    res = list()\n",
    "    tracker = [alphabet[0]] * maxlen\n",
    "    print(\"Starting with \", tracker)\n",
    "\n",
    "    alph_to_idx = {s, i for i, s in enumerate(alphabet)}\n",
    "    idx_to_alph = {v: k for k, v in alph_to_idx.items()}\n",
    "    while True:\n",
    "        res.append(copy.copy(tracker))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e01f7-01a0-4d75-a52d-0a2e1daf47d4",
   "metadata": {},
   "source": [
    "## Plot the attention outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf16d4f-171a-44e6-aec7-42a82c0801a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**get_forward_dict(train_input_ids, None, train_attn_mask, output_attentions=True))\n",
    "np_output = output.attentions[0].detach().numpy()\n",
    "np_output.shape, np.argmax(np_output, axis=-1)#np_output[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00912f8-d9dc-4787-9a72-c9c62c4cc688",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e8ca5-d0c3-4fdd-a43f-1745345027ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_heatmaps(sequences, idx, attn_output, dataset, mapping):\n",
    "    \"\"\"\n",
    "    Plots the heatmaps given by the output of the model.\n",
    "\n",
    "    sequence: The sequence given as input to the model.\n",
    "    idx: The index within the output.\n",
    "    output: As returned by the distilbert model.\n",
    "    dataset: The sequence-dataset.\n",
    "    mapping: The mapping to get the sequence as we'd like to have it.\n",
    "    \"\"\"\n",
    "\n",
    "    attn = attn_output[idx]\n",
    "    num_heads = attn.shape[0]\n",
    "    print(\"Number of heads: \", num_heads)\n",
    "\n",
    "    sequence_list, eos_idx = convert_ids_to_chars(mapping=mapping, tensor_1d=sequences[idx], eos_symbol=dataset.EOS)\n",
    "    for i in range(1, num_heads+1):\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        sns.heatmap(attn[i-1, :eos_idx+1, :eos_idx+1], vmin=0, vmax=1)\n",
    "        plt.plot()\n",
    "\n",
    "plot_heatmaps(train_input_ids, 0, np_output, train_dataset, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a776fc-a0cf-462d-ba56-55edf351314d",
   "metadata": {},
   "source": [
    "## All attention outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741af28-8f4b-4a87-bb83-4d496ad98797",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = model(**get_forward_dict(train_input_ids, None, train_attn_mask, output_attentions=True))\n",
    "train_predictions = np.argmax( train_res.logits.detach().numpy(), axis=-1)\n",
    "train_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44426621-3992-437c-b092-2a0c0b0c0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_attention(sequences, attentions, lengths, mapping, highlight_func):\n",
    "    \"\"\"\n",
    "    This function gives the attention at the relevant indices. It stacks all attentions together,\n",
    "    and returns them along with a mask for the attentions of interest and the unmapped symbols of \n",
    "    those.\n",
    "\n",
    "    In the mask, uninteresting fields need a zero. Interesting fields can be numbered by ascending integers.\n",
    "\n",
    "    IMPORTANT: If you want to do e.g. only at TP, ..., then you'll have to pre-filter that. sequences, attentions, \n",
    "    lengths are assumed to have same lengths in first dimension.\n",
    "\n",
    "    sequences: np.array\n",
    "    attentions: np.array\n",
    "    lengths: list with the lengths.\n",
    "    mapping: The casual mapping, from int to char\n",
    "    highlight_func: A function that takes in a sequences and returns the indices of the symbols of interest.\n",
    "    \"\"\"\n",
    "    attn_stack = None\n",
    "    symbols = list()\n",
    "    field_mask = list()\n",
    "    \n",
    "    for attn, seq, l in zip(attentions, sequences, lengths):\n",
    "        if len(attn.shape) > 2:            \n",
    "            # we have multiple heads\n",
    "            attn = np.mean(attn, axis=0) # (seq_length, seq_length)\n",
    "        attn = attn[:l]\n",
    "        attn_stack = attn if attn_stack is None else np.vstack((attn_stack, attn))\n",
    "\n",
    "        seq = list(seq[:l])\n",
    "        \n",
    "        #convert_ids_to_chars(mapping, torch.LongTensor(seq), eos_symbol=None) # for debugging\n",
    "        \n",
    "        symbols.extend(seq)\n",
    "        field_mask.extend(highlight_func(seq, mapping, PROBLEM))\n",
    "    return attn_stack, symbols, field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7789f-ce84-4830-a474-4c721293b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from highlighter_functions.tlt import highlight_tlt\n",
    "\n",
    "attn, symbols, mask = filter_attention(\n",
    "                                        train_input_ids.detach().numpy(), \n",
    "                                        train_res.attentions[0].detach().numpy(),\n",
    "                                        list(np.array(train_dataset.sequence_lengths)),\n",
    "                                        mapping,\n",
    "                                        highlight_tlt\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ae6c3-4fe7-4183-b32e-046235697468",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_of_interest = np.where(np.array(mask)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9b403-644a-4145-bd84-7d66b554754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "attn_transformed = pca.fit_transform(attn[idxs_of_interest])\n",
    "\n",
    "sns.scatterplot(x=attn_transformed[:, 0], y=attn_transformed[:, 1], hue=np.array(mask)[idxs_of_interest])\n",
    "\n",
    "plt.savefig(\"attn_{}.png\".format(PROBLEM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde16f6-3e1a-4a03-a071-29fe6724d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=attn_transformed[:, 0], y=attn_transformed[:, 2], hue=np.array(mask)[idxs_of_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad1f9f-1271-432c-8827-dc61dd98a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=attn_transformed[:, 1], y=attn_transformed[:, 2], hue=np.array(mask)[idxs_of_interest])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555aa060-446d-435a-9577-4ac5856183cb",
   "metadata": {},
   "source": [
    "## Attention outputs of TP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f4bd9b-b07e-46e4-be42-356776c88584",
   "metadata": {},
   "source": [
    "### Get the relevant fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e044500-7719-4fc1-b6f9-af4f61fce54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = model(**get_forward_dict(test_input_ids, None, test_attn_mask))\n",
    "test_predictions = np.argmax( test_res.logits.detach().numpy(), axis=-1)\n",
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fdf38e-eab0-451d-9cad-72ad9e4a945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the positive label\n",
    "train_dataset.label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb777937-273f-45bb-9eb7-97303fb1d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_LABEL = train_dataset.label_dict[\"1\"]\n",
    "NEGATIVE_LABEL = train_dataset.label_dict[\"0\"]\n",
    "\n",
    "positive_prediction_idxs = np.where(test_predictions==POSITIVE_LABEL)[0]\n",
    "positive_prediction_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70409f02-c773-4c67-adb8-9e559447a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_label_idxs = np.where(np.array(test_labels)==POSITIVE_LABEL)[0]\n",
    "positive_label_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a761355-ad16-48d2-b79f-995f67f7a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = convert_ids_to_chars(mapping, test_input_ids[0], eos_symbol=train_dataset.PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f26ed-32a8-4ca3-b77a-760fb42f6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_indices = set(list(positive_prediction_idxs)).intersection(set(list(positive_label_idxs)))\n",
    "len(TP_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1778b42-0f60-405e-97c6-ad5ab05e9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_res = model(**get_forward_dict(test_input_ids[list(TP_indices)], None, test_attn_mask[list(TP_indices)], output_attentions=True))\n",
    "tp_res.attentions[0].detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f8e9d-9091-4a3c-afa1-a4339a689089",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_attn, tp_symbols, tp_mask = filter_attention(\n",
    "                                                               test_input_ids[list(TP_indices)].detach().numpy(), \n",
    "                                                               tp_res.attentions[0].detach().numpy(),\n",
    "                                                               list(np.array(test_dataset.sequence_lengths)[list(TP_indices)]),\n",
    "                                                               mapping,\n",
    "                                                               highlight_tlt\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f362de-92a6-4dc7-a319-f9622a899fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_attn.shape, len(tp_symbols), len(tp_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8779d2-673a-436b-99c8-77e886395251",
   "metadata": {},
   "source": [
    "### Get information about that stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8be0c-1a63-4583-8d39-80f9dd591d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_of_interest = np.where(np.array(tp_mask)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1654a39-916f-4634-a046-35c1639c54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "attn_transformed = pca.fit_transform(tp_attn[idxs_of_interest])\n",
    "\n",
    "sns.scatterplot(x=attn_transformed[:, 0], y=attn_transformed[:, 1], hue=np.array(tp_mask)[idxs_of_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e08ac-a8fd-4ea4-bff7-8a8e89252535",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_idx = np.where(np.array(tp_mask)==0)[0]\n",
    "one_idx = np.where(np.array(tp_mask)==1)[0]\n",
    "two_idx = np.where(np.array(tp_mask)==2)[0]\n",
    "\n",
    "sns.scatterplot(x=attn_transformed[zero_idx, 0], y=attn_transformed[zero_idx, 1], hue=np.array(tp_mask)[zero_idx], palette=\"mako\")\n",
    "sns.scatterplot(x=attn_transformed[two_idx, 0], y=attn_transformed[two_idx, 1], hue=np.array(tp_mask)[two_idx])\n",
    "sns.scatterplot(x=attn_transformed[one_idx, 0], y=attn_transformed[one_idx, 1], hue=np.array(tp_mask)[one_idx], palette=\"rocket\")\n",
    "#plt.savefig(\"rep.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5275e2-7416-4614-92df-0ac319a1bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "attn_transformed = tsne.fit_transform(tp_attn[idxs_of_interest])\n",
    "\n",
    "sns.scatterplot(x=attn_transformed[:, 0], y=attn_transformed[:, 1], hue=np.array(tp_mask)[idxs_of_interest])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d766e20-bf81-48a4-a817-493a6be298a5",
   "metadata": {},
   "source": [
    "## Test how well a classifier can separate the point clouds\n",
    "\n",
    "### 1. Try a nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890086c-1802-49c1-af63-06212ed802c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors()\n",
    "nn = nn.fit(tp_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d729d2-bd90-4c09-a7e2-33c904c8ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, neigh_idxs = nn.kneighbors(tp_attn)\n",
    "distances.shape, neigh_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bacdef-5c32-430a-9a92-f1f7f713ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances[:5], neigh_idxs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec0999-73bd-4cdf-ab1c-a609888d3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = neigh_idxs[one_idx, 1]\n",
    "np.array(tp_mask)[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b111cc-c90d-425b-b930-467aed045f8d",
   "metadata": {},
   "source": [
    "## Attn outputs of TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e01a9-0228-4abd-8395-6669cf7f1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prediction_idxs = np.where(test_predictions==NEGATIVE_LABEL)[0]\n",
    "negative_label_idxs = np.where(np.array(test_labels)==NEGATIVE_LABEL)[0]\n",
    "\n",
    "negative_prediction_idxs, negative_label_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0a07f-abe5-4127-a84a-20d7af3885fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN_indices = set(list(negative_prediction_idxs)).intersection(set(list(negative_label_idxs)))\n",
    "\n",
    "tn_res = model(**get_forward_dict(test_input_ids[list(TN_indices)], None, test_attn_mask[list(TN_indices)], output_attentions=True))\n",
    "tn_res.attentions[0].detach().numpy().shape\n",
    "\n",
    "len(TN_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da2cd1-27be-4ac3-aeb6-d7b516ed470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_char_highlighter_func(seq, mapping):\n",
    "    \"\"\"\n",
    "    Only highlights first char of the sequence.\n",
    "\n",
    "    seq: np.array or list\n",
    "    \"\"\"\n",
    "    res = [0] * len(seq)\n",
    "    res[1] = 1\n",
    "    return res\n",
    "\n",
    "tn_attn, tn_symbols, tn_mask = filter_attention(\n",
    "                                                               test_input_ids[list(TN_indices)].detach().numpy(), \n",
    "                                                               tn_res.attentions[0].detach().numpy(),\n",
    "                                                               list(np.array(test_dataset.sequence_lengths)[list(TN_indices)]),\n",
    "                                                               mapping,\n",
    "                                                               first_char_highlighter_func\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14bc109-e7f5-4f58-bde4-cebcb10f6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "tp_attn_transformed = pca.fit_transform(tp_attn)\n",
    "tn_attn_transformed = pca.fit_transform(tn_attn)\n",
    "\n",
    "tp_one_idx = np.where(np.array(tp_mask)==1)[0]\n",
    "tn_one_idx = np.where(np.array(tn_mask)==1)[0]\n",
    "\n",
    "sns.scatterplot(x=tn_attn_transformed[tn_one_idx, 0], y=tn_attn_transformed[tn_one_idx, 1], hue=np.array(tn_mask)[tn_one_idx], palette=\"rocket\")\n",
    "sns.scatterplot(x=tp_attn_transformed[tp_one_idx, 0], y=tp_attn_transformed[tp_one_idx, 1], hue=np.array(tp_mask)[tp_one_idx], palette=\"mako\")\n",
    "#plt.savefig(\"rep.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb22bf7-a878-4907-96be-68b9d04aee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_a_idxs = np.where(np.logical_and(np.array(tn_symbols)==train_dataset.symbol_dict[\"a\"], np.array(tn_mask) == 1))[0]\n",
    "tn_no_a_idxs = np.where(np.logical_and(np.array(tn_symbols)!=train_dataset.symbol_dict[\"a\"], np.array(tn_mask) == 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926bdb5-2845-4244-8acf-f6f4b81dbc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=tn_attn_transformed[tn_no_a_idxs, 0], y=tn_attn_transformed[tn_no_a_idxs, 1], hue=np.array([1]*len(tn_no_a_idxs)), palette=\"mako\")\n",
    "sns.scatterplot(x=tp_attn_transformed[tp_one_idx, 0], y=tp_attn_transformed[tp_one_idx, 1], hue=np.array([2]*len(tp_one_idx)), palette=\"husl\")\n",
    "sns.scatterplot(x=tn_attn_transformed[tn_a_idxs, 0], y=tn_attn_transformed[tn_a_idxs, 1], hue=np.array([0]*len(tn_a_idxs)), palette=\"rocket\")\n",
    "plt.savefig(\"rep.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56bb26a-ac52-4a51-8890-8a59cbcce845",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0]*len(tn_a_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4238a-a878-4b5f-bb89-e2a722280f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_a_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dab1a2-9cd0-45a5-963b-7c38defcf849",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_mask == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48481f09-00a0-4955-97dd-995d8bb42afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
