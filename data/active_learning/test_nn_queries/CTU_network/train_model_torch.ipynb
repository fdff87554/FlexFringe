{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ccbdb0b-0508-494a-a8a8-c0d4ff724e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "VALIDATION_SPLIT = 0.02\n",
    "\n",
    "train_data_path = \"train_data.pk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c059c361-4927-4f3d-9988-f0dc5ff78596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 400000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pk\n",
    "\n",
    "train_data = pk.load(open(train_data_path, \"rb\"))\n",
    "X_train = train_data[\"X\"]\n",
    "X_train_oh = train_data[\"X_oh\"]\n",
    "\n",
    "alphabet_size = train_data[\"alphabet_size\"]\n",
    "\n",
    "len(X_train), len(X_train_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "401c7fa2-7629-4673-81bd-a9784509bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_train_oh, X_val_oh = train_test_split(X_train, X_train_oh, test_size=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d4a0f2-7094-462b-b15f-af6b316755db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(alphabet_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=False)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, alphabet_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm(x)[0]\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.linear(x))\n",
    "        x = F.softmax(self.output(x))\n",
    "        return x\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X_train, X_train_oh):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.X_train_oh = X_train_oh\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X_train[idx]), torch.tensor(self.X_train_oh[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da40c3-9ae5-4c54-8ce5-e96d2060f2cd",
   "metadata": {},
   "source": [
    "def get_model(input_shape, alphabet_size):\n",
    "    OUTPUT_DIM = alphabet_size # sigmoid output\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Embedding(alphabet_size, 20)(input_layer)\n",
    "    x = LSTM(64)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(OUTPUT_DIM, activation=\"softmax\")(x)\n",
    "    model = Model(input_layer, x)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"categorical_crossentropy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7051a418-ef36-44dc-a5fc-8a2dbb476c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(alphabet_size, 20, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fcc862-beda-40a9-9f05-46c3520e4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SequenceDataset(X_train, X_train_oh)\n",
    "train_dataloader = DataLoader(train_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "649a0fba-d6d4-45db-85b4-cd12e4088aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  batch 0 loss: 4.253195762634277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19014/1730588549.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.output(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 4.146942563524034\n",
      "  batch 2000 loss: 4.149731180683407\n",
      "  batch 3000 loss: 4.151008807408341\n",
      "  batch 4000 loss: 4.144310755690301\n",
      "  batch 5000 loss: 4.143533850642614\n",
      "  batch 6000 loss: 4.149032811476766\n",
      "  batch 7000 loss: 4.15057930670868\n",
      "  batch 8000 loss: 4.143346159605184\n",
      "  batch 9000 loss: 4.1491986294334575\n",
      "  batch 10000 loss: 4.1400208547669175\n",
      "  batch 11000 loss: 4.140362903287863\n",
      "  batch 12000 loss: 4.14102695297467\n",
      "  batch 13000 loss: 4.146002909694655\n",
      "  batch 14000 loss: 4.14032620810824\n",
      "  batch 15000 loss: 4.150905581521444\n",
      "  batch 16000 loss: 4.146035178802957\n",
      "  batch 17000 loss: 4.154010558908638\n",
      "  batch 18000 loss: 4.142508838471031\n",
      "  batch 19000 loss: 4.146937599236313\n",
      "  batch 20000 loss: 4.150960569770776\n",
      "  batch 21000 loss: 4.150744295973952\n",
      "  batch 22000 loss: 4.146199816968664\n",
      "  batch 23000 loss: 4.142450999781576\n",
      "  batch 24000 loss: 4.142107960464858\n",
      "  batch 25000 loss: 4.136277543129467\n",
      "  batch 26000 loss: 4.152200092281713\n",
      "  batch 27000 loss: 4.150654009204929\n",
      "  batch 28000 loss: 4.142566254307699\n",
      "  batch 29000 loss: 4.1423152355082085\n",
      "  batch 30000 loss: 4.141873104562905\n",
      "  batch 31000 loss: 4.144949855711967\n",
      "  batch 32000 loss: 4.142825497547489\n",
      "  batch 33000 loss: 4.139095534112078\n",
      "  batch 34000 loss: 4.153627702678777\n",
      "  batch 35000 loss: 4.148251005076574\n",
      "  batch 36000 loss: 4.140021877937194\n",
      "  batch 37000 loss: 4.136443851382441\n",
      "  batch 38000 loss: 4.1462373665110634\n",
      "  batch 39000 loss: 4.143943357641358\n",
      "  batch 40000 loss: 4.150135004505972\n",
      "  batch 41000 loss: 4.145103988331479\n",
      "  batch 42000 loss: 4.147753850905823\n",
      "  batch 43000 loss: 4.146313515985859\n",
      "  batch 44000 loss: 4.138066564212425\n",
      "  batch 45000 loss: 4.140915808338312\n",
      "  batch 46000 loss: 4.148784545793217\n",
      "  batch 47000 loss: 4.141118746838969\n",
      "  batch 48000 loss: 4.146210824207005\n",
      "  batch 49000 loss: 4.1412070093803965\n",
      "  batch 50000 loss: 4.145404172980373\n",
      "  batch 51000 loss: 4.1494841247120915\n",
      "  batch 52000 loss: 4.146496473282397\n",
      "  batch 53000 loss: 4.142049512125505\n",
      "  batch 54000 loss: 4.141967961163234\n",
      "  batch 55000 loss: 4.135526868285041\n",
      "  batch 56000 loss: 4.146190412459229\n",
      "  batch 57000 loss: 4.14403606375998\n",
      "  batch 58000 loss: 4.147498537946472\n",
      "  batch 59000 loss: 4.142501448594441\n",
      "  batch 60000 loss: 4.137923457008423\n",
      "  batch 61000 loss: 4.142789931670874\n",
      "  batch 62000 loss: 4.145674001839185\n",
      "  batch 63000 loss: 4.143252858131018\n",
      "  batch 64000 loss: 4.138666897382518\n",
      "  batch 65000 loss: 4.134146731519338\n",
      "  batch 66000 loss: 4.1325872140971125\n",
      "  batch 67000 loss: 4.1461042489951305\n",
      "  batch 68000 loss: 4.15104709224893\n",
      "  batch 69000 loss: 4.149952584666917\n",
      "  batch 70000 loss: 4.141319005902968\n",
      "  batch 71000 loss: 4.143777721185756\n",
      "  batch 72000 loss: 4.139749421571243\n",
      "  batch 73000 loss: 4.141455914084113\n",
      "  batch 74000 loss: 4.135440237066962\n",
      "  batch 75000 loss: 4.145711143088235\n",
      "  batch 76000 loss: 4.139324737410108\n",
      "  batch 77000 loss: 4.137714785625719\n",
      "  batch 78000 loss: 4.138656308242767\n",
      "  batch 79000 loss: 4.143941342146475\n",
      "  batch 80000 loss: 4.139635035285083\n",
      "  batch 81000 loss: 4.15033565238064\n",
      "  batch 82000 loss: 4.139964574685957\n",
      "  batch 83000 loss: 4.1454117813290985\n",
      "  batch 84000 loss: 4.141020712463783\n",
      "  batch 85000 loss: 4.145940171110991\n",
      "  batch 86000 loss: 4.13660058852277\n",
      "  batch 87000 loss: 4.145062053362405\n",
      "  batch 88000 loss: 4.145616128316429\n",
      "  batch 89000 loss: 4.142903470565124\n",
      "  batch 90000 loss: 4.147392501214777\n",
      "  batch 91000 loss: 4.146224341727748\n",
      "  batch 92000 loss: 4.148517066997107\n",
      "  batch 93000 loss: 4.145594983524627\n",
      "  batch 94000 loss: 4.1489976295209905\n",
      "  batch 95000 loss: 4.1454949985265745\n",
      "  batch 96000 loss: 4.143815304414068\n",
      "  batch 97000 loss: 4.14445662994204\n",
      "  batch 98000 loss: 4.141854643341811\n",
      "  batch 99000 loss: 4.14940953875238\n",
      "  batch 100000 loss: 4.14525941532164\n",
      "  batch 101000 loss: 4.139380970140654\n",
      "  batch 102000 loss: 4.136383709238693\n",
      "  batch 103000 loss: 4.132618377495535\n",
      "  batch 104000 loss: 4.141064442893593\n",
      "  batch 105000 loss: 4.141174507252197\n",
      "  batch 106000 loss: 4.148766495008696\n",
      "  batch 107000 loss: 4.131404784311717\n",
      "  batch 108000 loss: 4.144469066318753\n",
      "  batch 109000 loss: 4.148049078138193\n",
      "  batch 110000 loss: 4.139000432650364\n",
      "  batch 111000 loss: 4.145533214141986\n",
      "  batch 112000 loss: 4.144245984686867\n",
      "  batch 113000 loss: 4.139416653498734\n",
      "  batch 114000 loss: 4.138372918945008\n",
      "  batch 115000 loss: 4.147981479335911\n",
      "  batch 116000 loss: 4.141227797858129\n",
      "  batch 117000 loss: 4.137356029856519\n",
      "  batch 118000 loss: 4.1336205585439405\n",
      "  batch 119000 loss: 4.137051221754641\n",
      "  batch 120000 loss: 4.14952262022965\n",
      "  batch 121000 loss: 4.145892858239737\n",
      "  batch 122000 loss: 4.138941843822512\n",
      "  batch 123000 loss: 4.151601069722387\n",
      "  batch 124000 loss: 4.140706124977631\n",
      "  batch 125000 loss: 4.141383392979884\n",
      "  batch 126000 loss: 4.145869768868436\n",
      "  batch 127000 loss: 4.133241329012138\n",
      "  batch 128000 loss: 4.145648781310795\n",
      "  batch 129000 loss: 4.139347415488933\n",
      "  batch 130000 loss: 4.149460790191481\n",
      "  batch 131000 loss: 4.139966510092131\n",
      "  batch 132000 loss: 4.140544411710277\n",
      "  batch 133000 loss: 4.149020348480855\n",
      "  batch 134000 loss: 4.1421153742121914\n",
      "  batch 135000 loss: 4.14679323541685\n",
      "  batch 136000 loss: 4.140032561991315\n",
      "  batch 137000 loss: 4.145559546654334\n",
      "  batch 138000 loss: 4.138793722832206\n",
      "  batch 139000 loss: 4.147992373205498\n",
      "  batch 140000 loss: 4.1484648615132125\n",
      "  batch 141000 loss: 4.134420408720325\n",
      "  batch 142000 loss: 4.147503918171344\n",
      "  batch 143000 loss: 4.137069785851833\n",
      "  batch 144000 loss: 4.14395436295668\n",
      "  batch 145000 loss: 4.1390115216367125\n",
      "  batch 146000 loss: 4.1463715445847305\n",
      "  batch 147000 loss: 4.142129590979849\n",
      "  batch 148000 loss: 4.14459763699667\n",
      "  batch 149000 loss: 4.140380742861285\n",
      "  batch 150000 loss: 4.143709805328591\n",
      "  batch 151000 loss: 4.139110763348231\n",
      "  batch 152000 loss: 4.1457662389426515\n",
      "  batch 153000 loss: 4.1448083606633315\n",
      "  batch 154000 loss: 4.140872008688531\n",
      "  batch 155000 loss: 4.140527820006441\n",
      "  batch 156000 loss: 4.129815659493754\n",
      "  batch 157000 loss: 4.140838335613046\n",
      "  batch 158000 loss: 4.1413283188920795\n",
      "  batch 159000 loss: 4.137234247223482\n",
      "  batch 160000 loss: 4.138979723305982\n",
      "  batch 161000 loss: 4.143277426247884\n",
      "  batch 162000 loss: 4.142660333058942\n",
      "  batch 163000 loss: 4.134424596861242\n",
      "  batch 164000 loss: 4.134660263435689\n",
      "  batch 165000 loss: 4.142680159259255\n",
      "  batch 166000 loss: 4.143428772831967\n",
      "  batch 167000 loss: 4.142267831674673\n",
      "  batch 168000 loss: 4.146043074418201\n",
      "  batch 169000 loss: 4.140536264085649\n",
      "  batch 170000 loss: 4.133524892833738\n",
      "  batch 171000 loss: 4.139344416660872\n",
      "  batch 172000 loss: 4.137215819282604\n",
      "  batch 173000 loss: 4.130904346397066\n",
      "  batch 174000 loss: 4.145011613342974\n",
      "  batch 175000 loss: 4.136150180679193\n",
      "  batch 176000 loss: 4.143965710263903\n",
      "  batch 177000 loss: 4.137603234938422\n",
      "  batch 178000 loss: 4.1517126039389405\n",
      "  batch 179000 loss: 4.139984420796603\n",
      "  batch 180000 loss: 4.146251495862368\n",
      "  batch 181000 loss: 4.138762821303232\n",
      "  batch 182000 loss: 4.148131126974419\n",
      "  batch 183000 loss: 4.141024448697254\n",
      "  batch 184000 loss: 4.132048885869616\n",
      "  batch 185000 loss: 4.138877374034938\n",
      "  batch 186000 loss: 4.138698031657934\n",
      "  batch 187000 loss: 4.144621222310717\n",
      "  batch 188000 loss: 4.142019295519768\n",
      "  batch 189000 loss: 4.142045222206192\n",
      "  batch 190000 loss: 4.136751920371346\n",
      "  batch 191000 loss: 4.134879624477711\n",
      "  batch 192000 loss: 4.144146687337603\n",
      "  batch 193000 loss: 4.140902125402995\n",
      "  batch 194000 loss: 4.138004633808497\n",
      "  batch 195000 loss: 4.138921770470971\n",
      "  batch 196000 loss: 4.128878795403426\n",
      "  batch 197000 loss: 4.135627115599464\n",
      "  batch 198000 loss: 4.131506598707553\n",
      "  batch 199000 loss: 4.133490901323456\n",
      "  batch 200000 loss: 4.143026648102021\n",
      "  batch 201000 loss: 4.132989486195101\n",
      "  batch 202000 loss: 4.138415612520234\n",
      "  batch 203000 loss: 4.148305198737711\n",
      "  batch 204000 loss: 4.1346701857960575\n",
      "  batch 205000 loss: 4.1288657857230175\n",
      "  batch 206000 loss: 4.1330670946489665\n",
      "  batch 207000 loss: 4.142819814866962\n",
      "  batch 208000 loss: 4.145741341109321\n",
      "  batch 209000 loss: 4.134534716955639\n",
      "  batch 210000 loss: 4.139421186398373\n",
      "  batch 211000 loss: 4.138335546103028\n",
      "  batch 212000 loss: 4.140983647679198\n",
      "  batch 213000 loss: 4.139632209544832\n",
      "  batch 214000 loss: 4.144316578317772\n",
      "  batch 215000 loss: 4.130247926687834\n",
      "  batch 216000 loss: 4.13080483525666\n",
      "  batch 217000 loss: 4.128145682555974\n",
      "  batch 218000 loss: 4.134205713066912\n",
      "  batch 219000 loss: 4.1379384488607895\n",
      "  batch 220000 loss: 4.139545542240144\n",
      "  batch 221000 loss: 4.141711442158804\n",
      "  batch 222000 loss: 4.143462454731896\n",
      "  batch 223000 loss: 4.135173841063799\n",
      "  batch 224000 loss: 4.137348590748653\n",
      "  batch 225000 loss: 4.129436367025625\n",
      "  batch 226000 loss: 4.132149021250067\n",
      "  batch 227000 loss: 4.1221276266710625\n",
      "  batch 228000 loss: 4.130905212489481\n",
      "  batch 229000 loss: 4.140510634360893\n",
      "  batch 230000 loss: 4.139047903275027\n",
      "  batch 231000 loss: 4.129561614482696\n",
      "  batch 232000 loss: 4.133864021584478\n",
      "  batch 233000 loss: 4.127645241367818\n",
      "  batch 234000 loss: 4.130558478994073\n",
      "  batch 235000 loss: 4.132234281606651\n",
      "  batch 236000 loss: 4.138826670575624\n",
      "  batch 237000 loss: 4.127934832690341\n",
      "  batch 238000 loss: 4.139247617912117\n",
      "  batch 239000 loss: 4.135104689915512\n",
      "  batch 240000 loss: 4.13266767611684\n",
      "  batch 241000 loss: 4.1331562229447565\n",
      "  batch 242000 loss: 4.134780700031554\n",
      "  batch 243000 loss: 4.137114430767238\n",
      "  batch 244000 loss: 4.135156187024386\n",
      "  batch 245000 loss: 4.142622340212087\n",
      "  batch 246000 loss: 4.139197398122308\n",
      "  batch 247000 loss: 4.13386716877836\n",
      "  batch 248000 loss: 4.1379992142731465\n",
      "  batch 249000 loss: 4.13625927482229\n",
      "  batch 250000 loss: 4.123433906930868\n",
      "  batch 251000 loss: 4.131626057271162\n",
      "  batch 252000 loss: 4.126553390179979\n",
      "  batch 253000 loss: 4.123606445518768\n",
      "  batch 254000 loss: 4.130793353143248\n",
      "  batch 255000 loss: 4.126554157862878\n",
      "  batch 256000 loss: 4.129368141747247\n",
      "  batch 257000 loss: 4.134847718694715\n",
      "  batch 258000 loss: 4.132560089600728\n",
      "  batch 259000 loss: 4.133088129142924\n",
      "  batch 260000 loss: 4.127545251119493\n",
      "  batch 261000 loss: 4.134929842882087\n",
      "  batch 262000 loss: 4.1352766475337965\n",
      "  batch 263000 loss: 4.135634781322695\n",
      "  batch 264000 loss: 4.134926683724936\n",
      "  batch 265000 loss: 4.144643572364429\n",
      "  batch 266000 loss: 4.129066093804379\n",
      "  batch 267000 loss: 4.134651349071661\n",
      "  batch 268000 loss: 4.126836208523759\n",
      "  batch 269000 loss: 4.124272445889316\n",
      "  batch 270000 loss: 4.142442336488857\n",
      "  batch 271000 loss: 4.128690000831722\n",
      "  batch 272000 loss: 4.1376420345756735\n",
      "  batch 273000 loss: 4.127514426879934\n",
      "  batch 274000 loss: 4.1262855972665715\n",
      "  batch 275000 loss: 4.131037839813663\n",
      "  batch 276000 loss: 4.135325799004963\n",
      "  batch 277000 loss: 4.13366181943995\n",
      "  batch 278000 loss: 4.136824658780393\n",
      "  batch 279000 loss: 4.120630752935431\n",
      "  batch 280000 loss: 4.139713143264522\n",
      "  batch 281000 loss: 4.138134168007156\n",
      "  batch 282000 loss: 4.130955213494893\n",
      "  batch 283000 loss: 4.1208889212247035\n",
      "  batch 284000 loss: 4.1353481973557775\n",
      "  batch 285000 loss: 4.1375196243635335\n",
      "  batch 286000 loss: 4.131969119395634\n",
      "  batch 287000 loss: 4.136364749834654\n",
      "  batch 288000 loss: 4.124683856311351\n",
      "  batch 289000 loss: 4.12789712688453\n",
      "  batch 290000 loss: 4.127415272217087\n",
      "  batch 291000 loss: 4.133767256282676\n",
      "  batch 292000 loss: 4.131194214548123\n",
      "  batch 293000 loss: 4.12458347082293\n",
      "  batch 294000 loss: 4.141289326828899\n",
      "  batch 295000 loss: 4.1315411407116684\n",
      "  batch 296000 loss: 4.139243305450806\n",
      "  batch 297000 loss: 4.137694188916168\n",
      "  batch 298000 loss: 4.135691409942393\n",
      "  batch 299000 loss: 4.131424055714502\n",
      "  batch 300000 loss: 4.135501976692135\n",
      "  batch 301000 loss: 4.136038899003738\n",
      "  batch 302000 loss: 4.135646825709596\n",
      "  batch 303000 loss: 4.137060354105684\n",
      "  batch 304000 loss: 4.128629749646534\n",
      "  batch 305000 loss: 4.125909548314232\n",
      "  batch 306000 loss: 4.1345088287555845\n",
      "  batch 307000 loss: 4.129020971785164\n",
      "  batch 308000 loss: 4.135906644594305\n",
      "  batch 309000 loss: 4.134577308162047\n",
      "  batch 310000 loss: 4.135026635071725\n",
      "  batch 311000 loss: 4.136540787476484\n",
      "  batch 312000 loss: 4.133538928732729\n",
      "  batch 313000 loss: 4.143864920762447\n",
      "  batch 314000 loss: 4.128537214847404\n",
      "  batch 315000 loss: 4.140021355048816\n",
      "  batch 316000 loss: 4.121721210069647\n",
      "  batch 317000 loss: 4.134542087378885\n",
      "  batch 318000 loss: 4.136155678310804\n",
      "  batch 319000 loss: 4.1328162471254695\n",
      "  batch 320000 loss: 4.134298057400822\n",
      "  batch 321000 loss: 4.141976480296526\n",
      "  batch 322000 loss: 4.135181515277877\n",
      "  batch 323000 loss: 4.126406973086548\n",
      "  batch 324000 loss: 4.13139302372625\n",
      "  batch 325000 loss: 4.1349449000809155\n",
      "  batch 326000 loss: 4.132146125465088\n",
      "  batch 327000 loss: 4.138356190424494\n",
      "  batch 328000 loss: 4.12954288011681\n",
      "  batch 329000 loss: 4.124896275293263\n",
      "  batch 330000 loss: 4.122666083581879\n",
      "  batch 331000 loss: 4.132187549864531\n",
      "  batch 332000 loss: 4.126424393806311\n",
      "  batch 333000 loss: 4.1369037836446925\n",
      "  batch 334000 loss: 4.140124798701026\n",
      "  batch 335000 loss: 4.131933241685948\n",
      "  batch 336000 loss: 4.138399817259867\n",
      "  batch 337000 loss: 4.131092638363501\n",
      "  batch 338000 loss: 4.130598622589761\n",
      "  batch 339000 loss: 4.1379097569999175\n",
      "  batch 340000 loss: 4.130715546836275\n",
      "  batch 341000 loss: 4.1294694497538345\n",
      "  batch 342000 loss: 4.125787322969146\n",
      "  batch 343000 loss: 4.1282268472624555\n",
      "  batch 344000 loss: 4.129074340173569\n",
      "  batch 345000 loss: 4.137024868635335\n",
      "  batch 346000 loss: 4.128445811186778\n",
      "  batch 347000 loss: 4.137132407622631\n",
      "  batch 348000 loss: 4.136858950401798\n",
      "  batch 349000 loss: 4.133368252400715\n",
      "  batch 350000 loss: 4.132291680019551\n",
      "  batch 351000 loss: 4.124929029662318\n",
      "  batch 352000 loss: 4.138317938703839\n",
      "  batch 353000 loss: 4.136567314783939\n",
      "  batch 354000 loss: 4.134893943380587\n",
      "  batch 355000 loss: 4.125864579636238\n",
      "  batch 356000 loss: 4.133874828022537\n",
      "  batch 357000 loss: 4.1393308101434885\n",
      "  batch 358000 loss: 4.136284570674467\n",
      "  batch 359000 loss: 4.1307338879227675\n",
      "  batch 360000 loss: 4.1391611210281125\n",
      "  batch 361000 loss: 4.135079776139331\n",
      "  batch 362000 loss: 4.133082582853987\n",
      "  batch 363000 loss: 4.125138392335204\n",
      "  batch 364000 loss: 4.134653446280234\n",
      "  batch 365000 loss: 4.128248448651605\n",
      "  batch 366000 loss: 4.139460746902411\n",
      "  batch 367000 loss: 4.1330484802982985\n",
      "  batch 368000 loss: 4.1204092729308375\n",
      "  batch 369000 loss: 4.1409326626141905\n",
      "  batch 370000 loss: 4.1327062878995235\n",
      "  batch 371000 loss: 4.133284814075785\n",
      "  batch 372000 loss: 4.120167350480415\n",
      "  batch 373000 loss: 4.133064591120954\n",
      "  batch 374000 loss: 4.122136151000945\n",
      "  batch 375000 loss: 4.13362987696222\n",
      "  batch 376000 loss: 4.1323016456564226\n",
      "  batch 377000 loss: 4.131005589067935\n",
      "  batch 378000 loss: 4.140564455659099\n",
      "  batch 379000 loss: 4.134208139654365\n",
      "  batch 380000 loss: 4.139704795063494\n",
      "  batch 381000 loss: 4.135363376599884\n",
      "  batch 382000 loss: 4.132899555513357\n",
      "  batch 383000 loss: 4.139426453920462\n",
      "  batch 384000 loss: 4.134200244960275\n",
      "  batch 385000 loss: 4.136682976761374\n",
      "  batch 386000 loss: 4.137948824623674\n",
      "  batch 387000 loss: 4.1277626738548285\n",
      "  batch 388000 loss: 4.130660292166291\n",
      "  batch 389000 loss: 4.132045939988796\n",
      "  batch 390000 loss: 4.127740669650117\n",
      "  batch 391000 loss: 4.13211167202579\n",
      "Epoch:  1\n",
      "  batch 0 loss: 4.136182031630026\n",
      "  batch 1000 loss: 4.12728882837295\n",
      "  batch 2000 loss: 4.130609564483166\n",
      "  batch 3000 loss: 4.132054936600215\n",
      "  batch 4000 loss: 4.12586944624764\n",
      "  batch 5000 loss: 4.132498288799073\n",
      "  batch 6000 loss: 4.120162850833304\n",
      "  batch 7000 loss: 4.138126326311717\n",
      "  batch 8000 loss: 4.129856047598887\n",
      "  batch 9000 loss: 4.121672731126433\n",
      "  batch 10000 loss: 4.131276029117182\n",
      "  batch 11000 loss: 4.125629116705146\n",
      "  batch 12000 loss: 4.126939116420165\n",
      "  batch 13000 loss: 4.132305659843758\n",
      "  batch 14000 loss: 4.132503171200094\n",
      "  batch 15000 loss: 4.1217423661650505\n",
      "  batch 16000 loss: 4.132130373694678\n",
      "  batch 17000 loss: 4.130385246722991\n",
      "  batch 18000 loss: 4.119683669236574\n",
      "  batch 19000 loss: 4.123290754946073\n",
      "  batch 20000 loss: 4.1344903429063695\n",
      "  batch 21000 loss: 4.130736009069464\n",
      "  batch 22000 loss: 4.12907701634024\n",
      "  batch 23000 loss: 4.127272893520559\n",
      "  batch 24000 loss: 4.134343695398433\n",
      "  batch 25000 loss: 4.136094950832072\n",
      "  batch 26000 loss: 4.131257256034411\n",
      "  batch 27000 loss: 4.130229212712502\n",
      "  batch 28000 loss: 4.120641121129552\n",
      "  batch 29000 loss: 4.130456265186179\n",
      "  batch 30000 loss: 4.129355289835098\n",
      "  batch 31000 loss: 4.137114029218157\n",
      "  batch 32000 loss: 4.140359974949647\n",
      "  batch 33000 loss: 4.13854873883796\n",
      "  batch 34000 loss: 4.13623442280582\n",
      "  batch 35000 loss: 4.129237236967531\n",
      "  batch 36000 loss: 4.133803004086864\n",
      "  batch 37000 loss: 4.1278950723890135\n",
      "  batch 38000 loss: 4.142005283113322\n",
      "  batch 39000 loss: 4.12508210441741\n",
      "  batch 40000 loss: 4.134544356043771\n",
      "  batch 41000 loss: 4.135923984079651\n",
      "  batch 42000 loss: 4.135481909482283\n",
      "  batch 43000 loss: 4.131374890247623\n",
      "  batch 44000 loss: 4.12984522587097\n",
      "  batch 45000 loss: 4.127192145340731\n",
      "  batch 46000 loss: 4.125799039467413\n",
      "  batch 47000 loss: 4.139272358711305\n",
      "  batch 48000 loss: 4.134839385349821\n",
      "  batch 49000 loss: 4.1375456706305975\n",
      "  batch 50000 loss: 4.136977762818335\n",
      "  batch 51000 loss: 4.1324289222338\n",
      "  batch 52000 loss: 4.135553838239655\n",
      "  batch 53000 loss: 4.1393597541459215\n",
      "  batch 54000 loss: 4.128963679062596\n",
      "  batch 55000 loss: 4.131764650674664\n",
      "  batch 56000 loss: 4.147167420319837\n",
      "  batch 57000 loss: 4.125485473932693\n",
      "  batch 58000 loss: 4.122753621444197\n",
      "  batch 59000 loss: 4.123009795059939\n",
      "  batch 60000 loss: 4.1310712613794704\n",
      "  batch 61000 loss: 4.131390084493162\n",
      "  batch 62000 loss: 4.133075523190064\n",
      "  batch 63000 loss: 4.136887550355076\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#optimizer.zero_grad()\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, (x, x_oh) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m---> 12\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#print(list(x.size()), list(x_oh.size()))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#break\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(x, dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/decorators.py:47\u001b[0m, in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     45\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:294\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 294\u001b[0m     (filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mskipfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_impl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_wrapped_call_impl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m DONT_WRAP_FILES\n\u001b[1;32m    299\u001b[0m ):\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# call to a builtin without a frame for us to capture\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     fn \u001b[38;5;241m=\u001b[39m external_utils\u001b[38;5;241m.\u001b[39mwrap_inline(fn)\n\u001b[1;32m    303\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "divisor = 0.\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"Epoch: \", i)\n",
    "    #optimizer.zero_grad()\n",
    "    for j, (x, x_oh) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        #print(list(x.size()), list(x_oh.size()))\n",
    "        #break\n",
    "        x = torch.permute(x, dims=[1,0])\n",
    "        x_oh = torch.permute(x_oh, dims=[1,0, 2])\n",
    "        \n",
    "        outputs = model(x)\n",
    "        loss = loss_fn(torch.squeeze(outputs), torch.squeeze(x_oh))\n",
    "        loss.backward()\n",
    "        \n",
    "        #break\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        divisor += float( list(x.size())[1] )\n",
    "        #if j % 32 == 0:\n",
    "        #    optimizer.step()\n",
    "        #    optimizer.zero_grad()\n",
    "        if j % 1000 == 0:\n",
    "            last_loss = running_loss / divisor # loss per batch\n",
    "            print('  batch {} loss: {}'.format(j, last_loss))\n",
    "            running_loss = 0.\n",
    "            divisor = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffda14-b2f5-4174-ada1-6bd61d57f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3aa6f6-4e38-4aa4-baba-0ebab93ee930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
