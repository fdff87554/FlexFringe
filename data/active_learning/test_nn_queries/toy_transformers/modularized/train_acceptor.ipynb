{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b62867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from data_handling import SequenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model_definitions import AuAcceptor\n",
    "\n",
    "import math\n",
    "import pickle as pk\n",
    "\n",
    "DATA_PATH = \"../data/problem_4_train_dfa_accept_and_reject.dat\"\n",
    "EPOCHS = 120\n",
    "TRANSFORMER_NAME = \"transformer_problem_4.pk\" # the transformer that we'll load\n",
    "\n",
    "ACCEPTOR_NAME = \"acceptor_problem_4.pk\" # how to save model\n",
    "\n",
    "DATASET_CONTAINER_PATH = \"dataset_problem_4.pk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb504d",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7ac4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet size:  3\n",
      "Sequences loaded. Some examples: \n",
      "[['a', 'b', 'a', 'a', 'a', 'b', 'a', 'b', 'c', 'a', 'a', 'a', 'a'], ['a', 'b', 'c', 'b', 'b', 'b', 'a', 'b', 'c', 'c', 'a', 'c', 'a'], ['c', 'a', 'c']]\n",
      "The symbol dictionary: {'a': 0, 'b': 1, 'c': 2}\n"
     ]
    }
   ],
   "source": [
    "dataset = SequenceDataset(DATA_PATH, maxlen=15)\n",
    "dataset.initialize(DATASET_CONTAINER_PATH)\n",
    "dataset.encode_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a79ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7eec33",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5c848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuTransformer(\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (input_embedding): Embedding(6, 3)\n",
       "  (output_fnn): Linear(in_features=3, out_features=6, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (softmax_output): Softmax(dim=-1)\n",
       "  (attention_output_layer): Identity()\n",
       "  (attention_weight_layer): Identity()\n",
       "  (src_embedding_output_layer): Identity()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = torch.load(TRANSFORMER_NAME)\n",
    "\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d438a48-f91f-4d99-acc9-89a8886718a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuAcceptor(\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (input_embedding): Embedding(6, 3)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (hidden_layer): Linear(in_features=51, out_features=1, bias=True)\n",
       "  (attention_output_layer): Identity()\n",
       "  (attention_weight_layer): Identity()\n",
       "  (src_embedding_output_layer): Identity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuAcceptor(transformer_model=transformer, alphabet_size=4, \n",
    "                   embedding_dim=transformer.embedding_dim, maxlen_of_sequence=dataset.maxlen, freeze_transformer=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c7922",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce45df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  batch 0 loss: 0.09485961496829987\n",
      "  batch 1000 loss: 0.07841320924088359\n",
      "  batch 2000 loss: 0.07422118492051959\n",
      "Epoch:  1\n",
      "  batch 0 loss: 0.07185872994363308\n",
      "  batch 1000 loss: 0.06991370917856693\n",
      "  batch 2000 loss: 0.06696430980041623\n",
      "Epoch:  2\n",
      "  batch 0 loss: 0.06510196321457624\n",
      "  batch 1000 loss: 0.06365099645778537\n",
      "  batch 2000 loss: 0.0608622776158154\n",
      "Epoch:  3\n",
      "  batch 0 loss: 0.06008661155775189\n",
      "  batch 1000 loss: 0.05872839763946831\n",
      "  batch 2000 loss: 0.0566308329012245\n",
      "Epoch:  4\n",
      "  batch 0 loss: 0.05564231513440609\n",
      "  batch 1000 loss: 0.05442515068873763\n",
      "  batch 2000 loss: 0.053542224524542686\n",
      "Epoch:  5\n",
      "  batch 0 loss: 0.0528509355597198\n",
      "  batch 1000 loss: 0.0512775364946574\n",
      "  batch 2000 loss: 0.051006896924227475\n",
      "Epoch:  6\n",
      "  batch 0 loss: 0.04959078080207109\n",
      "  batch 1000 loss: 0.04952894089743495\n",
      "  batch 2000 loss: 0.04792770919576287\n",
      "Epoch:  7\n",
      "  batch 0 loss: 0.04670621825009585\n",
      "  batch 1000 loss: 0.04694832682795823\n",
      "  batch 2000 loss: 0.04526688189804554\n",
      "Epoch:  8\n",
      "  batch 0 loss: 0.04583273019641638\n",
      "  batch 1000 loss: 0.044425906508229675\n",
      "  batch 2000 loss: 0.04381809429451823\n",
      "Epoch:  9\n",
      "  batch 0 loss: 0.04336147378757596\n",
      "  batch 1000 loss: 0.04253447306714952\n",
      "  batch 2000 loss: 0.04179143982473761\n",
      "Epoch:  10\n",
      "  batch 0 loss: 0.04161430899985134\n",
      "  batch 1000 loss: 0.04085635205172002\n",
      "  batch 2000 loss: 0.04043350160587579\n",
      "Epoch:  11\n",
      "  batch 0 loss: 0.038569969773292544\n",
      "  batch 1000 loss: 0.03922519362531603\n",
      "  batch 2000 loss: 0.037801964913494884\n",
      "Epoch:  12\n",
      "  batch 0 loss: 0.03838520313613117\n",
      "  batch 1000 loss: 0.03732555500045419\n",
      "  batch 2000 loss: 0.036649174381047485\n",
      "Epoch:  13\n",
      "  batch 0 loss: 0.03633140569552779\n",
      "  batch 1000 loss: 0.035883463341742756\n",
      "  batch 2000 loss: 0.035117493169382216\n",
      "Epoch:  14\n",
      "  batch 0 loss: 0.0343157602455467\n",
      "  batch 1000 loss: 0.033940885544754565\n",
      "  batch 2000 loss: 0.033876380570232865\n",
      "Epoch:  15\n",
      "  batch 0 loss: 0.03307521965540945\n",
      "  batch 1000 loss: 0.03234213833976537\n",
      "  batch 2000 loss: 0.0327008875226602\n",
      "Epoch:  16\n",
      "  batch 0 loss: 0.03147646787762642\n",
      "  batch 1000 loss: 0.03133352404367179\n",
      "  batch 2000 loss: 0.030849644606467337\n",
      "Epoch:  17\n",
      "  batch 0 loss: 0.030469093138352037\n",
      "  batch 1000 loss: 0.02999088939651847\n",
      "  batch 2000 loss: 0.029516518996097148\n",
      "Epoch:  18\n",
      "  batch 0 loss: 0.029689960697665812\n",
      "  batch 1000 loss: 0.029229684718884527\n",
      "  batch 2000 loss: 0.028442202914506196\n",
      "Epoch:  19\n",
      "  batch 0 loss: 0.02765036029368639\n",
      "  batch 1000 loss: 0.02793958549015224\n",
      "  batch 2000 loss: 0.02765175046771765\n",
      "Epoch:  20\n",
      "  batch 0 loss: 0.026509994812309742\n",
      "  batch 1000 loss: 0.027263413683511317\n",
      "  batch 2000 loss: 0.025975737522356213\n",
      "Epoch:  21\n",
      "  batch 0 loss: 0.026223173702135682\n",
      "  batch 1000 loss: 0.025613783656153827\n",
      "  batch 2000 loss: 0.025709309063851833\n",
      "Epoch:  22\n",
      "  batch 0 loss: 0.02543609403818846\n",
      "  batch 1000 loss: 0.02427904580300674\n",
      "  batch 2000 loss: 0.025116994509939105\n",
      "Epoch:  23\n",
      "  batch 0 loss: 0.024933371348306536\n",
      "  batch 1000 loss: 0.024261703215539457\n",
      "  batch 2000 loss: 0.0235246542673558\n",
      "Epoch:  24\n",
      "  batch 0 loss: 0.02414581102505326\n",
      "  batch 1000 loss: 0.023841507371980698\n",
      "  batch 2000 loss: 0.023201554314233364\n",
      "Epoch:  25\n",
      "  batch 0 loss: 0.021780239964835347\n",
      "  batch 1000 loss: 0.022837711742613464\n",
      "  batch 2000 loss: 0.02189089491730556\n",
      "Epoch:  26\n",
      "  batch 0 loss: 0.022832781939767302\n",
      "  batch 1000 loss: 0.021844713028986006\n",
      "  batch 2000 loss: 0.0218506284779869\n",
      "Epoch:  27\n",
      "  batch 0 loss: 0.02160580461565405\n",
      "  batch 1000 loss: 0.021489793790969998\n",
      "  batch 2000 loss: 0.021084576885681598\n",
      "Epoch:  28\n",
      "  batch 0 loss: 0.02076030438579619\n",
      "  batch 1000 loss: 0.021368282455019653\n",
      "  batch 2000 loss: 0.020105949596501888\n",
      "Epoch:  29\n",
      "  batch 0 loss: 0.019893347196280955\n",
      "  batch 1000 loss: 0.019972647244576366\n",
      "  batch 2000 loss: 0.020347458271076903\n",
      "Epoch:  30\n",
      "  batch 0 loss: 0.0195333439623937\n",
      "  batch 1000 loss: 0.01972004719101824\n",
      "  batch 2000 loss: 0.019730666207149625\n",
      "Epoch:  31\n",
      "  batch 0 loss: 0.018620162636041642\n",
      "  batch 1000 loss: 0.019259882893878966\n",
      "  batch 2000 loss: 0.019023215173976496\n",
      "Epoch:  32\n",
      "  batch 0 loss: 0.01854584479704499\n",
      "  batch 1000 loss: 0.018427820765646176\n",
      "  batch 2000 loss: 0.01887754186964594\n",
      "Epoch:  33\n",
      "  batch 0 loss: 0.018222947923466563\n",
      "  batch 1000 loss: 0.01881542006926611\n",
      "  batch 2000 loss: 0.017893634433858097\n",
      "Epoch:  34\n",
      "  batch 0 loss: 0.017223114744294436\n",
      "  batch 1000 loss: 0.017686934067169206\n",
      "  batch 2000 loss: 0.0179748337585479\n",
      "Epoch:  35\n",
      "  batch 0 loss: 0.01724508954025805\n",
      "  batch 1000 loss: 0.017356857453938574\n",
      "  batch 2000 loss: 0.01721008106111549\n",
      "Epoch:  36\n",
      "  batch 0 loss: 0.017482493083924053\n",
      "  batch 1000 loss: 0.016785161134786904\n",
      "  batch 2000 loss: 0.017131522805662824\n",
      "Epoch:  37\n",
      "  batch 0 loss: 0.016939694489352404\n",
      "  batch 1000 loss: 0.016597023234935476\n",
      "  batch 2000 loss: 0.01708686654223129\n",
      "Epoch:  38\n",
      "  batch 0 loss: 0.015672366250772028\n",
      "  batch 1000 loss: 0.01623664649995044\n",
      "  batch 2000 loss: 0.016624863226665183\n",
      "Epoch:  39\n",
      "  batch 0 loss: 0.015671913163270802\n",
      "  batch 1000 loss: 0.016530141337309033\n",
      "  batch 2000 loss: 0.015455752195091918\n",
      "Epoch:  40\n",
      "  batch 0 loss: 0.015844403985422105\n",
      "  batch 1000 loss: 0.01558249501651153\n",
      "  batch 2000 loss: 0.015516593265812844\n",
      "Epoch:  41\n",
      "  batch 0 loss: 0.016148655568249525\n",
      "  batch 1000 loss: 0.016346395809436216\n",
      "  batch 2000 loss: 0.01491136109502986\n",
      "Epoch:  42\n",
      "  batch 0 loss: 0.014409215341322125\n",
      "  batch 1000 loss: 0.015055056955199689\n",
      "  batch 2000 loss: 0.015316660556942224\n",
      "Epoch:  43\n",
      "  batch 0 loss: 0.01483156433654949\n",
      "  batch 1000 loss: 0.01554260809009429\n",
      "  batch 2000 loss: 0.014533614266198128\n",
      "Epoch:  44\n",
      "  batch 0 loss: 0.014158464511856437\n",
      "  batch 1000 loss: 0.01493307563336566\n",
      "  batch 2000 loss: 0.014982973816571757\n",
      "Epoch:  45\n",
      "  batch 0 loss: 0.013278456492349506\n",
      "  batch 1000 loss: 0.014936599699081853\n",
      "  batch 2000 loss: 0.013951063237851486\n",
      "Epoch:  46\n",
      "  batch 0 loss: 0.014196574336849153\n",
      "  batch 1000 loss: 0.01428216002532281\n",
      "  batch 2000 loss: 0.014255068632541225\n",
      "Epoch:  47\n",
      "  batch 0 loss: 0.013767264659050852\n",
      "  batch 1000 loss: 0.013702951236162334\n",
      "  batch 2000 loss: 0.014066703128861264\n",
      "Epoch:  48\n",
      "  batch 0 loss: 0.014207622870337219\n",
      "  batch 1000 loss: 0.013722278654691764\n",
      "  batch 2000 loss: 0.013466733392793685\n",
      "Epoch:  49\n",
      "  batch 0 loss: 0.014421991457697004\n",
      "  batch 1000 loss: 0.01360809665848501\n",
      "  batch 2000 loss: 0.013853927658987231\n",
      "Epoch:  50\n",
      "  batch 0 loss: 0.012905732638435438\n",
      "  batch 1000 loss: 0.013220970754511655\n",
      "  batch 2000 loss: 0.013892344878870063\n",
      "Epoch:  51\n",
      "  batch 0 loss: 0.012757641177158803\n",
      "  batch 1000 loss: 0.01323806012107525\n",
      "  batch 2000 loss: 0.01311615554674063\n",
      "Epoch:  52\n",
      "  batch 0 loss: 0.013210168896708637\n",
      "  batch 1000 loss: 0.01297261187946424\n",
      "  batch 2000 loss: 0.012886199722648598\n",
      "Epoch:  53\n",
      "  batch 0 loss: 0.013450240151491016\n",
      "  batch 1000 loss: 0.012935408331337385\n",
      "  batch 2000 loss: 0.013043595188995824\n",
      "Epoch:  54\n",
      "  batch 0 loss: 0.012446558447089046\n",
      "  batch 1000 loss: 0.012873773750034161\n",
      "  batch 2000 loss: 0.012504826243733988\n",
      "Epoch:  55\n",
      "  batch 0 loss: 0.012826081658713519\n",
      "  batch 1000 loss: 0.012315637273713946\n",
      "  batch 2000 loss: 0.012893025505472906\n",
      "Epoch:  56\n",
      "  batch 0 loss: 0.012367899089353158\n",
      "  batch 1000 loss: 0.012230479278718122\n",
      "  batch 2000 loss: 0.012526057510549436\n",
      "Epoch:  57\n",
      "  batch 0 loss: 0.012703891009325162\n",
      "  batch 1000 loss: 0.012123947654268705\n",
      "  batch 2000 loss: 0.012573446695692837\n",
      "Epoch:  58\n",
      "  batch 0 loss: 0.012003903770819306\n",
      "  batch 1000 loss: 0.012105089671793395\n",
      "  batch 2000 loss: 0.01246364506939426\n",
      "Epoch:  59\n",
      "  batch 0 loss: 0.01152281680610031\n",
      "  batch 1000 loss: 0.012257158590655308\n",
      "  batch 2000 loss: 0.011552942079375498\n",
      "Epoch:  60\n",
      "  batch 0 loss: 0.01248050427576527\n",
      "  batch 1000 loss: 0.012298460634192453\n",
      "  batch 2000 loss: 0.01153432943066582\n",
      "Epoch:  61\n",
      "  batch 0 loss: 0.011824012610362842\n",
      "  batch 1000 loss: 0.011192300394061022\n",
      "  batch 2000 loss: 0.012444705240079202\n",
      "Epoch:  62\n",
      "  batch 0 loss: 0.011598511790158228\n",
      "  batch 1000 loss: 0.011890416977112182\n",
      "  batch 2000 loss: 0.01155359869892709\n",
      "Epoch:  63\n",
      "  batch 0 loss: 0.011397910485276953\n",
      "  batch 1000 loss: 0.011669534931366797\n",
      "  batch 2000 loss: 0.01166586325096432\n",
      "Epoch:  64\n",
      "  batch 0 loss: 0.011079027556348592\n",
      "  batch 1000 loss: 0.011393768175272271\n",
      "  batch 2000 loss: 0.011299531837576069\n",
      "Epoch:  65\n",
      "  batch 0 loss: 0.011808499726932495\n",
      "  batch 1000 loss: 0.011598817156569567\n",
      "  batch 2000 loss: 0.010869848818634637\n",
      "Epoch:  66\n",
      "  batch 0 loss: 0.011735912801465019\n",
      "  batch 1000 loss: 0.01154496422538068\n",
      "  batch 2000 loss: 0.011153063930687495\n",
      "Epoch:  67\n",
      "  batch 0 loss: 0.010756151222391054\n",
      "  batch 1000 loss: 0.011153578811208717\n",
      "  batch 2000 loss: 0.01099756773887202\n",
      "Epoch:  68\n",
      "  batch 0 loss: 0.011368089137366042\n",
      "  batch 1000 loss: 0.010789257633310626\n",
      "  batch 2000 loss: 0.01129744221502915\n",
      "Epoch:  69\n",
      "  batch 0 loss: 0.01098139622202143\n",
      "  batch 1000 loss: 0.011591338261583588\n",
      "  batch 2000 loss: 0.0107104163991753\n",
      "Epoch:  70\n",
      "  batch 0 loss: 0.010089099448174238\n",
      "  batch 1000 loss: 0.011008117302611935\n",
      "  batch 2000 loss: 0.010299960049655056\n",
      "Epoch:  71\n",
      "  batch 0 loss: 0.01163588759372942\n",
      "  batch 1000 loss: 0.011017510853678686\n",
      "  batch 2000 loss: 0.010700142553891056\n",
      "Epoch:  72\n",
      "  batch 0 loss: 0.01034633279812988\n",
      "  batch 1000 loss: 0.010795836595352739\n",
      "  batch 2000 loss: 0.010703784825862385\n",
      "Epoch:  73\n",
      "  batch 0 loss: 0.010352303898194805\n",
      "  batch 1000 loss: 0.01030479594762437\n",
      "  batch 2000 loss: 0.010243640941684133\n",
      "Epoch:  74\n",
      "  batch 0 loss: 0.01182366893463768\n",
      "  batch 1000 loss: 0.010367914531147108\n",
      "  batch 2000 loss: 0.010697206100914628\n",
      "Epoch:  75\n",
      "  batch 0 loss: 0.010339606758207083\n",
      "  batch 1000 loss: 0.010464702945842874\n",
      "  batch 2000 loss: 0.010062463425332681\n",
      "Epoch:  76\n",
      "  batch 0 loss: 0.011010627468698658\n",
      "  batch 1000 loss: 0.010237677805242129\n",
      "  batch 2000 loss: 0.010526177509105764\n",
      "Epoch:  77\n",
      "  batch 0 loss: 0.010143330787075683\n",
      "  batch 1000 loss: 0.01030604253540514\n",
      "  batch 2000 loss: 0.010156374238606077\n",
      "Epoch:  78\n",
      "  batch 0 loss: 0.010328327716793866\n",
      "  batch 1000 loss: 0.010225909703818616\n",
      "  batch 2000 loss: 0.009998036880395375\n",
      "Epoch:  79\n",
      "  batch 0 loss: 0.010415955884265714\n",
      "  batch 1000 loss: 0.010545757909188979\n",
      "  batch 2000 loss: 0.009887685160734691\n",
      "Epoch:  80\n",
      "  batch 0 loss: 0.009629387556924485\n",
      "  batch 1000 loss: 0.010040158585179598\n",
      "  batch 2000 loss: 0.009417507453472354\n",
      "Epoch:  81\n",
      "  batch 0 loss: 0.01120314377406612\n",
      "  batch 1000 loss: 0.009718886309106892\n",
      "  batch 2000 loss: 0.010211937787942589\n",
      "Epoch:  82\n",
      "  batch 0 loss: 0.009894184199278244\n",
      "  batch 1000 loss: 0.010231566640664823\n",
      "  batch 2000 loss: 0.009914001821249258\n",
      "Epoch:  83\n",
      "  batch 0 loss: 0.009134917168295942\n",
      "  batch 1000 loss: 0.010146557971369475\n",
      "  batch 2000 loss: 0.009303202148759737\n",
      "Epoch:  84\n",
      "  batch 0 loss: 0.010194323655450716\n",
      "  batch 1000 loss: 0.009600461602269206\n",
      "  batch 2000 loss: 0.009817541212483774\n",
      "Epoch:  85\n",
      "  batch 0 loss: 0.009968278236920014\n",
      "  batch 1000 loss: 0.0098462302130647\n",
      "  batch 2000 loss: 0.009515365922939964\n",
      "Epoch:  86\n",
      "  batch 0 loss: 0.009754980868194253\n",
      "  batch 1000 loss: 0.00981854601483792\n",
      "  batch 2000 loss: 0.00975759300251957\n",
      "Epoch:  87\n",
      "  batch 0 loss: 0.009088527688290923\n",
      "  batch 1000 loss: 0.009863930838357191\n",
      "  batch 2000 loss: 0.009080802098615094\n",
      "Epoch:  88\n",
      "  batch 0 loss: 0.010098643584758975\n",
      "  batch 1000 loss: 0.009198691897501703\n",
      "  batch 2000 loss: 0.009811085034860298\n",
      "Epoch:  89\n",
      "  batch 0 loss: 0.009587797049432993\n",
      "  batch 1000 loss: 0.009870351567922625\n",
      "  batch 2000 loss: 0.009587944735947531\n",
      "Epoch:  90\n",
      "  batch 0 loss: 0.008479685792815871\n",
      "  batch 1000 loss: 0.009961761674727313\n",
      "  batch 2000 loss: 0.008875388572749216\n",
      "Epoch:  91\n",
      "  batch 0 loss: 0.009470889674150385\n",
      "  batch 1000 loss: 0.009678193523606752\n",
      "  batch 2000 loss: 0.009096240751619916\n",
      "Epoch:  92\n",
      "  batch 0 loss: 0.00934396809351165\n",
      "  batch 1000 loss: 0.009056843124795705\n",
      "  batch 2000 loss: 0.009533037674962544\n",
      "Epoch:  93\n",
      "  batch 0 loss: 0.009421493600355461\n",
      "  batch 1000 loss: 0.009534561872831545\n",
      "  batch 2000 loss: 0.009032771661833977\n",
      "Epoch:  94\n",
      "  batch 0 loss: 0.009281781505560503\n",
      "  batch 1000 loss: 0.009171456741751172\n",
      "  batch 2000 loss: 0.00922933834034484\n",
      "Epoch:  95\n",
      "  batch 0 loss: 0.009362408797140233\n",
      "  batch 1000 loss: 0.009023217813228257\n",
      "  batch 2000 loss: 0.009374559749732725\n",
      "Epoch:  96\n",
      "  batch 0 loss: 0.009099377537262626\n",
      "  batch 1000 loss: 0.008559901300235652\n",
      "  batch 2000 loss: 0.00937691680912394\n",
      "Epoch:  97\n",
      "  batch 0 loss: 0.00980993637163192\n",
      "  batch 1000 loss: 0.008967045943034463\n",
      "  batch 2000 loss: 0.009225694126595045\n",
      "Epoch:  98\n",
      "  batch 0 loss: 0.009137200891855173\n",
      "  batch 1000 loss: 0.00868661165604135\n",
      "  batch 2000 loss: 0.009143173539661802\n",
      "Epoch:  99\n",
      "  batch 0 loss: 0.009528299617697484\n",
      "  batch 1000 loss: 0.00911952374590328\n",
      "  batch 2000 loss: 0.008561003510782029\n",
      "Epoch:  100\n",
      "  batch 0 loss: 0.00967508804530371\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, (test_string_ord, test_string_ord_sr, test_string_oh, test_string_oh_sr, labels, sequence_length) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m---> 13\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     test_string_ord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(test_string_ord, dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m     test_string_ord_sr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(test_string_ord_sr, dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/decorators.py:47\u001b[0m, in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     45\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:290\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:813\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m fname[:neglen]\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetsourcefile\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    814\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the filename that can be used to locate an object's source.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m    Return None if no way can be identified to get the source.\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    817\u001b[0m     filename \u001b[38;5;241m=\u001b[39m getfile(\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)# for 3 heads lr=0.00001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "divisor = 0.\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"Epoch: \", i)\n",
    "    for j, (test_string_ord, test_string_ord_sr, test_string_oh, test_string_oh_sr, labels, sequence_length) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        test_string_ord = torch.permute(test_string_ord, dims=[1,0])\n",
    "        test_string_ord_sr = torch.permute(test_string_ord_sr, dims=[1,0])\n",
    "        test_string_oh_sr = torch.permute(test_string_oh_sr, dims=[1,0,2])\n",
    "\n",
    "        #print(sequence_length, list(test_string_ord.size()), labels)\n",
    "        #break\n",
    "        \n",
    "        outputs = model(test_string_ord, test_string_ord_sr, sequence_length)\n",
    "        loss = loss_fn(torch.squeeze(outputs), torch.squeeze(labels).float())\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        divisor += float( list(test_string_ord.size())[1] )\n",
    "        if j % 1000 == 0:\n",
    "            last_loss = running_loss / divisor # loss per batch\n",
    "            print('  batch {} loss: {}'.format(j, last_loss))\n",
    "            running_loss = 0.\n",
    "            divisor = 0.\n",
    "    #break\n",
    "#raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9cf0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, ACCEPTOR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a25a26-ff3b-40a1-9610-76ebed0fabd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
