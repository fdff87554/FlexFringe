{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b62867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from data_handling import SequenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model_definitions import AuTransformer\n",
    "\n",
    "import math\n",
    "import pickle as pk\n",
    "\n",
    "DATA_PATH = \"../data/problem_1_train_dfa_accept_and_reject.dat\"\n",
    "EPOCHS = 1\n",
    "MODEL_NAME = \"trained_model.pk\" # the transformer that we'll load\n",
    "\n",
    "DATASET_CONTAINER_PATH = \"dataset.pk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb504d",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7ac4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet size:  4\n",
      "Sequences loaded. Some examples: \n",
      "[['a', 'a', 'd'], ['a', 'd', 'c', 'd', 'd', 'd', 'c'], ['c', 'c', 'a', 'a', 'c', 'b', 'a', 'c', 'a']]\n",
      "The symbol dictionary: {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n"
     ]
    }
   ],
   "source": [
    "dataset = SequenceDataset(DATA_PATH, maxlen=10)\n",
    "dataset.initialize(DATASET_CONTAINER_PATH)\n",
    "dataset.encode_sequences()\n",
    "dataset.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a79ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7eec33",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5c848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuTransformer(\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (input_embedding): Embedding(7, 3)\n",
       "  (output_fnn): Linear(in_features=3, out_features=7, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (softmax_output): Softmax(dim=-1)\n",
       "  (attention_output_layer): Identity()\n",
       "  (attention_weight_layer): Identity()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuTransformer(n_encoders=1, n_decoders=1, alphabet_size=4, embedding_dim=3, max_len=10)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c7922",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce45df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  batch 0 loss: 1.4126192331314087\n",
      "  batch 1000 loss: 1.4520285028219222\n",
      "  batch 2000 loss: 1.4544923737049102\n",
      "  batch 3000 loss: 1.45058392226696\n",
      "  batch 4000 loss: 1.449556448340416\n",
      "  batch 5000 loss: 1.4460676217079163\n",
      "  batch 6000 loss: 1.4481366548538208\n",
      "  batch 7000 loss: 1.443817057967186\n",
      "  batch 8000 loss: 1.4421689954996109\n",
      "  batch 9000 loss: 1.4423229407072067\n",
      "  batch 10000 loss: 1.4404325652122498\n",
      "  batch 11000 loss: 1.4386720995903015\n",
      "  batch 12000 loss: 1.4395964016914369\n",
      "Epoch:  1\n",
      "  batch 0 loss: 1.431784289453166\n",
      "  batch 1000 loss: 1.4392980933189392\n",
      "  batch 2000 loss: 1.438429934501648\n",
      "  batch 3000 loss: 1.4341865170001984\n",
      "  batch 4000 loss: 1.4365210058689117\n",
      "  batch 5000 loss: 1.435165327191353\n",
      "  batch 6000 loss: 1.4319866579771041\n",
      "  batch 7000 loss: 1.4261005415916443\n",
      "  batch 8000 loss: 1.4286127898693084\n",
      "  batch 9000 loss: 1.4300851653814315\n",
      "  batch 10000 loss: 1.4252449017763138\n",
      "  batch 11000 loss: 1.4292375328540803\n",
      "  batch 12000 loss: 1.424433528661728\n",
      "Epoch:  2\n",
      "  batch 0 loss: 1.4270990753816033\n",
      "  batch 1000 loss: 1.4213915067911147\n",
      "  batch 2000 loss: 1.4262943168878555\n",
      "  batch 3000 loss: 1.4236496301889419\n",
      "  batch 4000 loss: 1.424145890712738\n",
      "  batch 5000 loss: 1.4221675188541412\n",
      "  batch 6000 loss: 1.422794540643692\n",
      "  batch 7000 loss: 1.4204407814741136\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, (test_string_ord, test_string_ord_sr, test_string_oh, test_string_oh_sr, _, sequence_length) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     13\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m         test_string_ord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(test_string_ord, dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:632\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n\u001b[1;32m    635\u001b[0m     warn_msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of IterableDataset \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was reported to be \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (when accessing len(dataloader)), but \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    636\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples have been fetched. \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called,\n\u001b[1;32m    637\u001b[0m                                                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded)\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)# for 3 heads lr=0.00001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "divisor = 0.\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"Epoch: \", i)\n",
    "    for j, (test_string_ord, test_string_ord_sr, test_string_oh, test_string_oh_sr, _, sequence_length) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        test_string_ord = torch.permute(test_string_ord, dims=[1,0])\n",
    "        test_string_ord_sr = torch.permute(test_string_ord_sr, dims=[1,0])\n",
    "        test_string_oh_sr = torch.permute(test_string_oh_sr, dims=[1,0,2])\n",
    "        \n",
    "        outputs = model(test_string_ord, test_string_ord_sr)\n",
    "        loss = loss_fn(torch.squeeze(outputs), torch.squeeze(test_string_oh_sr))\n",
    "        loss.backward()\n",
    "        \n",
    "        #break\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        divisor += float( list(test_string_ord.size())[1] )\n",
    "        if j % 1000 == 0:\n",
    "            last_loss = running_loss / divisor # loss per batch\n",
    "            print('  batch {} loss: {}'.format(j, last_loss))\n",
    "            running_loss = 0.\n",
    "            divisor = 0.\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9cf0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a25a26-ff3b-40a1-9610-76ebed0fabd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
