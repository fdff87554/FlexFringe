{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b62867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from data_handling import SequenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model_definitions import AuTransformer\n",
    "\n",
    "import math\n",
    "import pickle as pk\n",
    "\n",
    "DATA_PATH = \"../data/pdfa_problem_1_train.dat\"\n",
    "EPOCHS = 35\n",
    "MODEL_NAME = \"transformer_pdfa_problem_1.pk\" # how to save model\n",
    "DATASET_SAVE_PATH = \"dataset_pdfa_problem_1.pk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb504d",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7ac4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet size:  3\n",
      "Sequences loaded. Some examples: \n",
      "[['b', 'c', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'c', 'a', 'a', 'c', 'b', 'c'], ['a', 'c', 'c', 'b', 'c', 'c', 'b', 'b', 'c', 'a', 'c', 'c', 'c', 'a', 'b'], ['c', 'a', 'a']]\n",
      "The symbol dictionary: {'b': 0, 'c': 1, 'a': 2}\n"
     ]
    }
   ],
   "source": [
    "dataset = SequenceDataset(DATA_PATH, maxlen=15)\n",
    "dataset.encode_sequences()\n",
    "dataset.save_state(DATASET_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a79ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eadfeb4-2864-494d-9726-6215d96da702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 0, 'c': 1, 'a': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.symbol_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7eec33",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f5c848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuTransformer(\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (input_embedding): Embedding(6, 3)\n",
       "  (output_fnn): Linear(in_features=3, out_features=6, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (softmax_output): Softmax(dim=-1)\n",
       "  (attention_output_layer): Identity()\n",
       "  (attention_weight_layer): Identity()\n",
       "  (src_embedding_output_layer): Identity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuTransformer(n_encoders=1, n_decoders=1, alphabet_size=3, embedding_dim=3, max_len=15)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c7922",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce45df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  batch 0 loss: 1.7453734874725342\n",
      "  batch 1000 loss: 1.8094243041276932\n",
      "  batch 2000 loss: 1.8054492086172105\n",
      "  batch 3000 loss: 1.805641761779785\n",
      "  batch 4000 loss: 1.8020022773742677\n",
      "  batch 5000 loss: 1.7994120483398437\n",
      "  batch 6000 loss: 1.797045438170433\n",
      "  batch 7000 loss: 1.792621690750122\n",
      "  batch 8000 loss: 1.78820743227005\n",
      "  batch 9000 loss: 1.7900837712287903\n",
      "Epoch:  1\n",
      "  batch 0 loss: 1.7840820249319076\n",
      "  batch 1000 loss: 1.7858892290592194\n",
      "  batch 2000 loss: 1.7834451261758804\n",
      "  batch 3000 loss: 1.77810253739357\n",
      "  batch 4000 loss: 1.7788764984607697\n",
      "  batch 5000 loss: 1.771708419084549\n",
      "  batch 6000 loss: 1.7733616417646407\n",
      "  batch 7000 loss: 1.7704540235996247\n",
      "  batch 8000 loss: 1.7662045642137527\n",
      "  batch 9000 loss: 1.7650806121826172\n",
      "Epoch:  2\n",
      "  batch 0 loss: 1.7651108053922653\n",
      "  batch 1000 loss: 1.7634481309652328\n",
      "  batch 2000 loss: 1.7580441371202469\n",
      "  batch 3000 loss: 1.7548397567272187\n",
      "  batch 4000 loss: 1.7484263614416122\n",
      "  batch 5000 loss: 1.7506955243349076\n",
      "  batch 6000 loss: 1.7492399139404298\n",
      "  batch 7000 loss: 1.7462438881397246\n",
      "  batch 8000 loss: 1.7458325406312942\n",
      "  batch 9000 loss: 1.742739420413971\n",
      "Epoch:  3\n",
      "  batch 0 loss: 1.7416175394058226\n",
      "  batch 1000 loss: 1.7360822994709015\n",
      "  batch 2000 loss: 1.7370994476079942\n",
      "  batch 3000 loss: 1.7328297977447509\n",
      "  batch 4000 loss: 1.7259951671361924\n",
      "  batch 5000 loss: 1.731686774969101\n",
      "  batch 6000 loss: 1.7293852630853652\n",
      "  batch 7000 loss: 1.7219599763154982\n",
      "  batch 8000 loss: 1.717983271241188\n",
      "  batch 9000 loss: 1.7221876417398452\n",
      "Epoch:  4\n",
      "  batch 0 loss: 1.716763755440712\n",
      "  batch 1000 loss: 1.7139488197565078\n",
      "  batch 2000 loss: 1.7118758144378663\n",
      "  batch 3000 loss: 1.7115262830257416\n",
      "  batch 4000 loss: 1.7106633273363114\n",
      "  batch 5000 loss: 1.7041590735912322\n",
      "  batch 6000 loss: 1.7089115748405457\n",
      "  batch 7000 loss: 1.7040578210353852\n",
      "  batch 8000 loss: 1.6997007602453231\n",
      "  batch 9000 loss: 1.7009506856203078\n",
      "Epoch:  5\n",
      "  batch 0 loss: 1.6958822909593583\n",
      "  batch 1000 loss: 1.6968543136119842\n",
      "  batch 2000 loss: 1.6924378288984299\n",
      "  batch 3000 loss: 1.6961556133031845\n",
      "  batch 4000 loss: 1.6861971117258072\n",
      "  batch 5000 loss: 1.6890468213558196\n",
      "  batch 6000 loss: 1.678718602180481\n",
      "  batch 7000 loss: 1.6851778099536896\n",
      "  batch 8000 loss: 1.6861063697338103\n",
      "  batch 9000 loss: 1.6821014845371247\n",
      "Epoch:  6\n",
      "  batch 0 loss: 1.680515553712845\n",
      "  batch 1000 loss: 1.677188924074173\n",
      "  batch 2000 loss: 1.6833415446281432\n",
      "  batch 3000 loss: 1.6714454765319824\n",
      "  batch 4000 loss: 1.669546053171158\n",
      "  batch 5000 loss: 1.6730070160627366\n",
      "  batch 6000 loss: 1.6657584577798843\n",
      "  batch 7000 loss: 1.666977313041687\n",
      "  batch 8000 loss: 1.6635248973369599\n",
      "  batch 9000 loss: 1.6699032814502717\n",
      "Epoch:  7\n",
      "  batch 0 loss: 1.6612384411096572\n",
      "  batch 1000 loss: 1.6647339825630187\n",
      "  batch 2000 loss: 1.6619325898885726\n",
      "  batch 3000 loss: 1.6598565742969513\n",
      "  batch 4000 loss: 1.6611263802051544\n",
      "  batch 5000 loss: 1.6536326171159745\n",
      "  batch 6000 loss: 1.6516948862075806\n",
      "  batch 7000 loss: 1.6513317885398864\n",
      "  batch 8000 loss: 1.6484359704256057\n",
      "  batch 9000 loss: 1.6430902502536773\n",
      "Epoch:  8\n",
      "  batch 0 loss: 1.6448149086236954\n",
      "  batch 1000 loss: 1.6383150428533555\n",
      "  batch 2000 loss: 1.6511576186418533\n",
      "  batch 3000 loss: 1.637582647204399\n",
      "  batch 4000 loss: 1.648277585506439\n",
      "  batch 5000 loss: 1.6326528714895248\n",
      "  batch 6000 loss: 1.629618038535118\n",
      "  batch 7000 loss: 1.640455916404724\n",
      "  batch 8000 loss: 1.6335220727920532\n",
      "  batch 9000 loss: 1.6350075299739837\n",
      "Epoch:  9\n",
      "  batch 0 loss: 1.6376405316591263\n",
      "  batch 1000 loss: 1.6308387169837952\n",
      "  batch 2000 loss: 1.6353687925338745\n",
      "  batch 3000 loss: 1.6275504004955292\n",
      "  batch 4000 loss: 1.6257043699026108\n",
      "  batch 5000 loss: 1.6324225471019744\n",
      "  batch 6000 loss: 1.61733018553257\n",
      "  batch 7000 loss: 1.6178502449989318\n",
      "  batch 8000 loss: 1.6126509498357773\n",
      "  batch 9000 loss: 1.6161195974349976\n",
      "Epoch:  10\n",
      "  batch 0 loss: 1.6180061321258545\n",
      "  batch 1000 loss: 1.6142286865711213\n",
      "  batch 2000 loss: 1.6169260520935058\n",
      "  batch 3000 loss: 1.6155065709352494\n",
      "  batch 4000 loss: 1.6104579899311067\n",
      "  batch 5000 loss: 1.6186314697265625\n",
      "  batch 6000 loss: 1.6085428674221038\n",
      "  batch 7000 loss: 1.6033488371372222\n",
      "  batch 8000 loss: 1.605555550813675\n",
      "  batch 9000 loss: 1.6036560341119765\n",
      "Epoch:  11\n",
      "  batch 0 loss: 1.6034110633134842\n",
      "  batch 1000 loss: 1.6042611459493636\n",
      "  batch 2000 loss: 1.5970103583335877\n",
      "  batch 3000 loss: 1.6021458163261413\n",
      "  batch 4000 loss: 1.601159875035286\n",
      "  batch 5000 loss: 1.5966362843513489\n",
      "  batch 6000 loss: 1.5987717101573944\n",
      "  batch 7000 loss: 1.6009081671237946\n",
      "  batch 8000 loss: 1.5912288833856583\n",
      "  batch 9000 loss: 1.5927293416261672\n",
      "Epoch:  12\n",
      "  batch 0 loss: 1.5997190597057342\n",
      "  batch 1000 loss: 1.593218778014183\n",
      "  batch 2000 loss: 1.5899264709949494\n",
      "  batch 3000 loss: 1.5898232938051224\n",
      "  batch 4000 loss: 1.5963930250406266\n",
      "  batch 5000 loss: 1.5875984559059142\n",
      "  batch 6000 loss: 1.5852182013988494\n",
      "  batch 7000 loss: 1.5855332742929458\n",
      "  batch 8000 loss: 1.590968507885933\n",
      "  batch 9000 loss: 1.5836677355766295\n",
      "Epoch:  13\n",
      "  batch 0 loss: 1.5787254565954207\n",
      "  batch 1000 loss: 1.5843323925733566\n",
      "  batch 2000 loss: 1.5722264986038208\n",
      "  batch 3000 loss: 1.5805997422933578\n",
      "  batch 4000 loss: 1.5843814735412598\n",
      "  batch 5000 loss: 1.5831316200494767\n",
      "  batch 6000 loss: 1.586771181344986\n",
      "  batch 7000 loss: 1.574159476995468\n",
      "  batch 8000 loss: 1.5776821186542511\n",
      "  batch 9000 loss: 1.5716680812835693\n",
      "Epoch:  14\n",
      "  batch 0 loss: 1.5732996011972427\n",
      "  batch 1000 loss: 1.5664850566387176\n",
      "  batch 2000 loss: 1.5763135699033737\n",
      "  batch 3000 loss: 1.5730738406181335\n",
      "  batch 4000 loss: 1.563724813580513\n",
      "  batch 5000 loss: 1.567900906085968\n",
      "  batch 6000 loss: 1.5733873484134675\n",
      "  batch 7000 loss: 1.5721282814741135\n",
      "  batch 8000 loss: 1.573671613574028\n",
      "  batch 9000 loss: 1.5720653779506684\n",
      "Epoch:  15\n",
      "  batch 0 loss: 1.5650999274253845\n",
      "  batch 1000 loss: 1.5749662919044494\n",
      "  batch 2000 loss: 1.5626732485294341\n",
      "  batch 3000 loss: 1.5627797733545303\n",
      "  batch 4000 loss: 1.5693140300512314\n",
      "  batch 5000 loss: 1.5659668749570848\n",
      "  batch 6000 loss: 1.5662787445783615\n",
      "  batch 7000 loss: 1.5579926491975784\n",
      "  batch 8000 loss: 1.5443156534433364\n",
      "  batch 9000 loss: 1.5650220606327057\n",
      "Epoch:  16\n",
      "  batch 0 loss: 1.5579382667541504\n",
      "  batch 1000 loss: 1.5606981741189956\n",
      "  batch 2000 loss: 1.564103681921959\n",
      "  batch 3000 loss: 1.5505549783706665\n",
      "  batch 4000 loss: 1.5654141597747804\n",
      "  batch 5000 loss: 1.5587484077215195\n",
      "  batch 6000 loss: 1.5502357528209687\n",
      "  batch 7000 loss: 1.5562588797807693\n",
      "  batch 8000 loss: 1.5539339250326156\n",
      "  batch 9000 loss: 1.5540220334529877\n",
      "Epoch:  17\n",
      "  batch 0 loss: 1.5435532060861588\n",
      "  batch 1000 loss: 1.5598232041597366\n",
      "  batch 2000 loss: 1.5622623633146286\n",
      "  batch 3000 loss: 1.5530950983762741\n",
      "  batch 4000 loss: 1.5424904061555862\n",
      "  batch 5000 loss: 1.5565042667388915\n",
      "  batch 6000 loss: 1.5463602442741393\n",
      "  batch 7000 loss: 1.5428460373878479\n",
      "  batch 8000 loss: 1.5433609728813171\n",
      "  batch 9000 loss: 1.5404500899314881\n",
      "Epoch:  18\n",
      "  batch 0 loss: 1.5467663197517394\n",
      "  batch 1000 loss: 1.5546370841264725\n",
      "  batch 2000 loss: 1.5467524292469024\n",
      "  batch 3000 loss: 1.553883590579033\n",
      "  batch 4000 loss: 1.5377091275453567\n",
      "  batch 5000 loss: 1.5291862733364106\n",
      "  batch 6000 loss: 1.543000612616539\n",
      "  batch 7000 loss: 1.5357426717281342\n",
      "  batch 8000 loss: 1.5437446074485779\n",
      "  batch 9000 loss: 1.5481798421144486\n",
      "Epoch:  19\n",
      "  batch 0 loss: 1.5448274890184401\n",
      "  batch 1000 loss: 1.5442811950445174\n",
      "  batch 2000 loss: 1.546593830227852\n",
      "  batch 3000 loss: 1.5425310065746307\n",
      "  batch 4000 loss: 1.5422366973161696\n",
      "  batch 5000 loss: 1.5340881201028824\n",
      "  batch 6000 loss: 1.5328903276920318\n",
      "  batch 7000 loss: 1.5420557553768157\n",
      "  batch 8000 loss: 1.5342137539386749\n",
      "  batch 9000 loss: 1.5339217014312745\n",
      "Epoch:  20\n",
      "  batch 0 loss: 1.533716464996338\n",
      "  batch 1000 loss: 1.536932089328766\n",
      "  batch 2000 loss: 1.531975630402565\n",
      "  batch 3000 loss: 1.5380960609912873\n",
      "  batch 4000 loss: 1.5368871001005173\n",
      "  batch 5000 loss: 1.537114938020706\n",
      "  batch 6000 loss: 1.5355715041160583\n",
      "  batch 7000 loss: 1.5246457642316817\n",
      "  batch 8000 loss: 1.5356950225830077\n",
      "  batch 9000 loss: 1.5305031481981277\n",
      "Epoch:  21\n",
      "  batch 0 loss: 1.5333460562229158\n",
      "  batch 1000 loss: 1.533183224916458\n",
      "  batch 2000 loss: 1.529193046450615\n",
      "  batch 3000 loss: 1.5262783885002136\n",
      "  batch 4000 loss: 1.538214445590973\n",
      "  batch 5000 loss: 1.5393827323913574\n",
      "  batch 6000 loss: 1.5231121282577516\n",
      "  batch 7000 loss: 1.5285353145599365\n",
      "  batch 8000 loss: 1.5218146669864654\n",
      "  batch 9000 loss: 1.534420282959938\n",
      "Epoch:  22\n",
      "  batch 0 loss: 1.5246903585195541\n",
      "  batch 1000 loss: 1.522061824440956\n",
      "  batch 2000 loss: 1.5297144532203675\n",
      "  batch 3000 loss: 1.5335898032188415\n",
      "  batch 4000 loss: 1.5136501643657685\n",
      "  batch 5000 loss: 1.528156951546669\n",
      "  batch 6000 loss: 1.528478854417801\n",
      "  batch 7000 loss: 1.5252721985578537\n",
      "  batch 8000 loss: 1.5284262286424637\n",
      "  batch 9000 loss: 1.5211824939250946\n",
      "Epoch:  23\n",
      "  batch 0 loss: 1.532214775800705\n",
      "  batch 1000 loss: 1.5395384360551834\n",
      "  batch 2000 loss: 1.518010969877243\n",
      "  batch 3000 loss: 1.5189079818725586\n",
      "  batch 4000 loss: 1.5246058163642884\n",
      "  batch 5000 loss: 1.5172751047611237\n",
      "  batch 6000 loss: 1.5238911685943604\n",
      "  batch 7000 loss: 1.524837676525116\n",
      "  batch 8000 loss: 1.5147766053676606\n",
      "  batch 9000 loss: 1.5240218247175217\n",
      "Epoch:  24\n",
      "  batch 0 loss: 1.5249825562238692\n",
      "  batch 1000 loss: 1.5284277203083039\n",
      "  batch 2000 loss: 1.531010892868042\n",
      "  batch 3000 loss: 1.5140606487989425\n",
      "  batch 4000 loss: 1.5238560880422591\n",
      "  batch 5000 loss: 1.5158521190881729\n",
      "  batch 6000 loss: 1.506525307893753\n",
      "  batch 7000 loss: 1.5213986941576003\n",
      "  batch 8000 loss: 1.5251276422739029\n",
      "  batch 9000 loss: 1.500022377371788\n",
      "Epoch:  25\n",
      "  batch 0 loss: 1.5349840335845948\n",
      "  batch 1000 loss: 1.5199017273187638\n",
      "  batch 2000 loss: 1.518551463842392\n",
      "  batch 3000 loss: 1.51783871614933\n",
      "  batch 4000 loss: 1.5239466701745987\n",
      "  batch 5000 loss: 1.5153644044399261\n",
      "  batch 6000 loss: 1.5191334331035613\n",
      "  batch 7000 loss: 1.5069119509458542\n",
      "  batch 8000 loss: 1.5152519782781602\n",
      "  batch 9000 loss: 1.5167136024236678\n",
      "Epoch:  26\n",
      "  batch 0 loss: 1.5231903002262115\n",
      "  batch 1000 loss: 1.5208059618473053\n",
      "  batch 2000 loss: 1.5276051898002625\n",
      "  batch 3000 loss: 1.508293787240982\n",
      "  batch 4000 loss: 1.5102388242483138\n",
      "  batch 5000 loss: 1.5231259679794311\n",
      "  batch 6000 loss: 1.5110266225337983\n",
      "  batch 7000 loss: 1.5128665709495543\n",
      "  batch 8000 loss: 1.5082944765090942\n",
      "  batch 9000 loss: 1.5208517830371857\n",
      "Epoch:  27\n",
      "  batch 0 loss: 1.5103573153018952\n",
      "  batch 1000 loss: 1.518453503370285\n",
      "  batch 2000 loss: 1.512025641322136\n",
      "  batch 3000 loss: 1.5157471081018448\n",
      "  batch 4000 loss: 1.5238043396472931\n",
      "  batch 5000 loss: 1.504668823003769\n",
      "  batch 6000 loss: 1.5004656386375428\n",
      "  batch 7000 loss: 1.5212435079813003\n",
      "  batch 8000 loss: 1.5187101324796677\n",
      "  batch 9000 loss: 1.5128538917303085\n",
      "Epoch:  28\n",
      "  batch 0 loss: 1.5052349874973296\n",
      "  batch 1000 loss: 1.5193699723482132\n",
      "  batch 2000 loss: 1.5107181270122527\n",
      "  batch 3000 loss: 1.5110161733627319\n",
      "  batch 4000 loss: 1.516650293111801\n",
      "  batch 5000 loss: 1.5219691618680955\n",
      "  batch 6000 loss: 1.5019283692836762\n",
      "  batch 7000 loss: 1.5163604004383087\n",
      "  batch 8000 loss: 1.5077222875356675\n",
      "  batch 9000 loss: 1.5107272176742554\n",
      "Epoch:  29\n",
      "  batch 0 loss: 1.4994282916784287\n",
      "  batch 1000 loss: 1.5151704040765763\n",
      "  batch 2000 loss: 1.4965430098772048\n",
      "  batch 3000 loss: 1.5101004358530044\n",
      "  batch 4000 loss: 1.5094130067825318\n",
      "  batch 5000 loss: 1.5206625382900238\n",
      "  batch 6000 loss: 1.507962931394577\n",
      "  batch 7000 loss: 1.499784004330635\n",
      "  batch 8000 loss: 1.5118278410434722\n",
      "  batch 9000 loss: 1.5171990381479263\n",
      "Epoch:  30\n",
      "  batch 0 loss: 1.5104909582138062\n",
      "  batch 1000 loss: 1.5097140884399414\n",
      "  batch 2000 loss: 1.5132617542743683\n",
      "  batch 3000 loss: 1.522143378853798\n",
      "  batch 4000 loss: 1.5020269204378127\n",
      "  batch 5000 loss: 1.5084251778125763\n",
      "  batch 6000 loss: 1.5048591086864471\n",
      "  batch 7000 loss: 1.499333948969841\n",
      "  batch 8000 loss: 1.5111548085212707\n",
      "  batch 9000 loss: 1.4994699074029922\n",
      "Epoch:  31\n",
      "  batch 0 loss: 1.5142604067325591\n",
      "  batch 1000 loss: 1.5066128935813903\n",
      "  batch 2000 loss: 1.5222318818569183\n",
      "  batch 3000 loss: 1.5067806397676469\n",
      "  batch 4000 loss: 1.5052603026628495\n",
      "  batch 5000 loss: 1.4953919610977173\n",
      "  batch 6000 loss: 1.5024020764827728\n",
      "  batch 7000 loss: 1.5052148380279542\n",
      "  batch 8000 loss: 1.5039587612152099\n",
      "  batch 9000 loss: 1.51899970805645\n",
      "Epoch:  32\n",
      "  batch 0 loss: 1.5042709908485412\n",
      "  batch 1000 loss: 1.5071019307374953\n",
      "  batch 2000 loss: 1.513163298368454\n",
      "  batch 3000 loss: 1.5049371912479401\n",
      "  batch 4000 loss: 1.5067590005397797\n",
      "  batch 5000 loss: 1.505167644739151\n",
      "  batch 6000 loss: 1.5017888596057891\n",
      "  batch 7000 loss: 1.5046205451488495\n",
      "  batch 8000 loss: 1.5053394680023193\n",
      "  batch 9000 loss: 1.5119302659034728\n",
      "Epoch:  33\n",
      "  batch 0 loss: 1.4982687193155289\n",
      "  batch 1000 loss: 1.4995495961904526\n",
      "  batch 2000 loss: 1.5169341702461243\n",
      "  batch 3000 loss: 1.503925376176834\n",
      "  batch 4000 loss: 1.5054341189861298\n",
      "  batch 5000 loss: 1.4894383457899094\n",
      "  batch 6000 loss: 1.5060970976352692\n",
      "  batch 7000 loss: 1.4987025532722473\n",
      "  batch 8000 loss: 1.5059628249406816\n",
      "  batch 9000 loss: 1.50822107899189\n",
      "Epoch:  34\n",
      "  batch 0 loss: 1.5140282980203628\n",
      "  batch 1000 loss: 1.5001471556425094\n",
      "  batch 2000 loss: 1.4989993214607238\n",
      "  batch 3000 loss: 1.5064825131893158\n",
      "  batch 4000 loss: 1.506553205728531\n",
      "  batch 5000 loss: 1.4991896612644195\n",
      "  batch 6000 loss: 1.5118638887405396\n",
      "  batch 7000 loss: 1.511384898662567\n",
      "  batch 8000 loss: 1.5060294929742812\n",
      "  batch 9000 loss: 1.5041475425958633\n"
     ]
    }
   ],
   "source": [
    "# training loop from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)# for 3 heads lr=0.00001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "divisor = 0.\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"Epoch: \", i)\n",
    "    for j, (test_string_ord, test_string_ord_sr, test_string_oh, test_string_oh_sr, _, sequence_length) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        test_string_ord = torch.permute(test_string_ord, dims=[1,0])\n",
    "        test_string_ord_sr = torch.permute(test_string_ord_sr, dims=[1,0])\n",
    "        test_string_oh_sr = torch.permute(test_string_oh_sr, dims=[1,0,2])\n",
    "        \n",
    "        outputs = model(test_string_ord, test_string_ord_sr)\n",
    "        loss = loss_fn(torch.squeeze(outputs), torch.squeeze(test_string_oh_sr))\n",
    "        loss.backward()\n",
    "        \n",
    "        #break\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        divisor += float( list(test_string_ord.size())[1] )\n",
    "        if j % 1000 == 0:\n",
    "            last_loss = running_loss / divisor # loss per batch\n",
    "            print('  batch {} loss: {}'.format(j, last_loss))\n",
    "            running_loss = 0.\n",
    "            divisor = 0.\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9cf0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef5d0f-7bec-4029-8946-da09d9bc3d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
