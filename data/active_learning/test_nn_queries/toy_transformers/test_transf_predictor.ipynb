{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39070bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "from transformer_definitions import AuTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "MODEL_NAME = \"trained_model_overtrained.pk\"\n",
    "DATA_PATH = \"problem_1_train_dfa.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278250d",
   "metadata": {},
   "source": [
    "# Model and data definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0dd67aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        div_term = 10000 ** ( (2 * torch.arange(0, d_model) ) / d_model)\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        for i in range(max_len):\n",
    "            if i % 2 == 0:    \n",
    "                pe[i, 0, :] = torch.sin(position[i] / div_term)\n",
    "            else:\n",
    "                pe[i, 0, :] = torch.cos(position[i] / div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x #self.dropout(x)\n",
    "\n",
    "\n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "                \n",
    "        return x, attn_output, attn_output_weights\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None): #must be same as encoder\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.masked_mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, query: torch.Tensor=None, key: torch.Tensor=None):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.masked_mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))#, is_causal=True)\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        if query is None or key is None: # only for debugging\n",
    "            attn_output, attn_output_weights = self.mha(query=x, key=x, value=x)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = self.mha(query=query, key=key, value=x)\n",
    "        \n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class AuTransformer(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(alphabet_size+3, embedding_dim) # +3 for start, stop, padding symbol\n",
    "        self.encoder = Encoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        self.decoder = Decoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        \n",
    "        self.output_fnn = nn.Linear(in_features=embedding_dim, out_features=alphabet_size+3) # +2 for start and stop\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax_output = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.attention_output_layer = nn.Identity() \n",
    "        self.attention_weight_layer = nn.Identity() \n",
    "        \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
    "        x_src = self.input_embedding(src)\n",
    "        x, attention_output, attention_weights = self.encoder(x_src)\n",
    "        attention_output = self.attention_output_layer(attention_output)\n",
    "        attention_weights = self.attention_weight_layer(attention_weights)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x_tgt = self.input_embedding(tgt)\n",
    "        x = self.decoder(x=x_tgt, query=x, key=x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.gelu(self.output_fnn(x))\n",
    "        x = self.softmax_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c451eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, datapath: str, maxlen: int, pad_sequences: bool=True, max_sequences: int=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert(os.path.isfile(datapath))\n",
    "        self.symbol_dict = dict()\n",
    "        self.label_dict = dict()\n",
    "        self.sequences, self.labels, self.sequence_lengths = self._read_sequences(datapath, max_sequences)\n",
    "        print(\"Sequences loaded. Some examples: \\n{}\".format(self.sequences[:3]))\n",
    "        \n",
    "        self.SOS = self.alphabet_size\n",
    "        self.EOS = self.alphabet_size + 1\n",
    "        self.PAD = self.alphabet_size + 2\n",
    "        self.maxlen = maxlen + 2  # +2 for EOS/PAD and SOS \n",
    "        self.pad_sequences = pad_sequences\n",
    "        \n",
    "    def encode_sequences(self):\n",
    "        self.ordinal_seq, self.ordinal_seq_sr = self._ordinal_encode_sequences(self.sequences)\n",
    "        self.one_hot_seq, self.one_hot_seq_sr = self._one_hot_encode_sequences(self.sequences)\n",
    "        \n",
    "        del self.sequences\n",
    "        self.sequences = None\n",
    "        \n",
    "        print(\"The symbol dictionary: {}\".format(self.symbol_dict))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ordinal_seq[idx], self.ordinal_seq_sr[idx], self.one_hot_seq[idx], \\\n",
    "               self.one_hot_seq_sr[idx], self.labels[idx], self.sequence_lengths[idx]\n",
    "       \n",
    "    def _read_sequences(self, datapath: str, max_sequences: int):\n",
    "        sequences = list()\n",
    "        labels = list()\n",
    "        sequence_lengths = list()\n",
    "        \n",
    "        for i, line in enumerate(open(datapath)):\n",
    "            if i == 0:\n",
    "                line = line.split()\n",
    "                self.alphabet_size = int(line[1])\n",
    "                print(\"Alphabet size: \", self.alphabet_size)\n",
    "                continue\n",
    "            elif max_sequences and i-1 >= max_sequences:\n",
    "                break\n",
    "            \n",
    "            line = line.split()\n",
    "            label = line[0]\n",
    "            if not label in self.label_dict:\n",
    "                self.label_dict[label] = len(self.label_dict)\n",
    "            label = self.label_dict[label]\n",
    "            labels.append(label)\n",
    "            \n",
    "            sequences.append(line[2:])\n",
    "            sequence_lengths.append(len(line) - 1)\n",
    "        return sequences, labels, sequence_lengths\n",
    "    \n",
    "    def _pad_one_hot(self, sequences: list, do_eos: bool=False):\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            #print(\"Before one hot:\\n{}\".format(seq))\n",
    "            current_size = len(seq)\n",
    "            \n",
    "            t = torch.zeros((self.maxlen - current_size, self.alphabet_size + 3), dtype=torch.float32)\n",
    "            t[:, self.PAD] = 1\n",
    "            if do_eos and self.maxlen > current_size:\n",
    "                t[0, self.PAD] = 0\n",
    "                t[0, self.EOS] = 1\n",
    "            \n",
    "            seq = torch.cat((seq, t), dim=0)\n",
    "            sequences[i] = seq\n",
    "            #print(\"After one hot:\\n{}\".format(seq))\n",
    "        return sequences\n",
    "    \n",
    "    def _one_hot_encode_sequences(self, strings: list):\n",
    "        res = list()\n",
    "        res_sr = list()\n",
    "        for string in strings:\n",
    "            x1, x2 = self._one_hot_encode_string(string)\n",
    "            res.append(x1)\n",
    "            res_sr.append(x2)\n",
    "            \n",
    "        if self.pad_sequences:\n",
    "            res = self._pad_one_hot(res)\n",
    "            res_sr = self._pad_one_hot(res_sr)\n",
    "\n",
    "        return res, res_sr\n",
    "    \n",
    "    def _one_hot_encode_string(self, string: list):\n",
    "        encoded_string = torch.zeros((len(string)+2, self.alphabet_size + 3), dtype=torch.float32) # alphabet_size + 3 because SOS, EOS, padding token\n",
    "        encoded_string[0][self.SOS] = 1\n",
    "        encoded_string[-1][self.EOS] = 1\n",
    "\n",
    "        encoded_string_sl = torch.zeros((len(string)+2, self.alphabet_size + 3), dtype=torch.float32)\n",
    "        encoded_string_sl[-2][self.EOS] = 1\n",
    "        encoded_string_sl[-1][self.PAD] = 1\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if not symbol in self.symbol_dict:\n",
    "                self.symbol_dict[symbol] = len(self.symbol_dict)\n",
    "\n",
    "            encoded_string[i+1][self.symbol_dict[symbol]] = 1\n",
    "            encoded_string_sl[i][self.symbol_dict[symbol]] = 1\n",
    "        encoded_string_sl.requires_grad_()\n",
    "        return encoded_string, encoded_string_sl\n",
    "    \n",
    "    def _pad_ordinal(self, sequences: list, do_eos: bool=False):\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            #print(\"Before ordinal:{}\".format(seq))\n",
    "            current_size = len(seq)\n",
    "            \n",
    "            t = torch.ones((self.maxlen - current_size,), dtype=torch.long)\n",
    "            t = t*self.PAD \n",
    "            if do_eos and self.maxlen > current_size:\n",
    "                t[0] = self.EOS\n",
    "            \n",
    "            seq = torch.cat((seq, t), dim=0)\n",
    "            sequences[i] = seq\n",
    "            #print(\"After ordinal:{}\".format(seq))\n",
    "        return sequences\n",
    "    \n",
    "    def _ordinal_encode_sequences(self, strings: list):\n",
    "        res = list()\n",
    "        res_sr = list()\n",
    "        for string in strings:\n",
    "            x1, x2 = self._ordinal_encode_string(string)\n",
    "            res.append(x1)\n",
    "            res_sr.append(x2)\n",
    "        \n",
    "        if self.pad_sequences: \n",
    "            res = self._pad_ordinal(res)\n",
    "            res_sr = self._pad_ordinal(res_sr)\n",
    "        return res, res_sr\n",
    "    \n",
    "    def _ordinal_encode_string(self, string: list):\n",
    "        encoded_string = torch.zeros((len(string)+2,), dtype=torch.long)\n",
    "        encoded_string[0] = self.SOS\n",
    "        encoded_string[-1] = self.EOS\n",
    "\n",
    "        encoded_string_sl = torch.zeros((len(string)+2,), dtype=torch.long)\n",
    "        encoded_string_sl[-2] = self.EOS\n",
    "        encoded_string_sl[-1] = self.PAD\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if not symbol in self.symbol_dict:\n",
    "                self.symbol_dict[symbol] = len(self.symbol_dict)\n",
    "\n",
    "            encoded_string[i+1] = self.symbol_dict[symbol]\n",
    "            encoded_string_sl[i] = self.symbol_dict[symbol]\n",
    "        return encoded_string, encoded_string_sl\n",
    "    \n",
    "    def get_alphabet_size(self):\n",
    "        return self.alphabet_size\n",
    "    \n",
    "    def initialize(self, path: str=\"dataset.pk\"):\n",
    "        data = pk.load(open(path, \"rb\"))\n",
    "        self.alphabet_size = self.alphabet_size\n",
    "        self.symbol_dict = self.symbol_dict\n",
    "        self.label_dict = self.label_dict\n",
    "        \n",
    "    def save_state(self, path: str=\"dataset.pk\"):\n",
    "        data = dict()\n",
    "        data[\"alphabet_size\"] = self.alphabet_size\n",
    "        data[\"symbol_dict\"] = self.symbol_dict\n",
    "        data[\"label_dict\"] = self.label_dict\n",
    "        pk.dump(data, open(path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768910a",
   "metadata": {},
   "source": [
    "# Check if the model learned the sequences properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "204beadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuTransformer(\n",
       "  (input_embedding): Embedding(7, 3)\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (masked_mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_fnn): Linear(in_features=3, out_features=7, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (softmax_output): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(MODEL_NAME)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb7f1b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet size:  4\n",
      "Sequences loaded. Some examples: \n",
      "[['a', 'b'], ['a', 'b'], ['c', 'd', 'a', 'b']]\n",
      "The symbol dictionary: {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n"
     ]
    }
   ],
   "source": [
    "dataset = SequenceDataset(DATA_PATH, maxlen=10, max_sequences=None)\n",
    "dataset.initialize()\n",
    "dataset.encode_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62e93138",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dda7bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_seq, ordinal_seq_sr, one_hot_seq, one_hot_seq_sr, label, sequence_length = dataset[test_idx]\n",
    "#ordinal_seq, ordinal_seq_sr, one_hot_seq, one_hot_seq_sr, label, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "245165a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AuTransformer' object has no attribute 'attention_output_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordinal_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordinal_seq_sr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 103\u001b[0m, in \u001b[0;36mAuTransformer.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#print(\"After_embedding: \", list(x_src.size()))\u001b[39;00m\n\u001b[1;32m    102\u001b[0m x, attention_output, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x_src)\n\u001b[0;32m--> 103\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_output_layer\u001b[49m(attention_output)\n\u001b[1;32m    104\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_weight_layer(attention_weights)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#print(\"After_encoder: \", list(x.size()))\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AuTransformer' object has no attribute 'attention_output_layer'"
     ]
    }
   ],
   "source": [
    "res = model(torch.unsqueeze(ordinal_seq, -1), torch.unsqueeze(ordinal_seq_sr, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8ecdc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([12, 1, 7], [12])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(res.size()), list(ordinal_seq_sr.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55825ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 3, 2, 3, 2, 3, 5, 6, 6, 6, 6, 6]),\n",
       " tensor([2, 3, 2, 3, 2, 3, 5, 6, 6, 6, 6, 6]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.squeeze(res), dim=1), ordinal_seq_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a679873",
   "metadata": {},
   "source": [
    "# Inspect the internal representations of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "516fd548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f02ad1589d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tutorial on hooks: https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/\n",
    "\n",
    "activation = {}\n",
    "def getActivation(name):\n",
    "    global activation\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        global activation\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.input_embedding.register_forward_hook(getActivation(\"embedding\"))\n",
    "#model.encoder.register_forward_hook(getActivation(\"encoder\"))\n",
    "model.encoder.mha.out_proj.register_forward_hook(getActivation(\"encoder_mha_outproj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a63aaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model(torch.unsqueeze(ordinal_seq, -1), torch.unsqueeze(ordinal_seq_sr, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20be648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding: [12, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "for name, t in activation.items():\n",
    "    print(\"{}: {}\".format(name, list(t.size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bdeeeefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding': tensor([[[ 1.5475,  0.1554, -0.8117]],\n",
       " \n",
       "         [[-2.6436, -0.1099,  0.0849]],\n",
       " \n",
       "         [[ 1.5475,  0.1554, -0.8117]],\n",
       " \n",
       "         [[-2.6436, -0.1099,  0.0849]],\n",
       " \n",
       "         [[ 1.5475,  0.1554, -0.8117]],\n",
       " \n",
       "         [[-2.6436, -0.1099,  0.0849]],\n",
       " \n",
       "         [[ 0.6160,  1.3943,  0.1340]],\n",
       " \n",
       "         [[ 1.5180, -0.8475,  4.9943]],\n",
       " \n",
       "         [[ 1.5180, -0.8475,  4.9943]],\n",
       " \n",
       "         [[ 1.5180, -0.8475,  4.9943]],\n",
       " \n",
       "         [[ 1.5180, -0.8475,  4.9943]],\n",
       " \n",
       "         [[ 1.5180, -0.8475,  4.9943]]])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33457357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
