{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf20ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "from transformer_definitions import AuTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "MODEL_NAME = \"trained_model.pk\"\n",
    "DATA_PATH = \"problem_1_train_dfa.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d7d64",
   "metadata": {},
   "source": [
    "# Model and data definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7131aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        div_term = 10000 ** ( (2 * torch.arange(0, d_model) ) / d_model)\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        for i in range(max_len):\n",
    "            if i % 2 == 0:    \n",
    "                pe[i, 0, :] = torch.sin(position[i] / div_term)\n",
    "            else:\n",
    "                pe[i, 0, :] = torch.cos(position[i] / div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x #self.dropout(x)\n",
    "\n",
    "\n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "                \n",
    "        return x, attn_output, attn_output_weights\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None): #must be same as encoder\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.masked_mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, query: torch.Tensor=None, key: torch.Tensor=None):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.masked_mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))#, is_causal=True)\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        if query is None or key is None: # only for debugging\n",
    "            attn_output, attn_output_weights = self.mha(query=x, key=x, value=x)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = self.mha(query=query, key=key, value=x)\n",
    "        \n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class AuTransformer(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(alphabet_size+3, embedding_dim) # +3 for start, stop, padding symbol\n",
    "        self.encoder = Encoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        self.decoder = Decoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        \n",
    "        self.output_fnn = nn.Linear(in_features=embedding_dim, out_features=alphabet_size+3) # +2 for start and stop\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax_output = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.attention_output_layer = nn.Identity() \n",
    "        self.attention_weight_layer = nn.Identity() \n",
    "        \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
    "        x_src = self.input_embedding(src)\n",
    "        x, attention_output, attention_weights = self.encoder(x_src)\n",
    "        #print(\"Before: \", attention_weights)\n",
    "        attention_output = self.attention_output_layer(attention_output)\n",
    "        attention_weights = self.attention_weight_layer(attention_weights)\n",
    "        #print(\"After: \", attention_weights)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x_tgt = self.input_embedding(tgt)\n",
    "        x = self.decoder(x=x_tgt, query=x, key=x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.gelu(self.output_fnn(x))\n",
    "        x = self.softmax_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02aa92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, datapath: str, maxlen: int, pad_sequences: bool=True, max_sequences: int=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert(os.path.isfile(datapath))\n",
    "        self.symbol_dict = dict()\n",
    "        self.label_dict = dict()\n",
    "        self.sequences, self.labels, self.sequence_lengths = self._read_sequences(datapath, max_sequences)\n",
    "        print(\"Sequences loaded. Some examples: \\n{}\".format(self.sequences[:3]))\n",
    "        \n",
    "        self.SOS = self.alphabet_size\n",
    "        self.EOS = self.alphabet_size + 1\n",
    "        self.PAD = self.alphabet_size + 2\n",
    "        self.maxlen = maxlen + 2  # +2 for EOS/PAD and SOS \n",
    "        self.pad_sequences = pad_sequences\n",
    "        \n",
    "    def encode_sequences(self):\n",
    "        self.ordinal_seq, self.ordinal_seq_sr = self._ordinal_encode_sequences(self.sequences)\n",
    "        self.one_hot_seq, self.one_hot_seq_sr = self._one_hot_encode_sequences(self.sequences)\n",
    "        \n",
    "        del self.sequences\n",
    "        self.sequences = None\n",
    "        \n",
    "        print(\"The symbol dictionary: {}\".format(self.symbol_dict))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ordinal_seq[idx], self.ordinal_seq_sr[idx], self.one_hot_seq[idx], \\\n",
    "               self.one_hot_seq_sr[idx], self.labels[idx], self.sequence_lengths[idx]\n",
    "       \n",
    "    def _read_sequences(self, datapath: str, max_sequences: int):\n",
    "        sequences = list()\n",
    "        labels = list()\n",
    "        sequence_lengths = list()\n",
    "        \n",
    "        for i, line in enumerate(open(datapath)):\n",
    "            if i == 0:\n",
    "                line = line.split()\n",
    "                self.alphabet_size = int(line[1])\n",
    "                print(\"Alphabet size: \", self.alphabet_size)\n",
    "                continue\n",
    "            elif max_sequences and i-1 >= max_sequences:\n",
    "                break\n",
    "            \n",
    "            line = line.split()\n",
    "            label = line[0]\n",
    "            if not label in self.label_dict:\n",
    "                self.label_dict[label] = len(self.label_dict)\n",
    "            label = self.label_dict[label]\n",
    "            labels.append(label)\n",
    "            \n",
    "            sequences.append(line[2:])\n",
    "            sequence_lengths.append(len(line) - 1)\n",
    "        return sequences, labels, sequence_lengths\n",
    "    \n",
    "    def _pad_one_hot(self, sequences: list, do_eos: bool=False):\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            #print(\"Before one hot:\\n{}\".format(seq))\n",
    "            current_size = len(seq)\n",
    "            \n",
    "            t = torch.zeros((self.maxlen - current_size, self.alphabet_size + 3), dtype=torch.float32)\n",
    "            t[:, self.PAD] = 1\n",
    "            if do_eos and self.maxlen > current_size:\n",
    "                t[0, self.PAD] = 0\n",
    "                t[0, self.EOS] = 1\n",
    "            \n",
    "            seq = torch.cat((seq, t), dim=0)\n",
    "            sequences[i] = seq\n",
    "            #print(\"After one hot:\\n{}\".format(seq))\n",
    "        return sequences\n",
    "    \n",
    "    def _one_hot_encode_sequences(self, strings: list):\n",
    "        res = list()\n",
    "        res_sr = list()\n",
    "        for string in strings:\n",
    "            x1, x2 = self._one_hot_encode_string(string)\n",
    "            res.append(x1)\n",
    "            res_sr.append(x2)\n",
    "            \n",
    "        if self.pad_sequences:\n",
    "            res = self._pad_one_hot(res)\n",
    "            res_sr = self._pad_one_hot(res_sr)\n",
    "\n",
    "        return res, res_sr\n",
    "    \n",
    "    def _one_hot_encode_string(self, string: list):\n",
    "        encoded_string = torch.zeros((len(string)+2, self.alphabet_size + 3), dtype=torch.float32) # alphabet_size + 3 because SOS, EOS, padding token\n",
    "        encoded_string[0][self.SOS] = 1\n",
    "        encoded_string[-1][self.EOS] = 1\n",
    "\n",
    "        encoded_string_sl = torch.zeros((len(string)+2, self.alphabet_size + 3), dtype=torch.float32)\n",
    "        encoded_string_sl[-2][self.EOS] = 1\n",
    "        encoded_string_sl[-1][self.PAD] = 1\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if not symbol in self.symbol_dict:\n",
    "                self.symbol_dict[symbol] = len(self.symbol_dict)\n",
    "\n",
    "            encoded_string[i+1][self.symbol_dict[symbol]] = 1\n",
    "            encoded_string_sl[i][self.symbol_dict[symbol]] = 1\n",
    "        encoded_string_sl.requires_grad_()\n",
    "        return encoded_string, encoded_string_sl\n",
    "    \n",
    "    def _pad_ordinal(self, sequences: list, do_eos: bool=False):\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            #print(\"Before ordinal:{}\".format(seq))\n",
    "            current_size = len(seq)\n",
    "            \n",
    "            t = torch.ones((self.maxlen - current_size,), dtype=torch.long)\n",
    "            t = t*self.PAD \n",
    "            if do_eos and self.maxlen > current_size:\n",
    "                t[0] = self.EOS\n",
    "            \n",
    "            seq = torch.cat((seq, t), dim=0)\n",
    "            sequences[i] = seq\n",
    "            #print(\"After ordinal:{}\".format(seq))\n",
    "        return sequences\n",
    "    \n",
    "    def _ordinal_encode_sequences(self, strings: list):\n",
    "        res = list()\n",
    "        res_sr = list()\n",
    "        for string in strings:\n",
    "            x1, x2 = self._ordinal_encode_string(string)\n",
    "            res.append(x1)\n",
    "            res_sr.append(x2)\n",
    "        \n",
    "        if self.pad_sequences: \n",
    "            res = self._pad_ordinal(res)\n",
    "            res_sr = self._pad_ordinal(res_sr)\n",
    "        return res, res_sr\n",
    "    \n",
    "    def _ordinal_encode_string(self, string: list):\n",
    "        encoded_string = torch.zeros((len(string)+2,), dtype=torch.long)\n",
    "        encoded_string[0] = self.SOS\n",
    "        encoded_string[-1] = self.EOS\n",
    "\n",
    "        encoded_string_sl = torch.zeros((len(string)+2,), dtype=torch.long)\n",
    "        encoded_string_sl[-2] = self.EOS\n",
    "        encoded_string_sl[-1] = self.PAD\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if not symbol in self.symbol_dict:\n",
    "                self.symbol_dict[symbol] = len(self.symbol_dict)\n",
    "\n",
    "            encoded_string[i+1] = self.symbol_dict[symbol]\n",
    "            encoded_string_sl[i] = self.symbol_dict[symbol]\n",
    "        return encoded_string, encoded_string_sl\n",
    "    \n",
    "    def get_alphabet_size(self):\n",
    "        return self.alphabet_size\n",
    "    \n",
    "    def initialize(self, path: str=\"dataset.pk\"):\n",
    "        data = pk.load(open(path, \"rb\"))\n",
    "        self.alphabet_size = self.alphabet_size\n",
    "        self.symbol_dict = self.symbol_dict\n",
    "        self.label_dict = self.label_dict\n",
    "        \n",
    "    def save_state(self, path: str=\"dataset.pk\"):\n",
    "        data = dict()\n",
    "        data[\"alphabet_size\"] = self.alphabet_size\n",
    "        data[\"symbol_dict\"] = self.symbol_dict\n",
    "        data[\"label_dict\"] = self.label_dict\n",
    "        pk.dump(data, open(path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf3533",
   "metadata": {},
   "source": [
    "# Check if the model learned the sequences properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d11967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuTransformer(\n",
       "  (input_embedding): Embedding(7, 3)\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (masked_mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_fnn): Linear(in_features=3, out_features=7, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (softmax_output): Softmax(dim=-1)\n",
       "  (attention_output_layer): Identity()\n",
       "  (attention_weight_layer): Identity()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(MODEL_NAME)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c7f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet size:  4\n",
      "Sequences loaded. Some examples: \n",
      "[['a', 'b'], ['a', 'b'], ['c', 'd', 'a', 'b']]\n",
      "The symbol dictionary: {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n"
     ]
    }
   ],
   "source": [
    "dataset = SequenceDataset(DATA_PATH, maxlen=10, max_sequences=None)\n",
    "dataset.initialize()\n",
    "dataset.encode_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e59911",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdec69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_seq, ordinal_seq_sr, one_hot_seq, one_hot_seq_sr, label, sequence_length = dataset[test_idx]\n",
    "#ordinal_seq, ordinal_seq_sr, one_hot_seq, one_hot_seq_sr, label, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce807220",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(torch.unsqueeze(ordinal_seq, -1), torch.unsqueeze(ordinal_seq_sr, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8849f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([12, 1, 7], [12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(res.size()), list(ordinal_seq_sr.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a283293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 3, 2, 3, 2, 3, 5, 6, 6, 6, 6, 6]),\n",
       " tensor([2, 3, 2, 3, 2, 3, 5, 6, 6, 6, 6, 6]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.squeeze(res), dim=1), ordinal_seq_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38eab1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1780e-12, 1.4222e-06, 1.0000e+00, 1.1780e-12, 9.9387e-13, 4.5924e-07,\n",
       "         1.1780e-12],\n",
       "        [4.7549e-12, 4.7549e-12, 4.7549e-12, 1.0000e+00, 4.0117e-12, 1.6505e-06,\n",
       "         1.8015e-07],\n",
       "        [1.1664e-12, 1.8209e-06, 1.0000e+00, 1.1664e-12, 9.8408e-13, 3.7062e-07,\n",
       "         1.1664e-12],\n",
       "        [4.8074e-12, 4.8074e-12, 4.8074e-12, 1.0000e+00, 4.0560e-12, 1.3310e-06,\n",
       "         2.3238e-07],\n",
       "        [1.2104e-12, 8.4219e-07, 1.0000e+00, 1.2104e-12, 1.0212e-12, 7.2722e-07,\n",
       "         1.2104e-12],\n",
       "        [5.4800e-12, 5.4800e-12, 5.4800e-12, 1.0000e+00, 4.6234e-12, 3.1223e-07,\n",
       "         1.3728e-06],\n",
       "        [1.1181e-12, 1.1181e-12, 2.2798e-07, 1.9881e-06, 9.4330e-13, 1.0000e+00,\n",
       "         1.1181e-12],\n",
       "        [6.6800e-08, 7.7910e-13, 7.7910e-13, 3.1592e-07, 6.5732e-13, 7.7910e-13,\n",
       "         1.0000e+00],\n",
       "        [1.9566e-08, 9.2453e-13, 9.2453e-13, 9.2743e-07, 7.8002e-13, 9.2453e-13,\n",
       "         1.0000e+00],\n",
       "        [2.5560e-07, 6.8659e-13, 6.8659e-13, 1.0349e-07, 5.7927e-13, 6.8659e-13,\n",
       "         1.0000e+00],\n",
       "        [5.1294e-07, 6.5834e-13, 6.5834e-13, 5.9379e-08, 5.5544e-13, 6.5834e-13,\n",
       "         1.0000e+00],\n",
       "        [1.8596e-07, 7.0359e-13, 7.0359e-13, 1.3408e-07, 5.9361e-13, 7.0359e-13,\n",
       "         1.0000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942ab96",
   "metadata": {},
   "source": [
    "# Inspect the internal representations of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75a0a2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f938575e200>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tutorial on hooks: https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/\n",
    "\n",
    "activation = {}\n",
    "def getActivation(name):\n",
    "    global activation\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        global activation\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.input_embedding.register_forward_hook(getActivation(\"embedding\"))\n",
    "#model.encoder.register_forward_hook(getActivation(\"encoder\"))\n",
    "model.attention_output_layer.register_forward_hook(getActivation(\"attention_output\"))\n",
    "model.attention_weight_layer.register_forward_hook(getActivation(\"attention_weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4f75248",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model(torch.unsqueeze(ordinal_seq, -1), torch.unsqueeze(ordinal_seq_sr, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b494c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding: [12, 1, 3]\n",
      "attention_output: [12, 1, 3]\n",
      "attention_weights: [1, 12, 12]\n"
     ]
    }
   ],
   "source": [
    "for name, t in activation.items():\n",
    "    print(\"{}: {}\".format(name, list(t.size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aed3664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([12, 12],\n",
       " tensor([[4.2063e-03, 3.8363e-02, 4.4841e-02, 2.5555e-01, 3.5213e-01, 5.2729e-02,\n",
       "          1.9417e-01, 1.5719e-03, 4.4500e-03, 1.6906e-02, 2.9645e-02, 5.4369e-03],\n",
       "         [2.4204e-03, 2.3007e-02, 4.7571e-02, 1.6831e-01, 4.1408e-01, 3.2123e-02,\n",
       "          2.2197e-01, 1.9804e-03, 5.0910e-03, 3.5400e-02, 3.7290e-02, 1.0764e-02],\n",
       "         [5.2721e-02, 1.0247e-02, 2.3416e-02, 2.6651e-03, 5.4248e-03, 8.1750e-03,\n",
       "          8.2890e-03, 3.9164e-01, 1.2639e-01, 1.0442e-01, 3.2908e-02, 2.3371e-01],\n",
       "         [3.1594e-03, 1.9194e-02, 6.1075e-02, 9.9865e-02, 3.6742e-01, 2.5309e-02,\n",
       "          2.1938e-01, 6.0317e-03, 1.1222e-02, 9.3844e-02, 5.8515e-02, 3.4986e-02],\n",
       "         [2.3977e-02, 2.8375e-03, 1.0347e-02, 5.0535e-04, 1.5900e-03, 2.1244e-03,\n",
       "          2.7387e-03, 4.4890e-01, 1.0146e-01, 1.0176e-01, 1.8106e-02, 2.8565e-01],\n",
       "         [2.5558e-03, 2.2539e-02, 5.0098e-02, 1.5574e-01, 4.0992e-01, 3.1169e-02,\n",
       "          2.2374e-01, 2.4103e-03, 5.8696e-03, 4.2100e-02, 4.0614e-02, 1.3246e-02],\n",
       "         [3.0420e-02, 4.1037e-03, 1.3056e-02, 8.0661e-04, 2.2325e-03, 3.1236e-03,\n",
       "          3.7272e-03, 4.3806e-01, 1.0906e-01, 1.0255e-01, 2.1475e-02, 2.7138e-01],\n",
       "         [1.2995e-01, 4.1983e-03, 3.7715e-03, 2.1220e-04, 1.4701e-04, 2.5447e-03,\n",
       "          3.7492e-04, 6.7017e-01, 1.0210e-01, 1.1686e-02, 5.1558e-03, 6.9692e-02],\n",
       "         [1.5269e-01, 7.6949e-03, 6.7321e-03, 5.7007e-04, 3.9761e-04, 4.9731e-03,\n",
       "          8.9935e-04, 6.0125e-01, 1.1777e-01, 1.7115e-02, 8.7167e-03, 8.1202e-02],\n",
       "         [2.0300e-02, 1.3717e-05, 1.3651e-05, 2.5264e-08, 1.4548e-08, 4.7705e-06,\n",
       "          1.0495e-07, 9.4916e-01, 1.8763e-02, 2.6465e-04, 3.4523e-05, 1.1448e-02],\n",
       "         [7.5536e-02, 2.4096e-03, 3.2407e-03, 1.2591e-04, 1.3108e-04, 1.4687e-03,\n",
       "          3.3112e-04, 6.9655e-01, 9.8266e-02, 1.7062e-02, 5.1344e-03, 9.9749e-02],\n",
       "         [3.3140e-02, 2.9276e-05, 2.2512e-05, 6.6124e-08, 2.9931e-08, 1.0536e-05,\n",
       "          2.0243e-07, 9.3325e-01, 2.2388e-02, 2.8283e-04, 5.0490e-05, 1.0828e-02]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.squeeze(activation[\"attention_weights\"])\n",
    "list(attention_weights.size()), attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4501770e",
   "metadata": {},
   "source": [
    "# Cluster the representations and express sequences in terms of their clusters. Can you see something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baa50be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = None\n",
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "with torch.no_grad():\n",
    "    for j, (test_string_ord, test_string_ord_sr, _, _, label, sequence_length) in enumerate(train_dataloader):\n",
    "        if j == 1000:\n",
    "            break\n",
    "        #print(test_string_ord, test_string_ord_sr)\n",
    "        #test_string_ord = test_string_ord[:sequence_length+2]\n",
    "        #test_string_ord_sr = te\n",
    "        #break\n",
    "        \n",
    "        test_string_ord = torch.permute(test_string_ord, dims=[1,0])\n",
    "        test_string_ord_sr = torch.permute(test_string_ord_sr, dims=[1,0])\n",
    "        \n",
    "        res = model(test_string_ord, test_string_ord_sr)\n",
    "        res = torch.squeeze(res)\n",
    "        \n",
    "        attn_weights = torch.squeeze(activation[\"attention_weights\"])\n",
    "        \n",
    "        if weights is None:\n",
    "            weights = attn_weights[1:sequence_length+1]\n",
    "        else:\n",
    "            weights = torch.cat((weights, attn_weights[1:sequence_length+1]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bdd8763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6856, 12)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = weights.numpy()\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adae70fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=4)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a8388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
