{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b62867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb504d",
   "metadata": {},
   "source": [
    "## Some helpers for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1618ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = dict()\n",
    "\n",
    "def one_hot_encode_string(string: list, alphabet_size: int):\n",
    "    global mapping_dict\n",
    "    \n",
    "    SOS = alphabet_size\n",
    "    EOS = alphabet_size + 1\n",
    "    PAD = alphabet_size + 2\n",
    "    \n",
    "    encoded_string = np.zeros((len(string)+2, alphabet_size + 3)) # alphabet_size + 3 because SOS, EOS, padding token\n",
    "    encoded_string[0][SOS] = 1\n",
    "    encoded_string[-1][EOS] = 1\n",
    "    \n",
    "    encoded_string_sr = np.zeros((len(string)+2, alphabet_size + 3))\n",
    "    encoded_string_sr[1][SOS] = 1\n",
    "    encoded_string_sr[0][PAD] = 1\n",
    "    \n",
    "    for i, symbol in enumerate(string):\n",
    "        if not symbol in mapping_dict:\n",
    "            mapping_dict[symbol] = len(mapping_dict)\n",
    "        \n",
    "        encoded_string[i+1][mapping_dict[symbol]] = 1\n",
    "        encoded_string_sr[i+2][mapping_dict[symbol]] = 1\n",
    "    return encoded_string, encoded_string_sr\n",
    "\n",
    "def ordinal_encode_string(string: list, alphabet_size: int):\n",
    "    global mapping_dict\n",
    "    \n",
    "    SOS = alphabet_size\n",
    "    EOS = alphabet_size + 1\n",
    "    PAD = alphabet_size + 2\n",
    "    \n",
    "    encoded_string = np.zeros((len(string)+2, 1))\n",
    "    encoded_string[0] = SOS\n",
    "    encoded_string[-1] = EOS\n",
    "    \n",
    "    encoded_string_sr = np.zeros((len(string)+2, 1))\n",
    "    encoded_string_sr[1] = EOS\n",
    "    encoded_string_sr[0] = PAD\n",
    "    \n",
    "    for i, symbol in enumerate(string):\n",
    "        if not symbol in mapping_dict:\n",
    "            mapping_dict[symbol] = len(mapping_dict)\n",
    "        \n",
    "        encoded_string[i+1] = mapping_dict[symbol]\n",
    "        encoded_string_sr[i+2] = mapping_dict[symbol]\n",
    "    return encoded_string, encoded_string_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7eec33",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61da3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using template from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "# tutorial about positional encoding: https://kikaben.com/transformers-positional-encoding/\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        div_term = 10000 ** ( (2 * torch.arange(0, d_model) ) / d_model)\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        for i in range(max_len):\n",
    "            if i % 2 == 0:    \n",
    "                pe[i, 0, :] = torch.sin(position[i] / div_term)\n",
    "            else:\n",
    "                pe[i, 0, :] = torch.cos(position[i] / div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x #self.dropout(x)\n",
    "\n",
    "\n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(alphabet_size+3, embedding_dim) if embedding_layer is None else embedding_layer # +3 for start, stop, padding symbol\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.input_embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None): #must be same as encoder\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(alphabet_size+3, embedding_dim) if embedding_layer is None else embedding_layer # +3 for start, stop, padding symbol\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.masked_mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, query: torch.Tensor=None, key: torch.Tensor=None):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        \n",
    "        x = self.input_embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.masked_mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))#, is_causal=True)\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        if query is None or key is None: # only for debugging\n",
    "            attn_output, attn_output_weights = self.mha(query=x, key=x, value=x)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = self.mha(query=query, key=key, value=x)\n",
    "        \n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac24b3",
   "metadata": {},
   "source": [
    "## Debugging encoder and decoder with a toy string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b14de1",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e796519f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [3.]]),\n",
       " array([[4.],\n",
       "        [3.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.]]),\n",
       " (5, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALPHABET_SIZE = 2\n",
    "test_string = [\"0\", \"1\", \"1\"]\n",
    "\n",
    "test_string_ord, test_string_ord_sr = ordinal_encode_string(test_string, ALPHABET_SIZE)\n",
    "test_string_ord, test_string_ord_sr, test_string_ord.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a49ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 1, 3],\n",
       " tensor([[[ 1.3217, -1.0965, -0.2252]],\n",
       " \n",
       "         [[ 1.2487, -1.1993, -0.0494]],\n",
       " \n",
       "         [[ 1.0641,  0.2746, -1.3387]],\n",
       " \n",
       "         [[-1.1065,  1.3160, -0.2095]],\n",
       " \n",
       "         [[-1.2807,  0.1208,  1.1599]]], grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Encoder(ALPHABET_SIZE, 3, max_len=30)\n",
    "test_res = model(torch.tensor(test_string_ord, dtype=torch.int32))\n",
    "list(test_res.size()), test_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6da585",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327b6fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 1, 3],\n",
       " tensor([[[ 1.3699, -0.3807, -0.9892]],\n",
       " \n",
       "         [[ 1.3300, -0.2487, -1.0813]],\n",
       " \n",
       "         [[ 0.4117,  0.9659, -1.3775]],\n",
       " \n",
       "         [[-1.3600,  1.0159,  0.3441]],\n",
       " \n",
       "         [[-1.1674, -0.1076,  1.2750]]], grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Decoder(ALPHABET_SIZE, 3, max_len=30)\n",
    "test_res = model(torch.tensor(test_string_ord, dtype=torch.int32))\n",
    "list(test_res.size()), test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53c28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class AuTransformer(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(alphabet_size+3, embedding_dim) # +3 for start, stop, padding symbol\n",
    "        self.encoder = Encoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        self.decoder = Decoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        \n",
    "        self.output_fnn = nn.Linear(in_features=embedding_dim, out_features=alphabet_size+3) # +2 for start and stop\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax_output = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
    "        x = self.encoder(src)\n",
    "        x = self.dropout(x)\n",
    "        x = self.decoder(x=tgt, query=x, key=x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.gelu(self.output_fnn(x))        \n",
    "        x = self.softmax_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f931b",
   "metadata": {},
   "source": [
    "## Debugging the whole network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b848d571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.]]),\n",
       " array([[0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string_oh, test_string_oh_sr = one_hot_encode_string(test_string, ALPHABET_SIZE)\n",
    "test_string_oh, test_string_oh_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5c848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.3690, 0.1370, 0.1465, 0.1751, 0.1723]],\n",
       " \n",
       "         [[0.1910, 0.1959, 0.2067, 0.2085, 0.1979]],\n",
       " \n",
       "         [[0.2938, 0.2124, 0.1581, 0.1540, 0.1817]],\n",
       " \n",
       "         [[0.1343, 0.3252, 0.2672, 0.1322, 0.1412]],\n",
       " \n",
       "         [[0.1061, 0.3750, 0.3009, 0.1064, 0.1116]]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " [5, 1, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuTransformer(ALPHABET_SIZE, 3, max_len=30)\n",
    "res = model(torch.tensor(test_string_ord, dtype=torch.int32), torch.tensor(test_string_ord_sr, dtype=torch.int32))\n",
    "res, list(res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3db8f9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuTransformer(\n",
       "  (input_embedding): Embedding(5, 3)\n",
       "  (encoder): Encoder(\n",
       "    (input_embedding): Embedding(5, 3)\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (input_embedding): Embedding(5, 3)\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (masked_mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_fnn): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (softmax_output): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c7922",
   "metadata": {},
   "source": [
    "## Overfit on a single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38aeae7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.7048, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.9048, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training loop from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()#nn.CrossEntropyLoss()\n",
    "\n",
    "test_string_ord = torch.tensor(test_string_ord, dtype=torch.long)\n",
    "test_string_ord_sr = torch.tensor(test_string_ord_sr, dtype=torch.long)\n",
    "\n",
    "test_string_oh = torch.tensor(test_string_oh, dtype=torch.float32, requires_grad=True)\n",
    "test_string_oh_sr = torch.tensor(test_string_oh_sr, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "loss_fn(test_string_oh, torch.squeeze(test_string_ord_sr.type(torch.LongTensor))), \\\n",
    "loss_fn(test_string_oh, torch.squeeze(test_string_ord.type(torch.LongTensor) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce45df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 1.517309710264206\n",
      "  batch 2000 loss: 1.5054040787220002\n",
      "  batch 3000 loss: 1.4933911712169647\n",
      "  batch 4000 loss: 1.481315294623375\n",
      "  batch 5000 loss: 1.4691589014530182\n",
      "  batch 6000 loss: 1.4567446029186248\n",
      "  batch 7000 loss: 1.4439928393363952\n",
      "  batch 8000 loss: 1.4311956275701523\n",
      "  batch 9000 loss: 1.4187696551084519\n",
      "  batch 10000 loss: 1.4067645512819291\n",
      "  batch 11000 loss: 1.3950567507743836\n",
      "  batch 12000 loss: 1.3835871900320054\n",
      "  batch 13000 loss: 1.3723446624279023\n",
      "  batch 14000 loss: 1.3613491530418396\n",
      "  batch 15000 loss: 1.3506313000917434\n",
      "  batch 16000 loss: 1.3402339998483659\n",
      "  batch 17000 loss: 1.330149579644203\n",
      "  batch 18000 loss: 1.3203274800777436\n",
      "  batch 19000 loss: 1.3107287448644638\n",
      "  batch 20000 loss: 1.3013671793937682\n",
      "  batch 21000 loss: 1.292262720823288\n",
      "  batch 22000 loss: 1.2834305545091629\n",
      "  batch 23000 loss: 1.2748874014616012\n",
      "  batch 24000 loss: 1.2666352112293244\n",
      "  batch 25000 loss: 1.2586599950790405\n",
      "  batch 26000 loss: 1.2509332666397095\n",
      "  batch 27000 loss: 1.2434236257076263\n",
      "  batch 28000 loss: 1.2361054396629334\n",
      "  batch 29000 loss: 1.2289699919223784\n",
      "  batch 30000 loss: 1.221985950112343\n",
      "  batch 31000 loss: 1.2151200454235076\n",
      "  batch 32000 loss: 1.2083351633548736\n",
      "  batch 33000 loss: 1.201621434688568\n",
      "  batch 34000 loss: 1.1949530198574065\n",
      "  batch 35000 loss: 1.1883198502063752\n",
      "  batch 36000 loss: 1.1817530072927476\n",
      "  batch 37000 loss: 1.175262316942215\n",
      "  batch 38000 loss: 1.168853920340538\n",
      "  batch 39000 loss: 1.1625341435670853\n",
      "  batch 40000 loss: 1.1563042877912522\n",
      "  batch 41000 loss: 1.150172903418541\n",
      "  batch 42000 loss: 1.1441393834352493\n",
      "  batch 43000 loss: 1.1381994631290435\n",
      "  batch 44000 loss: 1.1323534338474273\n",
      "  batch 45000 loss: 1.1265956996679305\n",
      "  batch 46000 loss: 1.1209208995103837\n",
      "  batch 47000 loss: 1.1153038840293885\n",
      "  batch 48000 loss: 1.1096836087703705\n",
      "  batch 49000 loss: 1.1040040119886398\n",
      "  batch 50000 loss: 1.0982495090961457\n",
      "  batch 51000 loss: 1.0924166421890258\n",
      "  batch 52000 loss: 1.0865122743844986\n",
      "  batch 53000 loss: 1.080541440963745\n",
      "  batch 54000 loss: 1.0745136712789536\n",
      "  batch 55000 loss: 1.0684383665323258\n",
      "  batch 56000 loss: 1.062322894334793\n",
      "  batch 57000 loss: 1.056180990934372\n",
      "  batch 58000 loss: 1.0500256116390227\n",
      "  batch 59000 loss: 1.0438723715543747\n",
      "  batch 60000 loss: 1.037740994811058\n",
      "  batch 61000 loss: 1.0316489728689193\n",
      "  batch 62000 loss: 1.0256179089546205\n",
      "  batch 63000 loss: 1.0196588629484176\n",
      "  batch 64000 loss: 1.0137851139307021\n",
      "  batch 65000 loss: 1.0080292164087294\n",
      "  batch 66000 loss: 1.0024106273055076\n",
      "  batch 67000 loss: 0.9969517086148262\n",
      "  batch 68000 loss: 0.9916719006896019\n",
      "  batch 69000 loss: 0.9865780938863754\n",
      "  batch 70000 loss: 0.9816816180348397\n",
      "  batch 71000 loss: 0.9769863190054894\n",
      "  batch 72000 loss: 0.9724991074800491\n",
      "  batch 73000 loss: 0.9682184072136879\n",
      "  batch 74000 loss: 0.9641465907692909\n",
      "  batch 75000 loss: 0.9602745155692101\n",
      "  batch 76000 loss: 0.9566014088988304\n",
      "  batch 77000 loss: 0.9531222470998764\n",
      "  batch 78000 loss: 0.9498308588862419\n",
      "  batch 79000 loss: 0.9467283134460449\n",
      "  batch 80000 loss: 0.9438049137592316\n",
      "  batch 81000 loss: 0.9410508471131325\n",
      "  batch 82000 loss: 0.9384638004302979\n",
      "  batch 83000 loss: 0.9360342497825622\n",
      "  batch 84000 loss: 0.9337545340657234\n",
      "  batch 85000 loss: 0.9316210948228836\n",
      "  batch 86000 loss: 0.9296286387443542\n",
      "  batch 87000 loss: 0.9277681443095207\n",
      "  batch 88000 loss: 0.9260325689911842\n",
      "  batch 89000 loss: 0.9244161034822465\n",
      "  batch 90000 loss: 0.9229104129076005\n",
      "  batch 91000 loss: 0.9215090126991272\n",
      "  batch 92000 loss: 0.9202083452939988\n",
      "  batch 93000 loss: 0.9190017038583755\n",
      "  batch 94000 loss: 0.9178832052946091\n",
      "  batch 95000 loss: 0.9168467148542404\n",
      "  batch 96000 loss: 0.9158874261975288\n",
      "  batch 97000 loss: 0.9150006248354912\n",
      "  batch 98000 loss: 0.9141816437244416\n",
      "  batch 99000 loss: 0.9134255017638206\n",
      "  batch 100000 loss: 0.9127279115915299\n"
     ]
    }
   ],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "\n",
    "for i in range(100000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(test_string_ord, test_string_ord_sr)\n",
    "    loss = loss_fn(torch.squeeze(outputs), test_string_oh_sr)\n",
    "    loss.backward()\n",
    "\n",
    "    # Adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Gather data and report\n",
    "    running_loss += loss.item()\n",
    "    if i % 1000 == 999:\n",
    "        last_loss = running_loss / 1000 # loss per batch\n",
    "        print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "        running_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05fbe533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[2.1130e-03, 2.4145e-03, 2.6499e-03, 2.0371e-03, 9.9079e-01]],\n",
       " \n",
       "         [[7.0820e-03, 6.4381e-03, 9.7242e-01, 5.9750e-03, 8.0843e-03]],\n",
       " \n",
       "         [[9.9079e-01, 3.1784e-03, 1.3962e-03, 1.1785e-03, 3.4584e-03]],\n",
       " \n",
       "         [[6.9245e-04, 9.9771e-01, 6.9072e-04, 4.1504e-04, 4.9191e-04]],\n",
       " \n",
       "         [[6.9256e-04, 9.9771e-01, 6.9064e-04, 4.1503e-04, 4.9190e-04]]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0., 0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.]], requires_grad=True),\n",
       " tensor([[0., 0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.]], requires_grad=True))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_string_ord, test_string_ord_sr), test_string_oh, test_string_oh_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f0582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
