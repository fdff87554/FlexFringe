{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b62867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DATA_PATH = \"problem_1_train_dfa.dat\"\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb504d",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbab3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, datapath: str, maxlen: int, pad_sequences: bool=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert(os.path.isfile(datapath))\n",
    "        self.symbol_dict = dict()\n",
    "        self.label_dict = dict()\n",
    "        sequences, self.labels, self.sequence_lengths = self._read_sequences(datapath)\n",
    "                \n",
    "        self.SOS = self.alphabet_size\n",
    "        self.EOS = self.alphabet_size + 1\n",
    "        self.PAD = self.alphabet_size + 2\n",
    "        self.maxlen = maxlen + 2  # +2 for EOS/PAD and SOS \n",
    "        self.pad_sequences = pad_sequences\n",
    "                \n",
    "        self.ordinal_seq, self.ordinal_seq_sr = self._ordinal_encode_sequences(sequences)\n",
    "        _, self.one_hot_seq_sr = self._one_hot_encode_sequences(sequences)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ordinal_seq[idx], self.ordinal_seq_sr[idx], self.one_hot_seq_sr[idx], \\\n",
    "               self.labels[idx], self.sequence_lengths[idx]\n",
    "       \n",
    "    def _read_sequences(self, datapath: str):\n",
    "        sequences = list()\n",
    "        labels = list()\n",
    "        sequence_lengths = list()\n",
    "        \n",
    "        for i, line in enumerate(open(datapath)):\n",
    "            if i == 0:\n",
    "                line = line.split()\n",
    "                self.alphabet_size = int(line[1])\n",
    "                print(\"Alphabet size: \", self.alphabet_size)\n",
    "                continue\n",
    "            \n",
    "            line = line.split()\n",
    "            label = line[0]\n",
    "            if not label in self.label_dict:\n",
    "                self.label_dict[label] = len(self.label_dict)\n",
    "            label = self.label_dict[label]\n",
    "            labels.append(label)\n",
    "            \n",
    "            sequences.append(line[2:])\n",
    "            sequence_lengths.append(len(line) - 1)\n",
    "        return sequences, labels, sequence_lengths\n",
    "    \n",
    "    def _pad_one_hot(self, sequences: list):\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            #print(\"Before one hot:\\n{}\".format(seq))\n",
    "            current_size = len(seq)\n",
    "            \n",
    "            t = torch.zeros((self.maxlen - current_size, self.alphabet_size + 3), dtype=torch.float32)\n",
    "            t[:, self.PAD] = 1 \n",
    "            seq = torch.cat((seq, t), dim=0)\n",
    "            sequences[i] = seq\n",
    "            #print(\"After one hot:\\n{}\".format(seq))\n",
    "        return sequences\n",
    "    \n",
    "    def _one_hot_encode_sequences(self, strings: list):\n",
    "        res = list()\n",
    "        res_sr = list()\n",
    "        for string in strings:\n",
    "            x1, x2 = self._one_hot_encode_string(string)\n",
    "            res.append(x1)\n",
    "            res_sr.append(x2)\n",
    "            \n",
    "        if self.pad_sequences:\n",
    "            res = self._pad_one_hot(res)\n",
    "            res_sr = self._pad_one_hot(res_sr)\n",
    "\n",
    "        return res, res_sr\n",
    "    \n",
    "    def _one_hot_encode_string(self, string: list):\n",
    "        encoded_string = torch.zeros((len(string)+2, self.alphabet_size + 3), dtype=torch.float32) # alphabet_size + 3 because SOS, EOS, padding token\n",
    "        encoded_string[0][self.SOS] = 1\n",
    "        encoded_string[-1][self.EOS] = 1\n",
    "\n",
    "        encoded_string_sr = torch.zeros((len(string)+2, self.alphabet_size + 3), dtype=torch.float32)\n",
    "        encoded_string_sr[1][self.SOS] = 1\n",
    "        encoded_string_sr[0][self.PAD] = 1\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if not symbol in self.symbol_dict:\n",
    "                self.symbol_dict[symbol] = len(self.symbol_dict)\n",
    "\n",
    "            encoded_string[i+1][self.symbol_dict[symbol]] = 1\n",
    "            encoded_string_sr[i+2][self.symbol_dict[symbol]] = 1\n",
    "        encoded_string_sr.requires_grad_()\n",
    "        return encoded_string, encoded_string_sr\n",
    "    \n",
    "    def _pad_ordinal(self, sequences: list):\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            #print(\"Before ordinal:{}\".format(seq))\n",
    "            current_size = len(seq)\n",
    "            \n",
    "            t = torch.ones((self.maxlen - current_size,), dtype=torch.long)\n",
    "            t = t*self.PAD \n",
    "            seq = torch.cat((seq, t), dim=0)\n",
    "            \n",
    "            sequences[i] = seq\n",
    "            #print(\"After ordinal:{}\".format(seq))\n",
    "        return sequences\n",
    "    \n",
    "    def _ordinal_encode_sequences(self, strings: list):\n",
    "        res = list()\n",
    "        res_sr = list()\n",
    "        for string in strings:\n",
    "            x1, x2 = self._ordinal_encode_string(string)\n",
    "            res.append(x1)\n",
    "            res_sr.append(x2)\n",
    "        \n",
    "        if self.pad_sequences: \n",
    "            res = self._pad_ordinal(res)\n",
    "            res_sr = self._pad_ordinal(res_sr)\n",
    "        return res, res_sr\n",
    "    \n",
    "    def _ordinal_encode_string(self, string: list):\n",
    "        encoded_string = torch.zeros((len(string)+2,), dtype=torch.long)\n",
    "        encoded_string[0] = self.SOS\n",
    "        encoded_string[-1] = self.EOS\n",
    "\n",
    "        encoded_string_sr = torch.zeros((len(string)+2,), dtype=torch.long)\n",
    "        encoded_string_sr[1] = self.EOS\n",
    "        encoded_string_sr[0] = self.PAD\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if not symbol in self.symbol_dict:\n",
    "                self.symbol_dict[symbol] = len(self.symbol_dict)\n",
    "\n",
    "            encoded_string[i+1] = self.symbol_dict[symbol]\n",
    "            encoded_string_sr[i+2] = self.symbol_dict[symbol]\n",
    "        return encoded_string, encoded_string_sr\n",
    "    \n",
    "    def get_alphabet_size(self):\n",
    "        return self.alphabet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7ac4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet size:  4\n"
     ]
    }
   ],
   "source": [
    "dataset = SequenceDataset(DATA_PATH, maxlen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a79ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7eec33",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61da3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using template from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "# tutorial about positional encoding: https://kikaben.com/transformers-positional-encoding/\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        div_term = 10000 ** ( (2 * torch.arange(0, d_model) ) / d_model)\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        for i in range(max_len):\n",
    "            if i % 2 == 0:    \n",
    "                pe[i, 0, :] = torch.sin(position[i] / div_term)\n",
    "            else:\n",
    "                pe[i, 0, :] = torch.cos(position[i] / div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x #self.dropout(x)\n",
    "\n",
    "\n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None): #must be same as encoder\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.masked_mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, query: torch.Tensor=None, key: torch.Tensor=None):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.masked_mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))#, is_causal=True)\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        if query is None or key is None: # only for debugging\n",
    "            attn_output, attn_output_weights = self.mha(query=x, key=x, value=x)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = self.mha(query=query, key=key, value=x)\n",
    "        \n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class AuTransformer(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(alphabet_size+3, embedding_dim) # +3 for start, stop, padding symbol\n",
    "        self.encoder = Encoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        self.decoder = Decoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        \n",
    "        self.output_fnn = nn.Linear(in_features=embedding_dim, out_features=alphabet_size+3) # +2 for start and stop\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax_output = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
    "        x_src = self.input_embedding(src)\n",
    "        x = self.encoder(x_src)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x_tgt = self.input_embedding(tgt)\n",
    "        x = self.decoder(x=x_tgt, query=x, key=x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.gelu(self.output_fnn(x))        \n",
    "        x = self.softmax_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f5c848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuTransformer(\n",
       "  (input_embedding): Embedding(7, 3)\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (masked_mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_fnn): Linear(in_features=3, out_features=7, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (softmax_output): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuTransformer(dataset.get_alphabet_size(), 3, max_len=10)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c7922",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce45df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  batch 500 loss: 0.4853694618344307\n",
      "  batch 1000 loss: 0.48501103526353834\n",
      "  batch 1500 loss: 0.4847971915602684\n",
      "  batch 2000 loss: 0.48424212235212327\n",
      "  batch 2500 loss: 0.48402298653125764\n",
      "  batch 3000 loss: 0.4835362580418587\n",
      "  batch 3500 loss: 0.4832295786738396\n",
      "  batch 4000 loss: 0.48290717267990113\n",
      "  batch 4500 loss: 0.48266510206460955\n",
      "  batch 5000 loss: 0.4820722975730896\n",
      "  batch 5500 loss: 0.4820260668992996\n",
      "  batch 6000 loss: 0.4816909151673317\n",
      "Epoch:  1\n",
      "  batch 500 loss: 0.4817117950518926\n",
      "  batch 1000 loss: 0.48168980622291563\n",
      "  batch 1500 loss: 0.4817409626841545\n",
      "  batch 2000 loss: 0.48161274737119675\n",
      "  batch 2500 loss: 0.4817170392870903\n",
      "  batch 3000 loss: 0.4814426982998848\n",
      "  batch 3500 loss: 0.4815480489730835\n",
      "  batch 4000 loss: 0.48164191246032717\n",
      "  batch 4500 loss: 0.4814795879125595\n",
      "  batch 5000 loss: 0.4815896006822586\n",
      "  batch 5500 loss: 0.4815115426778793\n",
      "  batch 6000 loss: 0.48145416092872617\n",
      "Epoch:  2\n",
      "  batch 500 loss: 0.48142785600821175\n",
      "  batch 1000 loss: 0.48162462359666824\n",
      "  batch 1500 loss: 0.4814650812149048\n",
      "  batch 2000 loss: 0.48135046249628066\n",
      "  batch 2500 loss: 0.48150033807754516\n",
      "  batch 3000 loss: 0.4814107095003128\n",
      "  batch 3500 loss: 0.4814291386604309\n",
      "  batch 4000 loss: 0.48147697633504866\n",
      "  batch 4500 loss: 0.4815230498313904\n",
      "  batch 5000 loss: 0.4811917373538017\n",
      "  batch 5500 loss: 0.48160788482427597\n",
      "  batch 6000 loss: 0.4813151590228081\n",
      "Epoch:  3\n",
      "  batch 500 loss: 0.4813384377161662\n",
      "  batch 1000 loss: 0.48130614483356476\n",
      "  batch 1500 loss: 0.4814337947368622\n",
      "  batch 2000 loss: 0.48142432945966723\n",
      "  batch 2500 loss: 0.48150290459394457\n",
      "  batch 3000 loss: 0.48139089173078536\n",
      "  batch 3500 loss: 0.4816189005970955\n",
      "  batch 4000 loss: 0.4815451023578644\n",
      "  batch 4500 loss: 0.4814764428138733\n",
      "  batch 5000 loss: 0.48154826736450196\n",
      "  batch 5500 loss: 0.4814403150677681\n",
      "  batch 6000 loss: 0.4813320606350899\n",
      "Epoch:  4\n",
      "  batch 500 loss: 0.48138698025544485\n",
      "  batch 1000 loss: 0.48135552525520325\n",
      "  batch 1500 loss: 0.4814106981754303\n",
      "  batch 2000 loss: 0.48140681064128876\n",
      "  batch 2500 loss: 0.4814928153157234\n",
      "  batch 3000 loss: 0.4815348682999611\n",
      "  batch 3500 loss: 0.4814724776148796\n",
      "  batch 4000 loss: 0.48144330710172656\n",
      "  batch 4500 loss: 0.48148099982738496\n",
      "  batch 5000 loss: 0.4814587458372116\n",
      "  batch 5500 loss: 0.4815676597356796\n",
      "  batch 6000 loss: 0.48133164781332016\n",
      "Epoch:  5\n",
      "  batch 500 loss: 0.48130065989494325\n",
      "  batch 1000 loss: 0.48146898883581163\n",
      "  batch 1500 loss: 0.48147481739521025\n",
      "  batch 2000 loss: 0.481309929728508\n",
      "  batch 2500 loss: 0.4815815538764\n",
      "  batch 3000 loss: 0.4812427105307579\n",
      "  batch 3500 loss: 0.4815295367240906\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m test_string_ord_sr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(test_string_ord_sr, dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     16\u001b[0m test_string_oh_sr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(test_string_oh_sr, dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_string_ord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_string_ord_sr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(torch\u001b[38;5;241m.\u001b[39msqueeze(outputs), test_string_oh_sr)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 103\u001b[0m, in \u001b[0;36mAuTransformer.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    100\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    102\u001b[0m x_tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_embedding(tgt)\n\u001b[0;32m--> 103\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_tgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    106\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_fnn(x))        \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, query, key)\u001b[0m\n\u001b[1;32m     63\u001b[0m sequence_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(x)\n\u001b[0;32m---> 66\u001b[0m attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_mha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_square_subsequent_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, is_causal=True)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_output \u001b[38;5;66;03m# skip-connection\u001b[39;00m\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:5403\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5400\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_causal \u001b[38;5;129;01mand\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFIXME: is_causal not implemented for need_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5403\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaddbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5405\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "divisor = 0.\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"Epoch: \", i)\n",
    "    for j, (test_string_ord, test_string_ord_sr, test_string_oh_sr, _, sequence_length) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        test_string_ord = torch.permute(test_string_ord, dims=[1,0])\n",
    "        test_string_ord_sr = torch.permute(test_string_ord_sr, dims=[1,0])\n",
    "        test_string_oh_sr = torch.permute(test_string_oh_sr, dims=[1,0,2])\n",
    "        \n",
    "        outputs = model(test_string_ord, test_string_ord_sr)\n",
    "        loss = loss_fn(torch.squeeze(outputs), test_string_oh_sr)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        divisor += float( list(test_string_ord.size())[1] )\n",
    "        if j % 500 == 499:\n",
    "            last_loss = running_loss / divisor # loss per batch\n",
    "            print('  batch {} loss: {}'.format(j + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "            divisor = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9cf0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"trained_model.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760b1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.load(\"trained_model.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05fbe533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuTransformer(\n",
       "  (input_embedding): Embedding(7, 3)\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (masked_mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_fnn): Linear(in_features=3, out_features=7, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (softmax_output): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(test_string_ord, test_string_ord_sr), test_string_oh, test_string_oh_sr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
