{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b62867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from transformer_definitions import *\n",
    "\n",
    "import math\n",
    "import pickle as pk\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DATA_PATH = \"problem_1_train_dfa.dat\"\n",
    "EPOCHS = 50\n",
    "MODEL_NAME = \"trained_model.pk\" # how to save model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb504d",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbab3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, datapath: str, maxlen: int, pad_sequences: bool=True, max_sequences: int=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert(os.path.isfile(datapath))\n",
    "        self.symbol_dict = dict()\n",
    "        self.label_dict = dict()\n",
    "        self.sequences, self.labels, self.sequence_lengths = self._read_sequences(datapath, max_sequences)\n",
    "        print(\"Sequences loaded. Some examples: \\n{}\".format(self.sequences[:3]))\n",
    "        \n",
    "        self.SOS = self.alphabet_size\n",
    "        self.EOS = self.alphabet_size + 1\n",
    "        self.PAD = self.alphabet_size + 2\n",
    "        self.maxlen = maxlen + 2  # +2 for EOS/PAD and SOS \n",
    "        self.pad_sequences = pad_sequences\n",
    "        \n",
    "    def encode_sequences(self):\n",
    "        self.ordinal_seq, self.ordinal_seq_sr = self._ordinal_encode_sequences(self.sequences)\n",
    "        self.one_hot_seq, self.one_hot_seq_sr = self._one_hot_encode_sequences(self.sequences)\n",
    "        \n",
    "        del self.sequences\n",
    "        self.sequences = None\n",
    "        \n",
    "        print(\"The symbol dictionary: {}\".format(self.symbol_dict))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ordinal_seq[idx], self.ordinal_seq_sr[idx], self.one_hot_seq[idx], \\\n",
    "               self.one_hot_seq_sr[idx], self.labels[idx], self.sequence_lengths[idx]\n",
    "       \n",
    "    def _read_sequences(self, datapath: str, max_sequences: int):\n",
    "        sequences = list()\n",
    "        labels = list()\n",
    "        sequence_lengths = list()\n",
    "        \n",
    "        for i, line in enumerate(open(datapath)):\n",
    "            if i == 0:\n",
    "                line = line.split()\n",
    "                self.alphabet_size = int(line[1])\n",
    "                print(\"Alphabet size: \", self.alphabet_size)\n",
    "                continue\n",
    "            elif max_sequences and i-1 >= max_sequences:\n",
    "                break\n",
    "            \n",
    "            line = line.split()\n",
    "            label = line[0]\n",
    "            if not label in self.label_dict:\n",
    "                self.label_dict[label] = len(self.label_dict)\n",
    "            label = self.label_dict[label]\n",
    "            labels.append(label)\n",
    "            \n",
    "            sequences.append(line[2:])\n",
    "            sequence_lengths.append(len(line) - 1)\n",
    "        return sequences, labels, sequence_lengths\n",
    "    \n",
    "    def _pad_one_hot(self, sequences: list, do_eos: bool=False):\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            #print(\"Before one hot:\\n{}\".format(seq))\n",
    "            current_size = len(seq)\n",
    "            \n",
    "            t = torch.zeros((self.maxlen - current_size, self.alphabet_size + 3), dtype=torch.float32)\n",
    "            t[:, self.PAD] = 1\n",
    "            if do_eos and self.maxlen > current_size:\n",
    "                t[0, self.PAD] = 0\n",
    "                t[0, self.EOS] = 1\n",
    "            \n",
    "            seq = torch.cat((seq, t), dim=0)\n",
    "            sequences[i] = seq\n",
    "            #print(\"After one hot:\\n{}\".format(seq))\n",
    "        return sequences\n",
    "    \n",
    "    def _one_hot_encode_sequences(self, strings: list):\n",
    "        res = list()\n",
    "        res_sr = list()\n",
    "        for string in strings:\n",
    "            x1, x2 = self._one_hot_encode_string(string)\n",
    "            res.append(x1)\n",
    "            res_sr.append(x2)\n",
    "            \n",
    "        if self.pad_sequences:\n",
    "            res = self._pad_one_hot(res)\n",
    "            res_sr = self._pad_one_hot(res_sr)\n",
    "\n",
    "        return res, res_sr\n",
    "    \n",
    "    def _one_hot_encode_string(self, string: list):\n",
    "        encoded_string = torch.zeros((len(string)+2, self.alphabet_size + 3), dtype=torch.float32) # alphabet_size + 3 because SOS, EOS, padding token\n",
    "        encoded_string[0][self.SOS] = 1\n",
    "        encoded_string[-1][self.EOS] = 1\n",
    "\n",
    "        encoded_string_sl = torch.zeros((len(string)+2, self.alphabet_size + 3), dtype=torch.float32)\n",
    "        encoded_string_sl[-2][self.EOS] = 1\n",
    "        encoded_string_sl[-1][self.PAD] = 1\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if not symbol in self.symbol_dict:\n",
    "                self.symbol_dict[symbol] = len(self.symbol_dict)\n",
    "\n",
    "            encoded_string[i+1][self.symbol_dict[symbol]] = 1\n",
    "            encoded_string_sl[i][self.symbol_dict[symbol]] = 1\n",
    "        encoded_string_sl.requires_grad_()\n",
    "        return encoded_string, encoded_string_sl\n",
    "    \n",
    "    def _pad_ordinal(self, sequences: list, do_eos: bool=False):\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            #print(\"Before ordinal:{}\".format(seq))\n",
    "            current_size = len(seq)\n",
    "            \n",
    "            t = torch.ones((self.maxlen - current_size,), dtype=torch.long)\n",
    "            t = t*self.PAD \n",
    "            if do_eos and self.maxlen > current_size:\n",
    "                t[0] = self.EOS\n",
    "            \n",
    "            seq = torch.cat((seq, t), dim=0)\n",
    "            sequences[i] = seq\n",
    "            #print(\"After ordinal:{}\".format(seq))\n",
    "        return sequences\n",
    "    \n",
    "    def _ordinal_encode_sequences(self, strings: list):\n",
    "        res = list()\n",
    "        res_sr = list()\n",
    "        for string in strings:\n",
    "            x1, x2 = self._ordinal_encode_string(string)\n",
    "            res.append(x1)\n",
    "            res_sr.append(x2)\n",
    "        \n",
    "        if self.pad_sequences: \n",
    "            res = self._pad_ordinal(res)\n",
    "            res_sr = self._pad_ordinal(res_sr)\n",
    "        return res, res_sr\n",
    "    \n",
    "    def _ordinal_encode_string(self, string: list):\n",
    "        encoded_string = torch.zeros((len(string)+2,), dtype=torch.long)\n",
    "        encoded_string[0] = self.SOS\n",
    "        encoded_string[-1] = self.EOS\n",
    "\n",
    "        encoded_string_sl = torch.zeros((len(string)+2,), dtype=torch.long)\n",
    "        encoded_string_sl[-2] = self.EOS\n",
    "        encoded_string_sl[-1] = self.PAD\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if not symbol in self.symbol_dict:\n",
    "                self.symbol_dict[symbol] = len(self.symbol_dict)\n",
    "\n",
    "            encoded_string[i+1] = self.symbol_dict[symbol]\n",
    "            encoded_string_sl[i] = self.symbol_dict[symbol]\n",
    "        return encoded_string, encoded_string_sl\n",
    "    \n",
    "    def get_alphabet_size(self):\n",
    "        return self.alphabet_size\n",
    "    \n",
    "    def initialize(self, path: str=\"dataset.pk\"):\n",
    "        data = pk.load(open(path, \"rb\"))\n",
    "        self.alphabet_size = self.alphabet_size\n",
    "        self.symbol_dict = self.symbol_dict\n",
    "        self.label_dict = self.label_dict\n",
    "        \n",
    "    def save_state(self, path: str=\"dataset.pk\"):\n",
    "        data = dict()\n",
    "        data[\"alphabet_size\"] = self.alphabet_size\n",
    "        data[\"symbol_dict\"] = self.symbol_dict\n",
    "        data[\"label_dict\"] = self.label_dict\n",
    "        pk.dump(data, open(path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7ac4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet size:  4\n",
      "Sequences loaded. Some examples: \n",
      "[['a', 'b'], ['a', 'b'], ['c', 'd', 'a', 'b']]\n",
      "The symbol dictionary: {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n"
     ]
    }
   ],
   "source": [
    "dataset = SequenceDataset(DATA_PATH, maxlen=10)\n",
    "dataset.encode_sequences()\n",
    "dataset.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a79ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7eec33",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd6e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using template from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "# tutorial about positional encoding: https://kikaben.com/transformers-positional-encoding/\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        div_term = 10000 ** ( (2 * torch.arange(0, d_model) ) / d_model)\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        for i in range(max_len):\n",
    "            if i % 2 == 0:    \n",
    "                pe[i, 0, :] = torch.sin(position[i] / div_term)\n",
    "            else:\n",
    "                pe[i, 0, :] = torch.cos(position[i] / div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x #self.dropout(x)\n",
    "\n",
    "\n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "                \n",
    "        return x, attn_output, attn_output_weights\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None): #must be same as encoder\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.masked_mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, query: torch.Tensor=None, key: torch.Tensor=None):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.masked_mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))#, is_causal=True)\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        if query is None or key is None: # only for debugging\n",
    "            attn_output, attn_output_weights = self.mha(query=x, key=x, value=x)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = self.mha(query=query, key=key, value=x)\n",
    "        \n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class AuTransformer(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(alphabet_size+3, embedding_dim) # +3 for start, stop, padding symbol\n",
    "        self.encoder = Encoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        self.decoder = Decoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        \n",
    "        self.output_fnn = nn.Linear(in_features=embedding_dim, out_features=alphabet_size+3) # +2 for start and stop\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax_output = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.attention_output_layer = nn.Identity() \n",
    "        self.attention_weight_layer = nn.Identity() \n",
    "        \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
    "        x_src = self.input_embedding(src)\n",
    "        x, attention_output, attention_weights = self.encoder(x_src)\n",
    "        attention_output = self.attention_output_layer(attention_output)\n",
    "        attention_weights = self.attention_weight_layer(attention_weights)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x_tgt = self.input_embedding(tgt)\n",
    "        x = self.decoder(x=x_tgt, query=x, key=x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.gelu(self.output_fnn(x))\n",
    "        x = self.softmax_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f5c848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuTransformer(\n",
       "  (input_embedding): Embedding(7, 3)\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (masked_mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_fnn): Linear(in_features=3, out_features=7, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (softmax_output): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuTransformer(dataset.get_alphabet_size(), 3, max_len=10)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c7922",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce45df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:14:50.348570: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  batch 0 loss: 1.8642925024032593\n",
      "  batch 1000 loss: 1.849761075258255\n",
      "  batch 2000 loss: 1.8440512104034423\n",
      "  batch 3000 loss: 1.8343844048976898\n",
      "  batch 4000 loss: 1.8237437331676483\n",
      "  batch 5000 loss: 1.8163463504314423\n",
      "  batch 6000 loss: 1.8120236954689026\n",
      "  batch 7000 loss: 1.8036180588006974\n",
      "  batch 8000 loss: 1.7928736736774444\n",
      "  batch 9000 loss: 1.7853946917057038\n",
      "  batch 10000 loss: 1.784455164551735\n",
      "  batch 11000 loss: 1.7733713262081146\n",
      "  batch 12000 loss: 1.7684401646852492\n",
      "Epoch:  1\n",
      "  batch 0 loss: 1.7541624094902064\n",
      "  batch 1000 loss: 1.754656299829483\n",
      "  batch 2000 loss: 1.7438372901678085\n",
      "  batch 3000 loss: 1.7408930761814116\n",
      "  batch 4000 loss: 1.728685742378235\n",
      "  batch 5000 loss: 1.7247979179620743\n",
      "  batch 6000 loss: 1.720530793428421\n",
      "  batch 7000 loss: 1.7062780493497849\n",
      "  batch 8000 loss: 1.6974281995296479\n",
      "  batch 9000 loss: 1.690214879512787\n",
      "  batch 10000 loss: 1.6887503609657288\n",
      "  batch 11000 loss: 1.6801608951091767\n",
      "  batch 12000 loss: 1.6739462106227874\n",
      "Epoch:  2\n",
      "  batch 0 loss: 1.6770798276971888\n",
      "  batch 1000 loss: 1.665870089173317\n",
      "  batch 2000 loss: 1.6642093336582184\n",
      "  batch 3000 loss: 1.6561601700782775\n",
      "  batch 4000 loss: 1.6528139625787734\n",
      "  batch 5000 loss: 1.6472971754074097\n",
      "  batch 6000 loss: 1.6394300006628038\n",
      "  batch 7000 loss: 1.6398677016496659\n",
      "  batch 8000 loss: 1.6288755058050155\n",
      "  batch 9000 loss: 1.622246810078621\n",
      "  batch 10000 loss: 1.6231896498203278\n",
      "  batch 11000 loss: 1.6082139868736267\n",
      "  batch 12000 loss: 1.6089375022649766\n",
      "Epoch:  3\n",
      "  batch 0 loss: 1.5972629556752214\n",
      "  batch 1000 loss: 1.6073490909337997\n",
      "  batch 2000 loss: 1.600803572177887\n",
      "  batch 3000 loss: 1.5934741468429565\n",
      "  batch 4000 loss: 1.5870560023784637\n",
      "  batch 5000 loss: 1.5823961404561997\n",
      "  batch 6000 loss: 1.5837214303016662\n",
      "  batch 7000 loss: 1.584107225894928\n",
      "  batch 8000 loss: 1.579096463561058\n",
      "  batch 9000 loss: 1.57535964512825\n",
      "  batch 10000 loss: 1.5711095505952835\n",
      "  batch 11000 loss: 1.5697036958932877\n",
      "  batch 12000 loss: 1.5560244245529176\n",
      "Epoch:  4\n",
      "  batch 0 loss: 1.5679613464207762\n",
      "  batch 1000 loss: 1.5542486897706986\n",
      "  batch 2000 loss: 1.5512747815847396\n",
      "  batch 3000 loss: 1.5547313055992127\n",
      "  batch 4000 loss: 1.5493603930473328\n",
      "  batch 5000 loss: 1.5387949411869049\n",
      "  batch 6000 loss: 1.5411060354709625\n",
      "  batch 7000 loss: 1.539538843035698\n",
      "  batch 8000 loss: 1.5358154991865158\n",
      "  batch 9000 loss: 1.5347172850370407\n",
      "  batch 10000 loss: 1.5348549519777297\n",
      "  batch 11000 loss: 1.5329426916837692\n",
      "  batch 12000 loss: 1.524327817082405\n",
      "Epoch:  5\n",
      "  batch 0 loss: 1.5137011378702492\n",
      "  batch 1000 loss: 1.5229239588975907\n",
      "  batch 2000 loss: 1.5188010855913163\n",
      "  batch 3000 loss: 1.5148626161813736\n",
      "  batch 4000 loss: 1.5153469084501265\n",
      "  batch 5000 loss: 1.504358162522316\n",
      "  batch 6000 loss: 1.508116047859192\n",
      "  batch 7000 loss: 1.4989444999694823\n",
      "  batch 8000 loss: 1.4980798670053481\n",
      "  batch 9000 loss: 1.4991797268390656\n",
      "  batch 10000 loss: 1.495459109902382\n",
      "  batch 11000 loss: 1.4921362801790237\n",
      "  batch 12000 loss: 1.4938363254070282\n",
      "Epoch:  6\n",
      "  batch 0 loss: 1.4858799598835133\n",
      "  batch 1000 loss: 1.488942384123802\n",
      "  batch 2000 loss: 1.4828653205633164\n",
      "  batch 3000 loss: 1.4783039100170134\n",
      "  batch 4000 loss: 1.4716299751996993\n",
      "  batch 5000 loss: 1.4733953042030334\n",
      "  batch 6000 loss: 1.4722413872480393\n",
      "  batch 7000 loss: 1.4703112051486968\n",
      "  batch 8000 loss: 1.4621957502365113\n",
      "  batch 9000 loss: 1.4681717854738237\n",
      "  batch 10000 loss: 1.464425922036171\n",
      "  batch 11000 loss: 1.4606389340162278\n",
      "  batch 12000 loss: 1.4617196772098542\n",
      "Epoch:  7\n",
      "  batch 0 loss: 1.4594473854861276\n",
      "  batch 1000 loss: 1.4544289643764496\n",
      "  batch 2000 loss: 1.452680770277977\n",
      "  batch 3000 loss: 1.4482286469936372\n",
      "  batch 4000 loss: 1.440347471833229\n",
      "  batch 5000 loss: 1.4470791889429093\n",
      "  batch 6000 loss: 1.4448482015132904\n",
      "  batch 7000 loss: 1.4408022062778474\n",
      "  batch 8000 loss: 1.4338134392499924\n",
      "  batch 9000 loss: 1.436206461071968\n",
      "  batch 10000 loss: 1.4371575841903685\n",
      "  batch 11000 loss: 1.4322037703990935\n",
      "  batch 12000 loss: 1.429710830450058\n",
      "Epoch:  8\n",
      "  batch 0 loss: 1.4373575376741814\n",
      "  batch 1000 loss: 1.427948284626007\n",
      "  batch 2000 loss: 1.429110550045967\n",
      "  batch 3000 loss: 1.4218028311729431\n",
      "  batch 4000 loss: 1.4207985317707061\n",
      "  batch 5000 loss: 1.4220487679243088\n",
      "  batch 6000 loss: 1.4210737220048903\n",
      "  batch 7000 loss: 1.4186256732940674\n",
      "  batch 8000 loss: 1.418200771331787\n",
      "  batch 9000 loss: 1.4143044941425325\n",
      "  batch 10000 loss: 1.413865866780281\n",
      "  batch 11000 loss: 1.4168747581243515\n",
      "  batch 12000 loss: 1.4119036656618118\n",
      "Epoch:  9\n",
      "  batch 0 loss: 1.4146296512398253\n",
      "  batch 1000 loss: 1.4098587054014207\n",
      "  batch 2000 loss: 1.4104450199604035\n",
      "  batch 3000 loss: 1.403559602022171\n",
      "  batch 4000 loss: 1.4028563059568404\n",
      "  batch 5000 loss: 1.4075450875759126\n",
      "  batch 6000 loss: 1.3990348440408706\n",
      "  batch 7000 loss: 1.4023912501335145\n",
      "  batch 8000 loss: 1.4004082850217818\n",
      "  batch 9000 loss: 1.3956248854398727\n",
      "  batch 10000 loss: 1.391119604587555\n",
      "  batch 11000 loss: 1.393162262916565\n",
      "  batch 12000 loss: 1.3866259704828263\n",
      "Epoch:  10\n",
      "  batch 0 loss: 1.3862894348022512\n",
      "  batch 1000 loss: 1.387113063931465\n",
      "  batch 2000 loss: 1.3813557151556015\n",
      "  batch 3000 loss: 1.3791890460252763\n",
      "  batch 4000 loss: 1.3765238229036332\n",
      "  batch 5000 loss: 1.3774963961839677\n",
      "  batch 6000 loss: 1.36875119805336\n",
      "  batch 7000 loss: 1.3668045494556427\n",
      "  batch 8000 loss: 1.362347847223282\n",
      "  batch 9000 loss: 1.3588030782938003\n",
      "  batch 10000 loss: 1.3542436430454254\n",
      "  batch 11000 loss: 1.3577294595241547\n",
      "  batch 12000 loss: 1.357121106505394\n",
      "Epoch:  11\n",
      "  batch 0 loss: 1.348293379099682\n",
      "  batch 1000 loss: 1.3487857608795166\n",
      "  batch 2000 loss: 1.3461112071275712\n",
      "  batch 3000 loss: 1.349852003455162\n",
      "  batch 4000 loss: 1.3434439799785614\n",
      "  batch 5000 loss: 1.3376531349420548\n",
      "  batch 6000 loss: 1.3375225217342377\n",
      "  batch 7000 loss: 1.3394568022489548\n",
      "  batch 8000 loss: 1.3319067162275315\n",
      "  batch 9000 loss: 1.3269082969427108\n",
      "  batch 10000 loss: 1.3294702014923097\n",
      "  batch 11000 loss: 1.3299116344451904\n",
      "  batch 12000 loss: 1.3237413475513458\n",
      "Epoch:  12\n",
      "  batch 0 loss: 1.3222092402101768\n",
      "  batch 1000 loss: 1.3252182438373565\n",
      "  batch 2000 loss: 1.3286615090370177\n",
      "  batch 3000 loss: 1.3185481246709823\n",
      "  batch 4000 loss: 1.3196474068164825\n",
      "  batch 5000 loss: 1.3097955384254456\n",
      "  batch 6000 loss: 1.31586649787426\n",
      "  batch 7000 loss: 1.3123295981884002\n",
      "  batch 8000 loss: 1.3108592445850373\n",
      "  batch 9000 loss: 1.3129608125686645\n",
      "  batch 10000 loss: 1.3060691635608672\n",
      "  batch 11000 loss: 1.3106174037456513\n",
      "  batch 12000 loss: 1.3032406730651855\n",
      "Epoch:  13\n",
      "  batch 0 loss: 1.31201068640558\n",
      "  batch 1000 loss: 1.3073922140598297\n",
      "  batch 2000 loss: 1.3054953132867813\n",
      "  batch 3000 loss: 1.3056008176803588\n",
      "  batch 4000 loss: 1.3033094249963761\n",
      "  batch 5000 loss: 1.3009215171337127\n",
      "  batch 6000 loss: 1.2974632042646408\n",
      "  batch 7000 loss: 1.303612629532814\n",
      "  batch 8000 loss: 1.3012253084182739\n",
      "  batch 9000 loss: 1.3011071908473968\n",
      "  batch 10000 loss: 1.3011577509641647\n",
      "  batch 11000 loss: 1.3020583726167678\n",
      "  batch 12000 loss: 1.3010054399967195\n",
      "Epoch:  14\n",
      "  batch 0 loss: 1.3020385254914513\n",
      "  batch 1000 loss: 1.2989385828971862\n",
      "  batch 2000 loss: 1.296555845975876\n",
      "  batch 3000 loss: 1.2941304939985274\n",
      "  batch 4000 loss: 1.3006711968183517\n",
      "  batch 5000 loss: 1.2960304837226868\n",
      "  batch 6000 loss: 1.2912461738586425\n",
      "  batch 7000 loss: 1.2981784259080886\n",
      "  batch 8000 loss: 1.2946654040813446\n",
      "  batch 9000 loss: 1.2931723253726959\n",
      "  batch 10000 loss: 1.3012969833612442\n",
      "  batch 11000 loss: 1.292942077755928\n",
      "  batch 12000 loss: 1.292629759669304\n",
      "Epoch:  15\n",
      "  batch 0 loss: 1.298345801404831\n",
      "  batch 1000 loss: 1.2879694427251815\n",
      "  batch 2000 loss: 1.291206589102745\n",
      "  batch 3000 loss: 1.294765477657318\n",
      "  batch 4000 loss: 1.2941267606019973\n",
      "  batch 5000 loss: 1.2919612561464309\n",
      "  batch 6000 loss: 1.2893673610687255\n",
      "  batch 7000 loss: 1.2970231181383134\n",
      "  batch 8000 loss: 1.2900991286039352\n",
      "  batch 9000 loss: 1.290241561293602\n",
      "  batch 10000 loss: 1.295482046365738\n",
      "  batch 11000 loss: 1.2903534170389175\n",
      "  batch 12000 loss: 1.2899048084020615\n",
      "Epoch:  16\n",
      "  batch 0 loss: 1.2957179554383762\n",
      "  batch 1000 loss: 1.2910959537029267\n",
      "  batch 2000 loss: 1.2925642133951187\n",
      "  batch 3000 loss: 1.2887954703569413\n",
      "  batch 4000 loss: 1.287024681210518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 5000 loss: 1.2886809856891632\n",
      "  batch 6000 loss: 1.2917154603004455\n",
      "  batch 7000 loss: 1.288399415373802\n",
      "  batch 8000 loss: 1.28885343337059\n",
      "  batch 9000 loss: 1.2912294952869414\n",
      "  batch 10000 loss: 1.29135799741745\n",
      "  batch 11000 loss: 1.2871426991224288\n",
      "  batch 12000 loss: 1.2899217044115066\n",
      "Epoch:  17\n",
      "  batch 0 loss: 1.2820767938890039\n",
      "  batch 1000 loss: 1.290476791024208\n",
      "  batch 2000 loss: 1.2899952150583267\n",
      "  batch 3000 loss: 1.2899896819591523\n",
      "  batch 4000 loss: 1.2897409137487412\n",
      "  batch 5000 loss: 1.285532755613327\n",
      "  batch 6000 loss: 1.2875641310214996\n",
      "  batch 7000 loss: 1.2864458593130113\n",
      "  batch 8000 loss: 1.2843213291168214\n",
      "  batch 9000 loss: 1.2831848030090331\n",
      "  batch 10000 loss: 1.291511397242546\n",
      "  batch 11000 loss: 1.2905567313432693\n",
      "  batch 12000 loss: 1.2865615142583846\n",
      "Epoch:  18\n",
      "  batch 0 loss: 1.2897901426662097\n",
      "  batch 1000 loss: 1.2871549139022826\n",
      "  batch 2000 loss: 1.2888737255334854\n",
      "  batch 3000 loss: 1.2884727692604065\n",
      "  batch 4000 loss: 1.2886509056091309\n",
      "  batch 5000 loss: 1.284523251771927\n",
      "  batch 6000 loss: 1.2860239213705063\n",
      "  batch 7000 loss: 1.2886194112300873\n",
      "  batch 8000 loss: 1.2798356211185455\n",
      "  batch 9000 loss: 1.2841983692646026\n",
      "  batch 10000 loss: 1.2855811610221863\n",
      "  batch 11000 loss: 1.286392988204956\n",
      "  batch 12000 loss: 1.2816832844018937\n",
      "Epoch:  19\n",
      "  batch 0 loss: 1.2797973617560132\n",
      "  batch 1000 loss: 1.282657638669014\n",
      "  batch 2000 loss: 1.2811161853075028\n",
      "  batch 3000 loss: 1.2864956024885177\n",
      "  batch 4000 loss: 1.2861022810935974\n",
      "  batch 5000 loss: 1.2849563474655152\n",
      "  batch 6000 loss: 1.284614383339882\n",
      "  batch 7000 loss: 1.2840890752077103\n",
      "  batch 8000 loss: 1.2832389451265336\n",
      "  batch 9000 loss: 1.283310510158539\n",
      "  batch 10000 loss: 1.2832691639661788\n",
      "  batch 11000 loss: 1.281721533894539\n",
      "  batch 12000 loss: 1.2782596457004547\n"
     ]
    }
   ],
   "source": [
    "# training loop from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "divisor = 0.\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"Epoch: \", i)\n",
    "    for j, (test_string_ord, test_string_ord_sr, test_string_oh, test_string_oh_sr, _, sequence_length) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        test_string_ord = torch.permute(test_string_ord, dims=[1,0])\n",
    "        test_string_ord_sr = torch.permute(test_string_ord_sr, dims=[1,0])\n",
    "        test_string_oh_sr = torch.permute(test_string_oh_sr, dims=[1,0,2])\n",
    "        \n",
    "        outputs = model(test_string_ord, test_string_ord_sr)\n",
    "        loss = loss_fn(torch.squeeze(outputs), torch.squeeze(test_string_oh_sr))\n",
    "        loss.backward()\n",
    "        \n",
    "        #break\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        divisor += float( list(test_string_ord.size())[1] )\n",
    "        if j % 1000 == 0:\n",
    "            last_loss = running_loss / divisor # loss per batch\n",
    "            print('  batch {} loss: {}'.format(j, last_loss))\n",
    "            running_loss = 0.\n",
    "            divisor = 0.\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9cf0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b60ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
