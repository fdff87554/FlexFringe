{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b62867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from transformer_definitions import *\n",
    "\n",
    "import math\n",
    "import pickle as pk\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DATA_PATH = \"problem_1_train_dfa.dat\"\n",
    "EPOCHS = 50\n",
    "MODEL_NAME = \"trained_model.pk\" # how to save model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb504d",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbab3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, datapath: str, maxlen: int, pad_sequences: bool=True, max_sequences: int=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert(os.path.isfile(datapath))\n",
    "        self.symbol_dict = dict()\n",
    "        self.label_dict = dict()\n",
    "        self.sequences, self.labels, self.sequence_lengths = self._read_sequences(datapath, max_sequences)\n",
    "        print(\"Sequences loaded. Some examples: \\n{}\".format(self.sequences[:3]))\n",
    "        \n",
    "        self.SOS = self.alphabet_size\n",
    "        self.EOS = self.alphabet_size + 1\n",
    "        self.PAD = self.alphabet_size + 2\n",
    "        self.maxlen = maxlen + 2  # +2 for EOS/PAD and SOS \n",
    "        self.pad_sequences = pad_sequences\n",
    "        \n",
    "    def encode_sequences(self):\n",
    "        self.ordinal_seq, self.ordinal_seq_sr = self._ordinal_encode_sequences(self.sequences)\n",
    "        self.one_hot_seq, self.one_hot_seq_sr = self._one_hot_encode_sequences(self.sequences)\n",
    "        \n",
    "        del self.sequences\n",
    "        self.sequences = None\n",
    "        \n",
    "        print(\"The symbol dictionary: {}\".format(self.symbol_dict))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ordinal_seq[idx], self.ordinal_seq_sr[idx], self.one_hot_seq[idx], \\\n",
    "               self.one_hot_seq_sr[idx], self.labels[idx], self.sequence_lengths[idx]\n",
    "       \n",
    "    def _read_sequences(self, datapath: str, max_sequences: int):\n",
    "        sequences = list()\n",
    "        labels = list()\n",
    "        sequence_lengths = list()\n",
    "        \n",
    "        for i, line in enumerate(open(datapath)):\n",
    "            if i == 0:\n",
    "                line = line.split()\n",
    "                self.alphabet_size = int(line[1])\n",
    "                print(\"Alphabet size: \", self.alphabet_size)\n",
    "                continue\n",
    "            elif max_sequences and i-1 >= max_sequences:\n",
    "                break\n",
    "            \n",
    "            line = line.split()\n",
    "            label = line[0]\n",
    "            if not label in self.label_dict:\n",
    "                self.label_dict[label] = len(self.label_dict)\n",
    "            label = self.label_dict[label]\n",
    "            labels.append(label)\n",
    "            \n",
    "            sequences.append(line[2:])\n",
    "            sequence_lengths.append(len(line) - 1)\n",
    "        return sequences, labels, sequence_lengths\n",
    "    \n",
    "    def _pad_one_hot(self, sequences: list, do_eos: bool=False):\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            #print(\"Before one hot:\\n{}\".format(seq))\n",
    "            current_size = len(seq)\n",
    "            \n",
    "            t = torch.zeros((self.maxlen - current_size, self.alphabet_size + 3), dtype=torch.float32)\n",
    "            t[:, self.PAD] = 1\n",
    "            if do_eos and self.maxlen > current_size:\n",
    "                t[0, self.PAD] = 0\n",
    "                t[0, self.EOS] = 1\n",
    "            \n",
    "            seq = torch.cat((seq, t), dim=0)\n",
    "            sequences[i] = seq\n",
    "            #print(\"After one hot:\\n{}\".format(seq))\n",
    "        return sequences\n",
    "    \n",
    "    def _one_hot_encode_sequences(self, strings: list):\n",
    "        res = list()\n",
    "        res_sr = list()\n",
    "        for string in strings:\n",
    "            x1, x2 = self._one_hot_encode_string(string)\n",
    "            res.append(x1)\n",
    "            res_sr.append(x2)\n",
    "            \n",
    "        if self.pad_sequences:\n",
    "            res = self._pad_one_hot(res)\n",
    "            res_sr = self._pad_one_hot(res_sr)\n",
    "\n",
    "        return res, res_sr\n",
    "    \n",
    "    def _one_hot_encode_string(self, string: list):\n",
    "        encoded_string = torch.zeros((len(string)+2, self.alphabet_size + 3), dtype=torch.float32) # alphabet_size + 3 because SOS, EOS, padding token\n",
    "        encoded_string[0][self.SOS] = 1\n",
    "        encoded_string[-1][self.EOS] = 1\n",
    "\n",
    "        encoded_string_sl = torch.zeros((len(string)+2, self.alphabet_size + 3), dtype=torch.float32)\n",
    "        encoded_string_sl[-2][self.EOS] = 1\n",
    "        encoded_string_sl[-1][self.PAD] = 1\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if not symbol in self.symbol_dict:\n",
    "                self.symbol_dict[symbol] = len(self.symbol_dict)\n",
    "\n",
    "            encoded_string[i+1][self.symbol_dict[symbol]] = 1\n",
    "            encoded_string_sl[i][self.symbol_dict[symbol]] = 1\n",
    "        encoded_string_sl.requires_grad_()\n",
    "        return encoded_string, encoded_string_sl\n",
    "    \n",
    "    def _pad_ordinal(self, sequences: list, do_eos: bool=False):\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            #print(\"Before ordinal:{}\".format(seq))\n",
    "            current_size = len(seq)\n",
    "            \n",
    "            t = torch.ones((self.maxlen - current_size,), dtype=torch.long)\n",
    "            t = t*self.PAD \n",
    "            if do_eos and self.maxlen > current_size:\n",
    "                t[0] = self.EOS\n",
    "            \n",
    "            seq = torch.cat((seq, t), dim=0)\n",
    "            sequences[i] = seq\n",
    "            #print(\"After ordinal:{}\".format(seq))\n",
    "        return sequences\n",
    "    \n",
    "    def _ordinal_encode_sequences(self, strings: list):\n",
    "        res = list()\n",
    "        res_sr = list()\n",
    "        for string in strings:\n",
    "            x1, x2 = self._ordinal_encode_string(string)\n",
    "            res.append(x1)\n",
    "            res_sr.append(x2)\n",
    "        \n",
    "        if self.pad_sequences: \n",
    "            res = self._pad_ordinal(res)\n",
    "            res_sr = self._pad_ordinal(res_sr)\n",
    "        return res, res_sr\n",
    "    \n",
    "    def _ordinal_encode_string(self, string: list):\n",
    "        encoded_string = torch.zeros((len(string)+2,), dtype=torch.long)\n",
    "        encoded_string[0] = self.SOS\n",
    "        encoded_string[-1] = self.EOS\n",
    "\n",
    "        encoded_string_sl = torch.zeros((len(string)+2,), dtype=torch.long)\n",
    "        encoded_string_sl[-2] = self.EOS\n",
    "        encoded_string_sl[-1] = self.PAD\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if not symbol in self.symbol_dict:\n",
    "                self.symbol_dict[symbol] = len(self.symbol_dict)\n",
    "\n",
    "            encoded_string[i+1] = self.symbol_dict[symbol]\n",
    "            encoded_string_sl[i] = self.symbol_dict[symbol]\n",
    "        return encoded_string, encoded_string_sl\n",
    "    \n",
    "    def get_alphabet_size(self):\n",
    "        return self.alphabet_size\n",
    "    \n",
    "    def initialize(self, path: str=\"dataset.pk\"):\n",
    "        data = pk.load(open(path, \"rb\"))\n",
    "        self.alphabet_size = self.alphabet_size\n",
    "        self.symbol_dict = self.symbol_dict\n",
    "        self.label_dict = self.label_dict\n",
    "        \n",
    "    def save_state(self, path: str=\"dataset.pk\"):\n",
    "        data = dict()\n",
    "        data[\"alphabet_size\"] = self.alphabet_size\n",
    "        data[\"symbol_dict\"] = self.symbol_dict\n",
    "        data[\"label_dict\"] = self.label_dict\n",
    "        pk.dump(data, open(path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7ac4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet size:  4\n",
      "Sequences loaded. Some examples: \n",
      "[['a', 'b'], ['a', 'b'], ['c', 'd', 'a', 'b']]\n",
      "The symbol dictionary: {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n"
     ]
    }
   ],
   "source": [
    "dataset = SequenceDataset(DATA_PATH, maxlen=10)\n",
    "dataset.encode_sequences()\n",
    "dataset.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a79ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7eec33",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f694589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using template from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "# tutorial about positional encoding: https://kikaben.com/transformers-positional-encoding/\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        div_term = 10000 ** ( (2 * torch.arange(0, d_model) ) / d_model)\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        for i in range(max_len):\n",
    "            if i % 2 == 0:    \n",
    "                pe[i, 0, :] = torch.sin(position[i] / div_term)\n",
    "            else:\n",
    "                pe[i, 0, :] = torch.cos(position[i] / div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x #self.dropout(x)\n",
    "\n",
    "\n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "                \n",
    "        return x, attn_output, attn_output_weights\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int, embedding_layer=None): #must be same as encoder\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model=embedding_dim, max_len=max_len+2)\n",
    "        \n",
    "        self.masked_mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1)\n",
    "        self.ln = nn.LayerNorm(embedding_dim, eps=1e-12, elementwise_affine=True)\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, query: torch.Tensor=None, key: torch.Tensor=None):\n",
    "        sequence_len = list(x.size())[0]\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.masked_mha(query=x, key=x, value=x, is_causal=True, \\\n",
    "                                                attn_mask=nn.Transformer.generate_square_subsequent_mask(sequence_len))#, is_causal=True)\n",
    "\n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        if query is None or key is None: # only for debugging\n",
    "            attn_output, attn_output_weights = self.mha(query=x, key=x, value=x)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = self.mha(query=query, key=key, value=x)\n",
    "        \n",
    "        x = x + attn_output # skip-connection\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# sidenote: understanding skip-connections: https://theaisummer.com/skip-connections/\n",
    "class AuTransformer(nn.Module):\n",
    "    def __init__(self, alphabet_size: int, embedding_dim: int, max_len:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(alphabet_size+3, embedding_dim) # +3 for start, stop, padding symbol\n",
    "        self.encoder = Encoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        self.decoder = Decoder(alphabet_size=alphabet_size, embedding_dim=embedding_dim, max_len=max_len, embedding_layer=self.input_embedding)\n",
    "        \n",
    "        self.output_fnn = nn.Linear(in_features=embedding_dim, out_features=alphabet_size+3) # +2 for start and stop\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax_output = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.attention_output_layer = nn.Identity() \n",
    "        self.attention_weight_layer = nn.Identity() \n",
    "        \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
    "        x_src = self.input_embedding(src)\n",
    "        x, attention_output, attention_weights = self.encoder(x_src)\n",
    "        attention_output = self.attention_output_layer(attention_output)\n",
    "        attention_weights = self.attention_weight_layer(attention_weights)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x_tgt = self.input_embedding(tgt)\n",
    "        x = self.decoder(x=x_tgt, query=x, key=x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.gelu(self.output_fnn(x))\n",
    "        x = self.softmax_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f5c848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuTransformer(\n",
       "  (input_embedding): Embedding(7, 3)\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (masked_mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((3,), eps=1e-12, elementwise_affine=True)\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_fnn): Linear(in_features=3, out_features=7, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (softmax_output): Softmax(dim=-1)\n",
       "  (attention_output_layer): Identity()\n",
       "  (attention_weight_layer): Identity()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuTransformer(dataset.get_alphabet_size(), 3, max_len=10)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c7922",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce45df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 14:34:13.200576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  batch 0 loss: 1.95803701877594\n",
      "  batch 1000 loss: 1.9591065953969955\n",
      "  batch 2000 loss: 1.9538644553422928\n",
      "  batch 3000 loss: 1.9506673114299775\n",
      "  batch 4000 loss: 1.9448303755521774\n",
      "  batch 5000 loss: 1.9421501959562302\n",
      "  batch 6000 loss: 1.937151843905449\n",
      "  batch 7000 loss: 1.9347624973058701\n",
      "  batch 8000 loss: 1.9290520778894424\n",
      "  batch 9000 loss: 1.9262640327215195\n",
      "  batch 10000 loss: 1.9237321529388427\n",
      "  batch 11000 loss: 1.9178233410120011\n",
      "  batch 12000 loss: 1.9152131612300873\n",
      "Epoch:  1\n",
      "  batch 0 loss: 1.9104465234159218\n",
      "  batch 1000 loss: 1.9091300227642058\n",
      "  batch 2000 loss: 1.9048727722167969\n",
      "  batch 3000 loss: 1.9009423274993897\n",
      "  batch 4000 loss: 1.8953926985263825\n",
      "  batch 5000 loss: 1.8902841042280196\n",
      "  batch 6000 loss: 1.8860702670812606\n",
      "  batch 7000 loss: 1.8811810594797134\n",
      "  batch 8000 loss: 1.8768854554891585\n",
      "  batch 9000 loss: 1.8710489856004715\n",
      "  batch 10000 loss: 1.8654451072216034\n",
      "  batch 11000 loss: 1.8608352576494216\n",
      "  batch 12000 loss: 1.853765730381012\n",
      "Epoch:  2\n",
      "  batch 0 loss: 1.8506411743485165\n",
      "  batch 1000 loss: 1.8455825988054275\n",
      "  batch 2000 loss: 1.8395291225910186\n",
      "  batch 3000 loss: 1.8332047152519226\n",
      "  batch 4000 loss: 1.8260359047651291\n",
      "  batch 5000 loss: 1.8191175856590271\n",
      "  batch 6000 loss: 1.813180654644966\n",
      "  batch 7000 loss: 1.8034728409051894\n",
      "  batch 8000 loss: 1.796493130326271\n",
      "  batch 9000 loss: 1.7886447212696075\n",
      "  batch 10000 loss: 1.7791728163957596\n",
      "  batch 11000 loss: 1.7712408788204193\n",
      "  batch 12000 loss: 1.7624886810779572\n",
      "Epoch:  3\n",
      "  batch 0 loss: 1.7558549120771363\n",
      "  batch 1000 loss: 1.749420572757721\n",
      "  batch 2000 loss: 1.7436977463960648\n",
      "  batch 3000 loss: 1.7323976230621339\n",
      "  batch 4000 loss: 1.723602543115616\n",
      "  batch 5000 loss: 1.7125831248760224\n",
      "  batch 6000 loss: 1.7059304223060607\n",
      "  batch 7000 loss: 1.6935948343276979\n",
      "  batch 8000 loss: 1.6858906155824662\n",
      "  batch 9000 loss: 1.6802358255386352\n",
      "  batch 10000 loss: 1.6674483255147934\n",
      "  batch 11000 loss: 1.6579938793182374\n",
      "  batch 12000 loss: 1.6516684987545014\n",
      "Epoch:  4\n",
      "  batch 0 loss: 1.6407393602409748\n",
      "  batch 1000 loss: 1.6390914026498795\n",
      "  batch 2000 loss: 1.6292956176996232\n",
      "  batch 3000 loss: 1.620518059849739\n",
      "  batch 4000 loss: 1.611428138256073\n",
      "  batch 5000 loss: 1.6062477281093597\n",
      "  batch 6000 loss: 1.5952795079946518\n",
      "  batch 7000 loss: 1.5848152544498444\n",
      "  batch 8000 loss: 1.579991707086563\n",
      "  batch 9000 loss: 1.5734513354301454\n",
      "  batch 10000 loss: 1.5670016423463822\n",
      "  batch 11000 loss: 1.5582079800367354\n",
      "  batch 12000 loss: 1.5551639760732652\n",
      "Epoch:  5\n",
      "  batch 0 loss: 1.5493281389326359\n",
      "  batch 1000 loss: 1.5471523834466934\n",
      "  batch 2000 loss: 1.538301039814949\n",
      "  batch 3000 loss: 1.532675701737404\n",
      "  batch 4000 loss: 1.5292354229688645\n",
      "  batch 5000 loss: 1.5231931164264678\n",
      "  batch 6000 loss: 1.5226240943670273\n",
      "  batch 7000 loss: 1.5141206345558167\n",
      "  batch 8000 loss: 1.5119524933099746\n",
      "  batch 9000 loss: 1.50595634496212\n",
      "  batch 10000 loss: 1.4983848148584367\n",
      "  batch 11000 loss: 1.4959745146036147\n",
      "  batch 12000 loss: 1.4967928347587585\n",
      "Epoch:  6\n",
      "  batch 0 loss: 1.4857175474616413\n",
      "  batch 1000 loss: 1.4898950933218003\n",
      "  batch 2000 loss: 1.4875445004701615\n",
      "  batch 3000 loss: 1.4811387996673584\n",
      "  batch 4000 loss: 1.4815720998048783\n",
      "  batch 5000 loss: 1.4764692511558533\n",
      "  batch 6000 loss: 1.4724086655378341\n",
      "  batch 7000 loss: 1.466185602426529\n",
      "  batch 8000 loss: 1.468823469877243\n",
      "  batch 9000 loss: 1.4696578829288482\n",
      "  batch 10000 loss: 1.4617839139699935\n",
      "  batch 11000 loss: 1.456380322098732\n",
      "  batch 12000 loss: 1.4566378566026688\n",
      "Epoch:  7\n",
      "  batch 0 loss: 1.4539677221767027\n",
      "  batch 1000 loss: 1.4530760682821273\n",
      "  batch 2000 loss: 1.4538989690542221\n",
      "  batch 3000 loss: 1.4465592045783997\n",
      "  batch 4000 loss: 1.4471258475780486\n",
      "  batch 5000 loss: 1.4446237086057663\n",
      "  batch 6000 loss: 1.4448321583271027\n",
      "  batch 7000 loss: 1.44810870885849\n",
      "  batch 8000 loss: 1.4417111793756485\n",
      "  batch 9000 loss: 1.4412569425106048\n",
      "  batch 10000 loss: 1.4389408415555953\n",
      "  batch 11000 loss: 1.4404987446069717\n",
      "  batch 12000 loss: 1.4377396866083145\n",
      "Epoch:  8\n",
      "  batch 0 loss: 1.4357448154025607\n",
      "  batch 1000 loss: 1.4323871475458145\n",
      "  batch 2000 loss: 1.4276344915628434\n",
      "  batch 3000 loss: 1.431940264225006\n",
      "  batch 4000 loss: 1.4298056490421296\n",
      "  batch 5000 loss: 1.4272601206302642\n",
      "  batch 6000 loss: 1.428827023744583\n",
      "  batch 7000 loss: 1.4284125329256059\n",
      "  batch 8000 loss: 1.4268845080137254\n",
      "  batch 9000 loss: 1.4243201253414155\n",
      "  batch 10000 loss: 1.419400439620018\n",
      "  batch 11000 loss: 1.4148244675397872\n",
      "  batch 12000 loss: 1.415711984038353\n",
      "Epoch:  9\n",
      "  batch 0 loss: 1.4106210871577665\n",
      "  batch 1000 loss: 1.4101492598056793\n",
      "  batch 2000 loss: 1.4149524885416032\n",
      "  batch 3000 loss: 1.4089407051801681\n",
      "  batch 4000 loss: 1.4059879053831101\n",
      "  batch 5000 loss: 1.407997722864151\n",
      "  batch 6000 loss: 1.400464919090271\n",
      "  batch 7000 loss: 1.3999516098499298\n",
      "  batch 8000 loss: 1.3984587641954422\n",
      "  batch 9000 loss: 1.3984338607788085\n",
      "  batch 10000 loss: 1.3897270978689193\n",
      "  batch 11000 loss: 1.3903267332315445\n",
      "  batch 12000 loss: 1.3874977197647094\n",
      "Epoch:  10\n",
      "  batch 0 loss: 1.3824967314498593\n",
      "  batch 1000 loss: 1.3819040135145186\n",
      "  batch 2000 loss: 1.3770737714767456\n",
      "  batch 3000 loss: 1.3772983750104903\n",
      "  batch 4000 loss: 1.3717328379154206\n",
      "  batch 5000 loss: 1.3692264811992645\n",
      "  batch 6000 loss: 1.3655572409629821\n",
      "  batch 7000 loss: 1.363783336520195\n",
      "  batch 8000 loss: 1.355063051223755\n",
      "  batch 9000 loss: 1.3574017020463944\n",
      "  batch 10000 loss: 1.3508970663547515\n",
      "  batch 11000 loss: 1.34713479411602\n",
      "  batch 12000 loss: 1.3424236748218537\n",
      "Epoch:  11\n",
      "  batch 0 loss: 1.34009529043127\n",
      "  batch 1000 loss: 1.341040711760521\n",
      "  batch 2000 loss: 1.3379195640087127\n",
      "  batch 3000 loss: 1.3335253331661225\n",
      "  batch 4000 loss: 1.3279468754529953\n",
      "  batch 5000 loss: 1.3260388526916504\n",
      "  batch 6000 loss: 1.3165463050603867\n",
      "  batch 7000 loss: 1.3203753769397735\n",
      "  batch 8000 loss: 1.3142050912380219\n",
      "  batch 9000 loss: 1.3135539979934692\n",
      "  batch 10000 loss: 1.3072501974105835\n",
      "  batch 11000 loss: 1.302558472275734\n",
      "  batch 12000 loss: 1.3004696700572969\n",
      "Epoch:  12\n",
      "  batch 0 loss: 1.3010158647190442\n",
      "  batch 1000 loss: 1.2990534955263138\n",
      "  batch 2000 loss: 1.297146344780922\n",
      "  batch 3000 loss: 1.294430212020874\n",
      "  batch 4000 loss: 1.290679733157158\n",
      "  batch 5000 loss: 1.2868586176633834\n",
      "  batch 6000 loss: 1.2850826499462127\n",
      "  batch 7000 loss: 1.2859017411470413\n",
      "  batch 8000 loss: 1.280057270169258\n",
      "  batch 9000 loss: 1.2810733481645584\n",
      "  batch 10000 loss: 1.2772931261062621\n",
      "  batch 11000 loss: 1.2754708180427552\n",
      "  batch 12000 loss: 1.2749411866664886\n",
      "Epoch:  13\n",
      "  batch 0 loss: 1.2713325938793143\n",
      "  batch 1000 loss: 1.2693487540483475\n",
      "  batch 2000 loss: 1.2705553640127183\n",
      "  batch 3000 loss: 1.269066701054573\n",
      "  batch 4000 loss: 1.2666467034816742\n",
      "  batch 5000 loss: 1.2642399363517762\n",
      "  batch 6000 loss: 1.2614771856069564\n",
      "  batch 7000 loss: 1.2637760800123214\n",
      "  batch 8000 loss: 1.2611765098571777\n",
      "  batch 9000 loss: 1.26017795586586\n",
      "  batch 10000 loss: 1.2566527974605561\n",
      "  batch 11000 loss: 1.2531676912307739\n",
      "  batch 12000 loss: 1.249714267373085\n",
      "Epoch:  14\n",
      "  batch 0 loss: 1.254112518997706\n",
      "  batch 1000 loss: 1.2539601743221283\n",
      "  batch 2000 loss: 1.2514940713644027\n",
      "  batch 3000 loss: 1.2502303504943848\n",
      "  batch 4000 loss: 1.2483118433952332\n",
      "  batch 5000 loss: 1.2450463378429413\n",
      "  batch 6000 loss: 1.2456805077791213\n",
      "  batch 7000 loss: 1.24479838013649\n",
      "  batch 8000 loss: 1.2427883299589157\n",
      "  batch 9000 loss: 1.242329695224762\n",
      "  batch 10000 loss: 1.2413197677135468\n",
      "  batch 11000 loss: 1.2413314300775529\n",
      "  batch 12000 loss: 1.238758050441742\n",
      "Epoch:  15\n",
      "  batch 0 loss: 1.2381415451415863\n",
      "  batch 1000 loss: 1.238698314666748\n",
      "  batch 2000 loss: 1.2372979794740677\n",
      "  batch 3000 loss: 1.2370924479961396\n",
      "  batch 4000 loss: 1.2360434658527375\n",
      "  batch 5000 loss: 1.2335860611200333\n",
      "  batch 6000 loss: 1.2309749668836594\n",
      "  batch 7000 loss: 1.2334705888032913\n",
      "  batch 8000 loss: 1.231402829170227\n",
      "  batch 9000 loss: 1.2302068217992783\n",
      "  batch 10000 loss: 1.2308837500810623\n",
      "  batch 11000 loss: 1.2294405575990677\n",
      "  batch 12000 loss: 1.2281526579856872\n",
      "Epoch:  16\n",
      "  batch 0 loss: 1.226404560936822\n",
      "  batch 1000 loss: 1.2268571268320083\n",
      "  batch 2000 loss: 1.2279987322092056\n",
      "  batch 3000 loss: 1.2265197236537932\n",
      "  batch 4000 loss: 1.2257835054397583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 5000 loss: 1.2231133804321288\n",
      "  batch 6000 loss: 1.2231419405937194\n",
      "  batch 7000 loss: 1.2250736169815064\n",
      "  batch 8000 loss: 1.222743262410164\n",
      "  batch 9000 loss: 1.2215712534189225\n",
      "  batch 10000 loss: 1.2199553837776185\n",
      "  batch 11000 loss: 1.2208569051027298\n",
      "  batch 12000 loss: 1.219822947025299\n",
      "Epoch:  17\n",
      "  batch 0 loss: 1.218863885009329\n",
      "  batch 1000 loss: 1.2194400792121887\n",
      "  batch 2000 loss: 1.218964905023575\n",
      "  batch 3000 loss: 1.2151407790184021\n",
      "  batch 4000 loss: 1.219127675652504\n",
      "  batch 5000 loss: 1.2164126929044723\n",
      "  batch 6000 loss: 1.2156547622680665\n",
      "  batch 7000 loss: 1.2155154950618743\n",
      "  batch 8000 loss: 1.2162469868659973\n",
      "  batch 9000 loss: 1.216186690568924\n",
      "  batch 10000 loss: 1.2129618784189224\n",
      "  batch 11000 loss: 1.2158989250659942\n",
      "  batch 12000 loss: 1.2145122898817062\n",
      "Epoch:  18\n",
      "  batch 0 loss: 1.213725077182757\n",
      "  batch 1000 loss: 1.2134830393791198\n",
      "  batch 2000 loss: 1.2134606751203536\n",
      "  batch 3000 loss: 1.2115380495786667\n",
      "  batch 4000 loss: 1.2122146623134613\n",
      "  batch 5000 loss: 1.2110662405490875\n",
      "  batch 6000 loss: 1.2116113827228545\n",
      "  batch 7000 loss: 1.2124571642875672\n",
      "  batch 8000 loss: 1.208619849562645\n",
      "  batch 9000 loss: 1.2097147799730301\n",
      "  batch 10000 loss: 1.2094266716241837\n",
      "  batch 11000 loss: 1.210794793009758\n",
      "  batch 12000 loss: 1.2059315078258515\n",
      "Epoch:  19\n",
      "  batch 0 loss: 1.2108515850221269\n",
      "  batch 1000 loss: 1.2092927241325377\n",
      "  batch 2000 loss: 1.2087976701259613\n",
      "  batch 3000 loss: 1.2102589530944825\n",
      "  batch 4000 loss: 1.2067788149118424\n",
      "  batch 5000 loss: 1.2061858793497087\n",
      "  batch 6000 loss: 1.2057331482172011\n",
      "  batch 7000 loss: 1.2060227187871932\n",
      "  batch 8000 loss: 1.206275224685669\n",
      "  batch 9000 loss: 1.2049662817716598\n",
      "  batch 10000 loss: 1.2055378388166427\n",
      "  batch 11000 loss: 1.206018840432167\n",
      "  batch 12000 loss: 1.2069768780469894\n",
      "Epoch:  20\n",
      "  batch 0 loss: 1.2038009339310103\n",
      "  batch 1000 loss: 1.2070328356027604\n",
      "  batch 2000 loss: 1.20414080286026\n",
      "  batch 3000 loss: 1.2033253935575485\n",
      "  batch 4000 loss: 1.2026553996801377\n",
      "  batch 5000 loss: 1.2066588948965074\n",
      "  batch 6000 loss: 1.2006608449220657\n",
      "  batch 7000 loss: 1.2020578925609589\n",
      "  batch 8000 loss: 1.204310865521431\n",
      "  batch 9000 loss: 1.2040636893510819\n",
      "  batch 10000 loss: 1.2044538758993149\n",
      "  batch 11000 loss: 1.2051370630264282\n",
      "  batch 12000 loss: 1.2014229224920272\n",
      "Epoch:  21\n",
      "  batch 0 loss: 1.2014255423337121\n",
      "  batch 1000 loss: 1.202279294729233\n",
      "  batch 2000 loss: 1.2016042368412019\n",
      "  batch 3000 loss: 1.204032076239586\n",
      "  batch 4000 loss: 1.201077049255371\n",
      "  batch 5000 loss: 1.2021511348485947\n",
      "  batch 6000 loss: 1.2007268130779267\n",
      "  batch 7000 loss: 1.1992079102993012\n",
      "  batch 8000 loss: 1.2030073713064193\n",
      "  batch 9000 loss: 1.2003152068853378\n",
      "  batch 10000 loss: 1.200841779589653\n",
      "  batch 11000 loss: 1.2001379417181015\n",
      "  batch 12000 loss: 1.200326785683632\n",
      "Epoch:  22\n",
      "  batch 0 loss: 1.2020849166093048\n",
      "  batch 1000 loss: 1.2009351373910904\n",
      "  batch 2000 loss: 1.199115553855896\n",
      "  batch 3000 loss: 1.2006423273086548\n",
      "  batch 4000 loss: 1.2006504298448564\n",
      "  batch 5000 loss: 1.1993755695819854\n",
      "  batch 6000 loss: 1.1989242644309999\n",
      "  batch 7000 loss: 1.199489107131958\n",
      "  batch 8000 loss: 1.1999530158042908\n",
      "  batch 9000 loss: 1.2017424775362016\n",
      "  batch 10000 loss: 1.1996888123750686\n",
      "  batch 11000 loss: 1.195892587184906\n",
      "  batch 12000 loss: 1.19611747443676\n",
      "Epoch:  23\n",
      "  batch 0 loss: 1.1970998723097521\n",
      "  batch 1000 loss: 1.201423235654831\n",
      "  batch 2000 loss: 1.1980951540470124\n",
      "  batch 3000 loss: 1.1966198921203612\n",
      "  batch 4000 loss: 1.1991992619037628\n",
      "  batch 5000 loss: 1.198379181742668\n",
      "  batch 6000 loss: 1.1958447567224502\n",
      "  batch 7000 loss: 1.1968374351263047\n",
      "  batch 8000 loss: 1.199441912651062\n",
      "  batch 9000 loss: 1.1975607591867448\n",
      "  batch 10000 loss: 1.1975697275400161\n",
      "  batch 11000 loss: 1.1973684751987457\n",
      "  batch 12000 loss: 1.194896737575531\n",
      "Epoch:  24\n",
      "  batch 0 loss: 1.195132447011543\n",
      "  batch 1000 loss: 1.19665398311615\n",
      "  batch 2000 loss: 1.1968921576738358\n",
      "  batch 3000 loss: 1.1958830513954162\n",
      "  batch 4000 loss: 1.1976463222503662\n",
      "  batch 5000 loss: 1.1966153512001036\n",
      "  batch 6000 loss: 1.195402529358864\n",
      "  batch 7000 loss: 1.196013325691223\n",
      "  batch 8000 loss: 1.1990516513586045\n",
      "  batch 9000 loss: 1.197987426519394\n",
      "  batch 10000 loss: 1.194807346343994\n",
      "  batch 11000 loss: 1.1954223166704179\n",
      "  batch 12000 loss: 1.1937968831062318\n",
      "Epoch:  25\n",
      "  batch 0 loss: 1.1980854646124022\n",
      "  batch 1000 loss: 1.1975479984283448\n",
      "  batch 2000 loss: 1.1952948215007781\n",
      "  batch 3000 loss: 1.1973320981264115\n",
      "  batch 4000 loss: 1.1957560335397721\n",
      "  batch 5000 loss: 1.1957594039440156\n",
      "  batch 6000 loss: 1.194548270225525\n",
      "  batch 7000 loss: 1.1957439776659011\n",
      "  batch 8000 loss: 1.1944218542575835\n",
      "  batch 9000 loss: 1.1959926606416702\n",
      "  batch 10000 loss: 1.1939276667833327\n",
      "  batch 11000 loss: 1.1941058033704757\n",
      "  batch 12000 loss: 1.1935709643363952\n",
      "Epoch:  26\n",
      "  batch 0 loss: 1.1950214796194725\n",
      "  batch 1000 loss: 1.1955960892438888\n",
      "  batch 2000 loss: 1.1957388073205948\n",
      "  batch 3000 loss: 1.1918373218774796\n",
      "  batch 4000 loss: 1.1946986101865769\n",
      "  batch 5000 loss: 1.1936166249513627\n",
      "  batch 6000 loss: 1.194981094479561\n",
      "  batch 7000 loss: 1.1940282707214356\n",
      "  batch 8000 loss: 1.197202972650528\n",
      "  batch 9000 loss: 1.1955915186405182\n",
      "  batch 10000 loss: 1.1924459434747696\n",
      "  batch 11000 loss: 1.1950001491308213\n",
      "  batch 12000 loss: 1.1926521723270416\n",
      "Epoch:  27\n",
      "  batch 0 loss: 1.1939392832393196\n",
      "  batch 1000 loss: 1.1950669481754304\n",
      "  batch 2000 loss: 1.1943811326026916\n",
      "  batch 3000 loss: 1.1939763882160186\n",
      "  batch 4000 loss: 1.1927102227211\n",
      "  batch 5000 loss: 1.1940832179784775\n",
      "  batch 6000 loss: 1.1937078630924225\n",
      "  batch 7000 loss: 1.1932051224708558\n",
      "  batch 8000 loss: 1.1942895060777665\n",
      "  batch 9000 loss: 1.1921735861301421\n",
      "  batch 10000 loss: 1.194149555683136\n",
      "  batch 11000 loss: 1.1945378081798554\n",
      "  batch 12000 loss: 1.1922428513765335\n",
      "Epoch:  28\n",
      "  batch 0 loss: 1.1940380330840346\n",
      "  batch 1000 loss: 1.1919136793613434\n",
      "  batch 2000 loss: 1.1933199894428252\n",
      "  batch 3000 loss: 1.1930703369379043\n",
      "  batch 4000 loss: 1.1944066241979598\n",
      "  batch 5000 loss: 1.1939284316301346\n",
      "  batch 6000 loss: 1.192251740694046\n",
      "  batch 7000 loss: 1.193258857369423\n",
      "  batch 8000 loss: 1.1933264317512513\n",
      "  batch 9000 loss: 1.1935010824203491\n",
      "  batch 10000 loss: 1.1946026403903962\n",
      "  batch 11000 loss: 1.192328577876091\n",
      "  batch 12000 loss: 1.1922280980348587\n",
      "Epoch:  29\n",
      "  batch 0 loss: 1.192323504072247\n",
      "  batch 1000 loss: 1.1956906921863555\n",
      "  batch 2000 loss: 1.192821923017502\n",
      "  batch 3000 loss: 1.1921020114421845\n",
      "  batch 4000 loss: 1.1942766057252885\n",
      "  batch 5000 loss: 1.1922322442531585\n",
      "  batch 6000 loss: 1.1928220570087433\n",
      "  batch 7000 loss: 1.1914702916145326\n",
      "  batch 8000 loss: 1.192013438463211\n",
      "  batch 9000 loss: 1.1910535683631898\n",
      "  batch 10000 loss: 1.1913499643802643\n",
      "  batch 11000 loss: 1.1923176898956298\n",
      "  batch 12000 loss: 1.1942712044715882\n",
      "Epoch:  30\n",
      "  batch 0 loss: 1.1924120926295065\n",
      "  batch 1000 loss: 1.1925566642284393\n",
      "  batch 2000 loss: 1.1939294480085374\n",
      "  batch 3000 loss: 1.1916666998863221\n",
      "  batch 4000 loss: 1.192575510621071\n",
      "  batch 5000 loss: 1.1932179746627807\n",
      "  batch 6000 loss: 1.1929157999753952\n",
      "  batch 7000 loss: 1.191860367655754\n",
      "  batch 8000 loss: 1.1901929789781571\n",
      "  batch 9000 loss: 1.193304371714592\n",
      "  batch 10000 loss: 1.1905268610715867\n",
      "  batch 11000 loss: 1.1925636954307557\n",
      "  batch 12000 loss: 1.1926871622800828\n",
      "Epoch:  31\n",
      "  batch 0 loss: 1.1924225156154697\n",
      "  batch 1000 loss: 1.192658801794052\n",
      "  batch 2000 loss: 1.1929160990715026\n",
      "  batch 3000 loss: 1.1930067282915116\n",
      "  batch 4000 loss: 1.1919027328491212\n",
      "  batch 5000 loss: 1.1904942331314088\n",
      "  batch 6000 loss: 1.1931300418376922\n",
      "  batch 7000 loss: 1.1930763535499573\n",
      "  batch 8000 loss: 1.190512876868248\n",
      "  batch 9000 loss: 1.191759136915207\n",
      "  batch 10000 loss: 1.1930381766557694\n",
      "  batch 11000 loss: 1.191711434364319\n",
      "  batch 12000 loss: 1.1901771631240845\n",
      "Epoch:  32\n",
      "  batch 0 loss: 1.1934956127545648\n",
      "  batch 1000 loss: 1.1919932388067245\n",
      "  batch 2000 loss: 1.1918662588596345\n",
      "  batch 3000 loss: 1.192178701043129\n",
      "  batch 4000 loss: 1.192200822353363\n",
      "  batch 5000 loss: 1.1923655565977096\n",
      "  batch 6000 loss: 1.1925672382116317\n",
      "  batch 7000 loss: 1.1907125409841537\n",
      "  batch 8000 loss: 1.1937328296899796\n",
      "  batch 9000 loss: 1.1895645347833634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 10000 loss: 1.192585237145424\n",
      "  batch 11000 loss: 1.190426566004753\n",
      "  batch 12000 loss: 1.1921987518072128\n",
      "Epoch:  33\n",
      "  batch 0 loss: 1.1910765720938994\n",
      "  batch 1000 loss: 1.1925922795534134\n",
      "  batch 2000 loss: 1.1930726083517074\n",
      "  batch 3000 loss: 1.1938586111068725\n",
      "  batch 4000 loss: 1.1910223573446275\n",
      "  batch 5000 loss: 1.1905447891950607\n",
      "  batch 6000 loss: 1.1915727878808975\n",
      "  batch 7000 loss: 1.19155235850811\n",
      "  batch 8000 loss: 1.1909854239225388\n",
      "  batch 9000 loss: 1.190323097229004\n",
      "  batch 10000 loss: 1.1921313903331756\n",
      "  batch 11000 loss: 1.191726474046707\n",
      "  batch 12000 loss: 1.1926711412668227\n",
      "Epoch:  34\n",
      "  batch 0 loss: 1.1848810826889191\n",
      "  batch 1000 loss: 1.190201898097992\n",
      "  batch 2000 loss: 1.18933495759964\n",
      "  batch 3000 loss: 1.1915154901742935\n",
      "  batch 4000 loss: 1.190967538714409\n",
      "  batch 5000 loss: 1.1940390442609787\n",
      "  batch 6000 loss: 1.1899739521741868\n",
      "  batch 7000 loss: 1.191983201265335\n",
      "  batch 8000 loss: 1.1910122215747834\n",
      "  batch 9000 loss: 1.1914068937301636\n",
      "  batch 10000 loss: 1.1934004142284393\n",
      "  batch 11000 loss: 1.1921155341863632\n",
      "  batch 12000 loss: 1.1920175788402558\n",
      "Epoch:  35\n",
      "  batch 0 loss: 1.1924829442894418\n",
      "  batch 1000 loss: 1.193851354956627\n",
      "  batch 2000 loss: 1.1891060695648192\n",
      "  batch 3000 loss: 1.1896164944171905\n",
      "  batch 4000 loss: 1.1904821853637695\n",
      "  batch 5000 loss: 1.1909867343902587\n",
      "  batch 6000 loss: 1.1915443024635315\n",
      "  batch 7000 loss: 1.192494319319725\n",
      "  batch 8000 loss: 1.1921848208904267\n",
      "  batch 9000 loss: 1.1924085379838945\n",
      "  batch 10000 loss: 1.1917770081758499\n",
      "  batch 11000 loss: 1.1918977274894715\n",
      "  batch 12000 loss: 1.1916657370328902\n",
      "Epoch:  36\n",
      "  batch 0 loss: 1.1872603676535867\n",
      "  batch 1000 loss: 1.1918965139389037\n",
      "  batch 2000 loss: 1.1921358530521393\n",
      "  batch 3000 loss: 1.1930520225763321\n",
      "  batch 4000 loss: 1.1894188833236694\n",
      "  batch 5000 loss: 1.1887061424255372\n",
      "  batch 6000 loss: 1.1906480691432952\n",
      "  batch 7000 loss: 1.1939121114015578\n",
      "  batch 8000 loss: 1.190385213136673\n",
      "  batch 9000 loss: 1.1884591958522797\n",
      "  batch 10000 loss: 1.192324673652649\n",
      "  batch 11000 loss: 1.1918023706674576\n",
      "  batch 12000 loss: 1.1932560452222825\n",
      "Epoch:  37\n",
      "  batch 0 loss: 1.1898045616117792\n",
      "  batch 1000 loss: 1.1917391260862351\n",
      "  batch 2000 loss: 1.1902075817584992\n",
      "  batch 3000 loss: 1.1905008401870727\n",
      "  batch 4000 loss: 1.1928979932069779\n",
      "  batch 5000 loss: 1.19082444190979\n",
      "  batch 6000 loss: 1.1920994876623154\n",
      "  batch 7000 loss: 1.190720733165741\n",
      "  batch 8000 loss: 1.1907042828798293\n",
      "  batch 9000 loss: 1.1916576820611953\n",
      "  batch 10000 loss: 1.1915949075222014\n",
      "  batch 11000 loss: 1.1913264020681382\n",
      "  batch 12000 loss: 1.1905957489013672\n",
      "Epoch:  38\n",
      "  batch 0 loss: 1.190048544495194\n",
      "  batch 1000 loss: 1.1914025686979295\n",
      "  batch 2000 loss: 1.1918672850131988\n",
      "  batch 3000 loss: 1.191312573671341\n",
      "  batch 4000 loss: 1.1920094534158707\n",
      "  batch 5000 loss: 1.1899435645341874\n",
      "  batch 6000 loss: 1.1925763239860534\n",
      "  batch 7000 loss: 1.190426453948021\n",
      "  batch 8000 loss: 1.190344136118889\n",
      "  batch 9000 loss: 1.1910233061313629\n",
      "  batch 10000 loss: 1.1923104721307753\n",
      "  batch 11000 loss: 1.191679785490036\n",
      "  batch 12000 loss: 1.1899705622196197\n",
      "Epoch:  39\n",
      "  batch 0 loss: 1.1870149293732564\n",
      "  batch 1000 loss: 1.1899839528799057\n",
      "  batch 2000 loss: 1.1920297536849975\n",
      "  batch 3000 loss: 1.1915746769905091\n",
      "  batch 4000 loss: 1.1908686740398406\n",
      "  batch 5000 loss: 1.1913840928077697\n",
      "  batch 6000 loss: 1.191329651594162\n",
      "  batch 7000 loss: 1.1916620560884477\n",
      "  batch 8000 loss: 1.1919455853700638\n",
      "  batch 9000 loss: 1.1905043740272523\n",
      "  batch 10000 loss: 1.1905903856754303\n",
      "  batch 11000 loss: 1.1892974172830582\n",
      "  batch 12000 loss: 1.1914930140972138\n",
      "Epoch:  40\n",
      "  batch 0 loss: 1.1918082847338334\n",
      "  batch 1000 loss: 1.1914374214410781\n",
      "  batch 2000 loss: 1.191190640926361\n",
      "  batch 3000 loss: 1.188813334465027\n",
      "  batch 4000 loss: 1.1914928082227707\n",
      "  batch 5000 loss: 1.1924562355279922\n",
      "  batch 6000 loss: 1.1904488855600357\n",
      "  batch 7000 loss: 1.1903329622745513\n",
      "  batch 8000 loss: 1.190061746954918\n",
      "  batch 9000 loss: 1.1908567450046539\n",
      "  batch 10000 loss: 1.1910090742111206\n",
      "  batch 11000 loss: 1.19076755297184\n",
      "  batch 12000 loss: 1.1926420691013335\n",
      "Epoch:  41\n",
      "  batch 0 loss: 1.1934155743531507\n",
      "  batch 1000 loss: 1.190249532341957\n",
      "  batch 2000 loss: 1.1909900060892105\n",
      "  batch 3000 loss: 1.1938383790254592\n",
      "  batch 4000 loss: 1.1894611449241639\n",
      "  batch 5000 loss: 1.190258665919304\n",
      "  batch 6000 loss: 1.192305833220482\n",
      "  batch 7000 loss: 1.1910426020622253\n",
      "  batch 8000 loss: 1.192231963157654\n",
      "  batch 9000 loss: 1.1906244370937347\n",
      "  batch 10000 loss: 1.191266188263893\n",
      "  batch 11000 loss: 1.1904268709421157\n",
      "  batch 12000 loss: 1.1891098979711532\n",
      "Epoch:  42\n",
      "  batch 0 loss: 1.1903773269267997\n",
      "  batch 1000 loss: 1.1922130670547486\n",
      "  batch 2000 loss: 1.1901268618106842\n",
      "  batch 3000 loss: 1.1919689002037048\n",
      "  batch 4000 loss: 1.189561365365982\n",
      "  batch 5000 loss: 1.1919562009572984\n",
      "  batch 6000 loss: 1.1902872532606126\n",
      "  batch 7000 loss: 1.1906096374988555\n",
      "  batch 8000 loss: 1.1897834392786026\n",
      "  batch 9000 loss: 1.1905282171964646\n",
      "  batch 10000 loss: 1.1902795094251633\n",
      "  batch 11000 loss: 1.1907619827985763\n",
      "  batch 12000 loss: 1.1916284730434419\n",
      "Epoch:  43\n",
      "  batch 0 loss: 1.1959418732710558\n",
      "  batch 1000 loss: 1.1901705878973008\n",
      "  batch 2000 loss: 1.1910590080022811\n",
      "  batch 3000 loss: 1.191382865667343\n",
      "  batch 4000 loss: 1.1891291526556016\n",
      "  batch 5000 loss: 1.191607030272484\n",
      "  batch 6000 loss: 1.1906705663204193\n",
      "  batch 7000 loss: 1.1906483441591262\n",
      "  batch 8000 loss: 1.190076470017433\n",
      "  batch 9000 loss: 1.19270093023777\n",
      "  batch 10000 loss: 1.1904596403837204\n",
      "  batch 11000 loss: 1.1893505284786223\n",
      "  batch 12000 loss: 1.193104905128479\n",
      "Epoch:  44\n",
      "  batch 0 loss: 1.1916183273398917\n",
      "  batch 1000 loss: 1.1912727477550507\n",
      "  batch 2000 loss: 1.190172511458397\n",
      "  batch 3000 loss: 1.1894546581506729\n",
      "  batch 4000 loss: 1.189646561384201\n",
      "  batch 5000 loss: 1.1903504792451858\n",
      "  batch 6000 loss: 1.1925352848768234\n",
      "  batch 7000 loss: 1.190297989487648\n",
      "  batch 8000 loss: 1.1897185039520264\n",
      "  batch 9000 loss: 1.1926619499921798\n",
      "  batch 10000 loss: 1.1894672870635987\n",
      "  batch 11000 loss: 1.1929042639732361\n",
      "  batch 12000 loss: 1.1917722667455672\n",
      "Epoch:  45\n",
      "  batch 0 loss: 1.1907816603528931\n",
      "  batch 1000 loss: 1.1899567668437958\n",
      "  batch 2000 loss: 1.1905995347499847\n",
      "  batch 3000 loss: 1.1909683750867843\n",
      "  batch 4000 loss: 1.1926497939825058\n",
      "  batch 5000 loss: 1.1908906878232957\n",
      "  batch 6000 loss: 1.192252306818962\n",
      "  batch 7000 loss: 1.1905849480628967\n",
      "  batch 8000 loss: 1.1901894918680191\n",
      "  batch 9000 loss: 1.1910534917116165\n",
      "  batch 10000 loss: 1.190083758354187\n",
      "  batch 11000 loss: 1.1912003508806228\n",
      "  batch 12000 loss: 1.1899208109378814\n",
      "Epoch:  46\n",
      "  batch 0 loss: 1.189442571164783\n",
      "  batch 1000 loss: 1.1894382264614105\n",
      "  batch 2000 loss: 1.1918944733142853\n",
      "  batch 3000 loss: 1.1939788422584534\n",
      "  batch 4000 loss: 1.1894477047920227\n",
      "  batch 5000 loss: 1.1923805136680603\n",
      "  batch 6000 loss: 1.1899810808897018\n",
      "  batch 7000 loss: 1.1923816248178483\n",
      "  batch 8000 loss: 1.1902233216762543\n",
      "  batch 9000 loss: 1.1900680487155915\n",
      "  batch 10000 loss: 1.1925315184593202\n",
      "  batch 11000 loss: 1.1890367512702942\n",
      "  batch 12000 loss: 1.1886341021060944\n",
      "Epoch:  47\n",
      "  batch 0 loss: 1.1891654075596871\n",
      "  batch 1000 loss: 1.190776725769043\n",
      "  batch 2000 loss: 1.1903655511140823\n",
      "  batch 3000 loss: 1.190284261584282\n",
      "  batch 4000 loss: 1.190287962079048\n",
      "  batch 5000 loss: 1.192754138469696\n",
      "  batch 6000 loss: 1.1899709310531617\n",
      "  batch 7000 loss: 1.1906789982318877\n",
      "  batch 8000 loss: 1.1894044687747956\n",
      "  batch 9000 loss: 1.1913955951929092\n",
      "  batch 10000 loss: 1.190365495443344\n",
      "  batch 11000 loss: 1.1925761052370072\n",
      "  batch 12000 loss: 1.1907531249523162\n",
      "Epoch:  48\n",
      "  batch 0 loss: 1.1890976533343895\n",
      "  batch 1000 loss: 1.192569619655609\n",
      "  batch 2000 loss: 1.192971440553665\n",
      "  batch 3000 loss: 1.1909032508134842\n",
      "  batch 4000 loss: 1.1913002362251282\n",
      "  batch 5000 loss: 1.190103240132332\n",
      "  batch 6000 loss: 1.1903480281829835\n",
      "  batch 7000 loss: 1.1886779317855836\n",
      "  batch 8000 loss: 1.1905052505731584\n",
      "  batch 9000 loss: 1.1889813174009323\n",
      "  batch 10000 loss: 1.189704093694687\n",
      "  batch 11000 loss: 1.1916697081327439\n",
      "  batch 12000 loss: 1.1914345214366913\n",
      "Epoch:  49\n",
      "  batch 0 loss: 1.19012961483965\n",
      "  batch 1000 loss: 1.1903312863111497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 2000 loss: 1.1915256227254867\n",
      "  batch 3000 loss: 1.1915087594985962\n",
      "  batch 4000 loss: 1.191200368642807\n",
      "  batch 5000 loss: 1.189533034324646\n",
      "  batch 6000 loss: 1.1904689708948135\n",
      "  batch 7000 loss: 1.1882459480762482\n",
      "  batch 8000 loss: 1.1901676877737046\n",
      "  batch 9000 loss: 1.1922130932807922\n",
      "  batch 10000 loss: 1.1905537041425704\n",
      "  batch 11000 loss: 1.1907032012939454\n",
      "  batch 12000 loss: 1.1921330848932266\n"
     ]
    }
   ],
   "source": [
    "# training loop from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "divisor = 0.\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"Epoch: \", i)\n",
    "    for j, (test_string_ord, test_string_ord_sr, test_string_oh, test_string_oh_sr, _, sequence_length) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        test_string_ord = torch.permute(test_string_ord, dims=[1,0])\n",
    "        test_string_ord_sr = torch.permute(test_string_ord_sr, dims=[1,0])\n",
    "        test_string_oh_sr = torch.permute(test_string_oh_sr, dims=[1,0,2])\n",
    "        \n",
    "        outputs = model(test_string_ord, test_string_ord_sr)\n",
    "        loss = loss_fn(torch.squeeze(outputs), torch.squeeze(test_string_oh_sr))\n",
    "        loss.backward()\n",
    "        \n",
    "        #break\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        divisor += float( list(test_string_ord.size())[1] )\n",
    "        if j % 1000 == 0:\n",
    "            last_loss = running_loss / divisor # loss per batch\n",
    "            print('  batch {} loss: {}'.format(j, last_loss))\n",
    "            running_loss = 0.\n",
    "            divisor = 0.\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9cf0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b86fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
