{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c733729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 12:09:21.125833: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Reshape\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Embedding\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT=0.05\n",
    "\n",
    "MAX_LINESIZE = 20 # this is the maximum length of a line. We need this for zero padding\n",
    "ZERO_PADDING_CHAR = \"X\"\n",
    "INPUT_READER_BATCH_SIZE = int(1e5)\n",
    "NEUTRAL_LABEL = \"0\"\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "TRAIN_FILEPATH = os.path.join(\"july_week_5_train.dat.encoded.dat.extracted_sequences.dat\")\n",
    "TEST_FILEPATH = os.path.join(\"july_week_5_test.dat.encoded.dat.extracted_sequences.dat\")\n",
    "LABEL_FILEPATH = os.path.join(\"july_week_5_test.dat.labels.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937bf60a",
   "metadata": {},
   "source": [
    "def get_batch_of_input(infile, n_lines):\n",
    "    \"\"\"Only if a file is too large to fit into memory\"\"\"\n",
    "    res = list()\n",
    "    for _ in range(n_lines):\n",
    "        line = infile.readline()\n",
    "        if line==\"\":\n",
    "            return None\n",
    "        res.append(np.array( line.split()[2:] ))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8218f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(line):\n",
    "    symbols = set( line.split() )\n",
    "    if len(symbols) > 1 or not NEUTRAL_LABEL in symbols:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def read_input(infile, label_file=None, input_alph=None, MAXLINES=int(1e6)):\n",
    "    res = list()\n",
    "    labels = None \n",
    "    if label_file:\n",
    "        labels = list()\n",
    "    \n",
    "    alphabet = dict() if input_alph is None else input_alph\n",
    "    if not ZERO_PADDING_CHAR in alphabet:\n",
    "        alphabet[ZERO_PADDING_CHAR] = len(alphabet)\n",
    "                \n",
    "    for i, line in enumerate(infile):\n",
    "        if i == MAXLINES:\n",
    "            break\n",
    "            \n",
    "        linesplit = line.split()[2:]\n",
    "        line = list()\n",
    "        for x in linesplit:\n",
    "            if not x in alphabet:\n",
    "                alphabet[x] = len(alphabet)\n",
    "            line.append( alphabet[x] )\n",
    "        if len(line) < MAX_LINESIZE:\n",
    "            line.extend( [ alphabet[ZERO_PADDING_CHAR] ] * (MAX_LINESIZE - len(line)) )\n",
    "        res.append( np.array(line) )\n",
    "        \n",
    "        if label_file:\n",
    "            label_line = label_file.readline()\n",
    "            labels.append( get_label(label_line) )\n",
    "\n",
    "    reverse_alphabet = {v:k for k,v in alphabet.items()}\n",
    "    labels = np.array(labels) if labels is not None else labels\n",
    "    return np.array(res), alphabet, reverse_alphabet, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c8f4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 3 1 4 5 1 5 6 1 1 7 5 5 6 1 6 8 4 1]\n",
      " [2 3 1 4 5 1 5 6 1 1 7 5 5 6 1 6 8 4 1 6]] (3000000, 20) 76\n"
     ]
    }
   ],
   "source": [
    "trainfile = open(TRAIN_FILEPATH, \"rt\")\n",
    "train_header = trainfile.readline()\n",
    "\n",
    "x_train, alphabet, _, _ = read_input(infile=trainfile, MAXLINES=int(3e6))\n",
    "ALPHABET_SIZE = len(alphabet)\n",
    "\n",
    "print(x_train[:3], x_train.shape, ALPHABET_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6778cd",
   "metadata": {},
   "source": [
    "x_train_target_one_hot = np.zeros((x_train.shape[0], x_train.shape[1] - 1, len(alphabet)), dtype=\"float32\")\n",
    "\n",
    "for i, row in enumerate(x_train[:, 1:]):\n",
    "    for j, mapped_symbol in enumerate(row):\n",
    "        x_train_target_one_hot[i, j, mapped_symbol] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d354bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_target_one_hot = np.zeros((x_train.shape[0], len(alphabet)), dtype=\"float32\")\n",
    "\n",
    "for i, row in enumerate(x_train):\n",
    "    mapped_symbol = row[-1]\n",
    "    x_train_target_one_hot[i, mapped_symbol] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3e0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(OUTPUT_DIM, N_SEQUENCES):\n",
    "    INPUT_SHAPE = (N_SEQUENCES)\n",
    "\n",
    "    input_layer = Input(shape=(N_SEQUENCES,))\n",
    "    embedding_layer = Embedding(OUTPUT_DIM, EMBEDDING_DIM, trainable=True)\n",
    "    rnn_layer = LSTM(EMBEDDING_DIM)\n",
    "    hidden_layer_1 = Dense(EMBEDDING_DIM)\n",
    "    output_layer = Dense(OUTPUT_DIM, activation=\"softmax\")\n",
    "\n",
    "    x = embedding_layer(input_layer)\n",
    "    x = rnn_layer(x)\n",
    "    x = hidden_layer_1(x)\n",
    "    x = output_layer(x)\n",
    "\n",
    "    model = Model(input_layer, x)\n",
    "    model.compile(\n",
    "      loss=\"categorical_crossentropy\",\n",
    "      optimizer=\"adam\",\n",
    "      metrics=[\"categorical_crossentropy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model(ALPHABET_SIZE, MAX_LINESIZE-1)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    mode=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f61222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 12:09:38.277109: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 410400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 12:09:38.834133: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 820800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84375/84375 [==============================] - 343s 4ms/step - loss: 0.7746 - categorical_crossentropy: 0.7746 - val_loss: 0.8257 - val_categorical_crossentropy: 0.8257\n",
      "Epoch 2/10\n",
      "84375/84375 [==============================] - 423s 5ms/step - loss: 0.7376 - categorical_crossentropy: 0.7376 - val_loss: 0.8233 - val_categorical_crossentropy: 0.8233\n",
      "Epoch 3/10\n",
      "84375/84375 [==============================] - 352s 4ms/step - loss: 0.7291 - categorical_crossentropy: 0.7291 - val_loss: 0.8166 - val_categorical_crossentropy: 0.8166\n",
      "Epoch 4/10\n",
      "84375/84375 [==============================] - 330s 4ms/step - loss: 0.7247 - categorical_crossentropy: 0.7247 - val_loss: 0.8172 - val_categorical_crossentropy: 0.8172\n",
      "Epoch 5/10\n",
      "84375/84375 [==============================] - 329s 4ms/step - loss: 0.7220 - categorical_crossentropy: 0.7220 - val_loss: 0.8143 - val_categorical_crossentropy: 0.8143\n",
      "Epoch 6/10\n",
      "84375/84375 [==============================] - 328s 4ms/step - loss: 0.7203 - categorical_crossentropy: 0.7203 - val_loss: 0.8167 - val_categorical_crossentropy: 0.8167\n",
      "Epoch 7/10\n",
      "84375/84375 [==============================] - 329s 4ms/step - loss: 0.7190 - categorical_crossentropy: 0.7190 - val_loss: 0.8188 - val_categorical_crossentropy: 0.8188\n",
      "Epoch 8/10\n",
      "84375/84375 [==============================] - 329s 4ms/step - loss: 0.7177 - categorical_crossentropy: 0.7177 - val_loss: 0.8177 - val_categorical_crossentropy: 0.8177\n",
      "Epoch 9/10\n",
      "84375/84375 [==============================] - 329s 4ms/step - loss: 0.7170 - categorical_crossentropy: 0.7170 - val_loss: 0.8121 - val_categorical_crossentropy: 0.8121\n",
      "Epoch 10/10\n",
      "84375/84375 [==============================] - 329s 4ms/step - loss: 0.7163 - categorical_crossentropy: 0.7163 - val_loss: 0.8138 - val_categorical_crossentropy: 0.8138\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  x_train[:, :-1], \n",
    "  x_train_target_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d99ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train\n",
    "del x_train_target_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2088efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = open(TRAIN_FILEPATH, \"rt\")\n",
    "test_header = testfile.readline()\n",
    "\n",
    "label_infile = open(LABEL_FILEPATH, \"rt\")\n",
    "x_test, alphabet, reverse_alphabet, y_test = read_input(infile=testfile, label_file=label_infile, input_alph=alphabet, MAXLINES=int(2e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f17694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet_symbols = set( range(ALPHABET_SIZE) )\n",
    "\n",
    "TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "idx_to_delete = list()\n",
    "for i, (row, label) in enumerate(zip(x_test, y_test)):\n",
    "    for x in row:\n",
    "        if not x in alphabet_symbols:\n",
    "            # we see a new symbol. What to do with that one now?\n",
    "            if label == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "            idx_to_delete.append(i)\n",
    "TP, FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e38402b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.delete(x_test, idx_to_delete, axis=0)\n",
    "y_test = np.delete(y_test, idx_to_delete, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45e6377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 13:06:55.663820: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 304000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62500/62500 [==============================] - 101s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 13:08:54.694014: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 608000000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "x_test_pred = model.predict(x_test[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ff05041",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_target_one_hot = np.zeros((x_test.shape[0], len(alphabet)), dtype=\"float32\")\n",
    "\n",
    "for i, row in enumerate(x_test):\n",
    "    mapped_symbol = row[-1]\n",
    "    x_test_target_one_hot[i, mapped_symbol] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d297a051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 93171, 152301, 1753069)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "for label, pred, target in zip(y_test, x_test_pred, x_test_target_one_hot):\n",
    "    true_idx = np.where(target==1)\n",
    "    pred_sort_idx = np.argsort(pred)\n",
    "    if true_idx in pred_sort_idx[-K:]:\n",
    "        if label == 0:\n",
    "            TN += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "    else:\n",
    "        if label == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "TP, FP, FN, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab8824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
